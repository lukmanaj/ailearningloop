[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Lukman blogs about his journey from a practicing health practitioner to learning Machine Learning after joining the first cohort of the Arewa Data Science Academy fellowship that began in January, 2023."
  },
  {
    "objectID": "posts/web-scraping/index.html",
    "href": "posts/web-scraping/index.html",
    "title": "Web Scraping a Daily Trust Article using Requests and Beautiful Soup Libraries",
    "section": "",
    "text": "Web scraping is a key skill all data scientists should have. This is why day 22 of our ongoing Arewa Data Science Academy fellowship covered web scraping using BeautifulSoup. In this article, I try to explain and practice this important skill by scraping a daily trust page that contains an article related to the just concluded Nigerian presidential election.  Firstly, for this task, one needs the following: - A Python installation -I have Python 3.10 installed,  - requests library - BeautifulSoup4 library\nBoth libraries can be installed using pip, or if one uses a conda environment - like I do - the libraries can be installed using conda install . In addition, the documentation for both libraries, which are hyperlinked above, provide description for how to install the libraries.  For the purpose of this article, I used a colab Jupyter notebook, which is free from google and only requires a google account and an internet connection. There’s no need to install anything else.  We first begin by importing the two required libraries\nimport requests\nfrom bs4 import BeautifulSoup\nRequests library enables us to get the content of the page as a html and BeautifulSoup allows us to parse the html. \nWe then create a beautifulSoup object that we can use to navigate the content of the page as follows:\nurl = 'https://dailytrust.com/last-saturday-polls-raised-issues-that-require-immediate-solutions-inec/'\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.content,'html.parser')\nsoup.prettify()\nThe url of the page is first declared, then the response object is created using the get method of requests. Afterwards, the content attribute of the response object is passed to BeautifulSoup to create a soup object that we can use to navigate through the content of the web page. The prettify method is used to show the basic structure of the web page as this can help in pinpointing the exact content needed, in our case, the paragraphs. \nTo keep things simple for now, this is the approach I followed:\nparagraphs =  soup.find_all('p')\nprint(soup.title)\nfor paragraph in paragraphs:\n  if not paragraph.find('a'):\n    print(paragraph.get_text())\nFirst, the find_all() method is used to get all the p tags as a list and this is assigned to the paragraphs variable. Then for the main target of this article, I print the title attribute of the soup object and then loop through each item in the paragraphs variable and print, this gives the entire content of the article. I discovered that the hyperlinked titles of related articles where included and I removed them by excluding all p tags that contained a tags. \nIn conclusion, web scraping is highly useful to a data scientist as it automates getting content from the internet. One does not have to cut and paste long pieces of text or other contents."
  },
  {
    "objectID": "posts/arewads/index.html",
    "href": "posts/arewads/index.html",
    "title": "Joining The First Cohort of the Arewa Data Science Academy Fellowship",
    "section": "",
    "text": "The link for the application to the fellowship was sent to me by my friend Abbas. I reluctantly opened the link and I was surprised to see that I had to complete an introduction to Python course in Udacity to complete the application. They made uploading the certificate a required field. Trust me, I always do my due diligence. Upon completing the course, I found that I could not get a certificate without payment. I took a screenshot if my course dashboard showing that I have completed the course and uploaded that instead. A few days later, I received an email telling me that I had gotten in. This to me, is the beginning of my Python journey. Prior to Arewa, I had only learnt theoretically and had no IDE installed on my laptop. I also only used colab to practice on rare ocassions. After joining Arewa, I was forced to install VSCode and practice daily. Our mentor, Shamsuddeen always says that he wants us to imbibe atomic habits, based on the famous book by James Clear. Having an IDE on my laptop and a structured course with exercises to practice daily has improved my understanding and overall appreciation of the Python programming language.\nOur arewa data science fellowship officially started on the 23rd of January, 2023 with Phase one of four which is broken down into: - 30 Days of Python challenge, designed to improve our Python skills in preparation for Machine Learning and Deep Learning with Pytorch. - The coursera course ‘Learning How to Learn’ which is to improve our learning efficiency as there are numerous concepts to learn in data science. - Git and Github, and - Blogging, to document our learning curve, the reason I’m here.\nThe data science fellowship course is structured for self-paced learning with twice weekly office-hours with the mentors on the weekends to understand concepts and discuss challenges encountered in the exercises given.\nI will discuss more on the fellowship as I touch on the different subjects I encounter in my programming journey with arewa.\n\nPractice makes improvement and possible mastery. This is my take home from my first few weeks in this fellowship. Practice is what I have been missing in my learning journey. As such, it is a great pleasure to be part of the first cohort of this fellowship. In sha Allah, greatness awaits."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Welcome to my blog. I have loved maths and computations since my earliest childhood memories. I recently got the opportunity to put this passion to use in my amazing data science journey."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ailearningloop",
    "section": "",
    "text": "Welcome To My Blog\n\n\n\n\n\n\n\nwelcome\n\n\n\n\n\n\n\n\n\n\n\nJul 18, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n  \n\n\n\n\nWeb Scraping a Daily Trust Article using Requests and Beautiful Soup Libraries\n\n\n\n\n\n\n\nPython\n\n\nrequests\n\n\nweb scraping\n\n\n\n\n\n\n\n\n\n\n\nMar 4, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n  \n\n\n\n\nPractising Concepts Learnt So far in ArewaDS\n\n\n\n\n\n\n\nPython\n\n\nhow to learn\n\n\n\n\n\n\n\n\n\n\n\nFeb 21, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n  \n\n\n\n\nNecessity of Develoing a Learning Method\n\n\n\n\n\n\n\nhow to learn\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n  \n\n\n\n\nHow I Started Learning Python\n\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n  \n\n\n\n\nJoining The First Cohort of the Arewa Data Science Academy Fellowship\n\n\n\n\n\n\n\narewads\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n  \n\n\n\n\nHow I Started Learning Python\n\n\n\n\n\n\n\norigins\n\n\n\n\n\n\n\n\n\n\n\nFeb 12, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/how-to-learn/index.html",
    "href": "posts/how-to-learn/index.html",
    "title": "Necessity of Develoing a Learning Method",
    "section": "",
    "text": "The Arewa Data Science fellowship that I am currently enrolled in required that we complete Barbara Oakley’s and Terrence Sejnowski’s MOOC (massive open online course) on coursera, ‘Learning How to Learn’, as part of the curriculum in the first stage of the fellowship. The fellowship intends to make us efficient learners as we prepare to learn increasingly complex concepts in machine learning and deep learning.I recently completed the course and I found it necessary to share some of the techniques that I have implemented to boost my learning efficiency.\nThe most important and impactful thing is making a good plan. Journaling my learning journey and taking note of techniques that worked and those that did not work out so well for me has helped me hone in on the most efficient techniques I can use. A part of my planning includes making sure that I have a good environment for learning, free of distractions. And after that, adopt the pomodoro technique to ensure that I am completely focussed and make the best use of my learning time. All these are documented as I have found that documentation is an effective way of tracking progress.\nHere are some advices I have for improving learning efficiency: 1. Chunking: Always breakdown the material and ensure that you have dialled down to the main gist. This will improve recall even after a long time 2. Interleaving: I find interleaving better than blocked practice. In interleaving, one tries to mix up different subject materials to prevent boredom whereas blocked practice focuses on completely understanding a subject material in one sitting preferably. Here, I mean allocating sufficient time for different subject materials in a sitting. Interleaving works best for me and I recommend it. 3. Use those flashcards: Memory aids are important in retaining information and I advise that you try some techniques out and settle on the ones that work best for your learning style. 4. Make handwritten notes: Writing helps in consolidating the information in the brain and it is a good way of improving recall and understanding of study material. 5. Spaced repetition: This is far better than trying to ‘conclude’ the learning in one sitting. One can be tempted to read a chapter or topic multiple times in one sitting with no plans of ever revisiting the chapter. This does not work. It is far more effective to plan repeat readings ahead of time. Reading the chapter in different sittings is a more effective way to learn according to the course.\nIn conclusion, the most important step is to have a plan and the next thing is to implement while documenting and iterating. Clever hardwork leads to success."
  },
  {
    "objectID": "posts/30-days-python/index.html",
    "href": "posts/30-days-python/index.html",
    "title": "How I Started Learning Python",
    "section": "",
    "text": "As part of the first phase of the Arewa Data science fellowship, the mentors organised a 30 days of Python challenge that consists of exercises tailored to a particular topic per day. Each day offers a challenge on a particular topic in Python. For example, day 5 covers list and exercises of an appropriate hardness level are provided for practice. This is to support understanding of the concept of lists in Python. In addition, the challenge employs a 5 days a week format, since the weekends serve as virtual office hours and can be used to refactor our code and get ready for the next week.\nParticipating in this 30 Days of Python challenge has given me an avenue to practice the Python concepts I have learnt in the past. Before we began the challenge proper, we were taught how to set up VSCode as an IDE for our Python practice. VSCode is an awesome code editor/IDE I must confess. Afterwards, we were instructed to push our daily exercises to the github repository we all made specifically for the challenge. This challenge has significantly improved my Python experience and I am grateful to Arewa Data Science for the opportunity.\nIn subsequent outings, I will try to explain my experience going through some of the exercises and also discuss what I learnt in some of the virtual office hours and of course, the MOOC, ‘Learning How to Learn’ on coursera. Arewa Data Science Fellowship is an interesting addition to my routine with a potential to be life changing."
  },
  {
    "objectID": "posts/practice-concepts/index.html",
    "href": "posts/practice-concepts/index.html",
    "title": "Practising Concepts Learnt So far in ArewaDS",
    "section": "",
    "text": "What is the output of this code and why?\n\n\n\n\npractice\n\n\n\nI selected this question as it covers so many of concepts we have covered in our Arewa Data Science 30 days of Python so far.\nIt can be summarised as follows: 1. range(1,6,2) is the sequence 1,3,5\n\narr is [1,3,5]\nThe change made to arr1 reflects in arr. So arr also becomes [1,3,5,10]\n*arr, converts arr to tuple and returns (1,3,5,10)\n\nThe question covers the following topics: - range (day 10, introduced during loops) - list comprehension (day 13) - mutation (day 5, lists) - unpacking (day 17)\nrange is a built-in function that helps in creating sequence of integers. It accepts three arguments; start, stop and step. Start is inclusive but stop is exclusive. In this question, range(1,6,2) gives the sequence of 1,3,5.\nList comprehension: The list comprehension statement, arr = [x for x in range(1,6,2)], goes through each element in the sequence, range(1,6,2), and includes it in a list. Since it has been established that range(1,6,2) is 1,3,5 then arr is [1,3,5].\nMutation: This is a concept that is hard for most Python beginners like me to grasp. arr1 = arr does not make a copy of arr and put it in arr1, rather it gives an additional name, arr1 to the object named arr. This implies that the list [1,3,5] now has two names, arr and arr1. This is how assignment works in Python. So any change done to arr1 will reflect in arr and vice versa. Hence arr1.append(10) will add ten to the same list object referenced by both arr and arr1.\nUnpacking: The final ingredient is the unpacking operator- asterisk. The functions returns arr, and this is what we need to define. Unpacking means separating the items of an iterable, in this case the elements of the list arr. The question is then, what happens to the separated elements? If only arr is run, an error is raised as the separated elements need to go into something e.g. list, tuple etc. So arr will not work but [*arr] will work as the elements will be unpacked into a list. The code surrounding the operator determines where the output will be unpacked into. In our case, arr, is given and the syntax for a tuple is an item followed by a comma. Therefore a tuple will be returned.\nIn conclusion, the answer is C for all the reasons above.\n\nEveryday I learn something new in my Data Science journey with Arewa Data Science Fellowship. Less than a month ago, I knew almost nothing in Python but right now, everyday my confidence is increasing when it comes to my Python skills. It is a pleasure to be a part of this and I am incredibly grateful to the mentors for the opportunity to be a part of this amazing journey."
  },
  {
    "objectID": "posts/starting-python/index.html",
    "href": "posts/starting-python/index.html",
    "title": "How I Started Learning Python",
    "section": "",
    "text": "I became interested in Python about a year after I started my first full time job. I just felt that I should have some programming skills since I work with the internet and a laptop almost all the time. I started taking some informal courses on different platforms.\nI remember briefly attempting the ‘Data Analysis with Python certification’ on Freecodecamp but I never got to complete it. My interest was still there but I never took the next step. I then took the Google Data Analytics certification on coursera where I got introduced to R. After that, I took the IBM Data Science 10-course specialization and I got my first contact with Python. I learnt but I did not practice enough and after a few months, I only knew the basics but I could not apply the concepts to build projects on my own. The IBM Data Science Certificate was quite Pythonic as out of the ten courses in the specialization, five were Python courses:\n\nPython for data science\nData Analysis with Python\nPython project for data science\nData Visualization with Python\nMachine Learning with Python\n\nIn the end, I didn’t have a clear pathway to a future but I am happy that I am now in a fellowship with a community that can support me as I strive to achieve the programming goal. Alhamdulillah."
  }
]