[
  {
    "objectID": "posts/deeplearning-mentorship/index.html",
    "href": "posts/deeplearning-mentorship/index.html",
    "title": "Yet Another Quarter of Dedicated Mentorship",
    "section": "",
    "text": "I am happy to announce to all that I have been recognized once again as one of the top contributing mentors for the two specializations I mentor at the DeepLearning.AI forum, the Data Analytics Professional Certification and the Tensorflow Developer Professional Certification. Here are the certifications\n\n\n\ndata analytics\n\n\n\n\n\ntensorflow developer\n\n\nSuper grateful to DeepLearning.AI for granting me this wonderful opportunity."
  },
  {
    "objectID": "posts/mentorship-recognition/index.html",
    "href": "posts/mentorship-recognition/index.html",
    "title": "Celebrating Recognition from DeepLearning.AI",
    "section": "",
    "text": "I am thrilled to share that I have been recognized by DeepLearning.AI for my contributions and guidance as a mentor in for the TensorFlow Developer Professional Certificate Specialization in the DeepLearning.AI forum. This honor reflects my passion for empowering learners in the field of artificial intelligence and supporting their journeys toward mastering deep learning.\n As a mentor, I’ve had the privilege of engaging with an incredible community of learners, offering insights, answering questions, and fostering collaboration. This recognition strengthens my commitment to knowledge sharing and the collective growth of AI enthusiasts worldwide.\nI am grateful to DeepLearning.AI for the recognition and acknowledgment. Here’s to continuing to learn, teach, and contribute to the ever-evolving field of AI!"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Welcome to my blog. I have loved maths and computations since my earliest childhood memories. I recently got the opportunity to put this passion to use in my amazing data science journey."
  },
  {
    "objectID": "posts/viz/index.html",
    "href": "posts/viz/index.html",
    "title": "Creating Beautiful Visualizations with Python: A Guide to Pandas Matplotlib and Seaborn",
    "section": "",
    "text": "Data visualization is a crucial aspect of data analysis as it helps in understanding data better. Python provides several libraries for data visualization, and in this article, we will explore three of the most commonly used libraries for data visualization in Python: pandas, matplotlib, and seaborn.\nPandas is a powerful library for data manipulation and analysis. It provides data structures for efficiently storing and manipulating large datasets. One of the primary data structures provided by pandas is the DataFrame, which is a two-dimensional table-like structure with labeled axes. The pandas library also provides a number of methods for data manipulation and cleaning, such as data cleaning, data merging, and data transformation.\nMatplotlib is a plotting library for Python that provides a range of 2D plotting capabilities. It provides a high level of customization, making it possible to create complex visualizations with ease. Matplotlib also provides support for a wide range of output formats, including PDF, PNG, and SVG.\nSeaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for creating attractive and informative statistical graphics. Seaborn makes it easy to create complex visualizations by providing a range of built-in styles and color palettes.\nLet’s start by exploring the pandas library.\n\nData visualization with pandas\nPandas provides a number of methods for data visualization. One of the most commonly used methods is the plot method, which can be used to create a wide range of visualizations, including line plots, scatter plots, bar plots, and histograms.\nTo demonstrate the use of pandas for data visualization, let’s start by loading a sample dataset. We will use the famous Iris dataset, which contains information about the lengths and widths of petals and sepals for three different species of Iris flowers. I first came across this dataset when I first started learning R’s tidyverse.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the iris dataset\niris = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')\n\nWe can use the plot method to create a scatter plot of the petal length and width for the Iris dataset:\n\niris.plot(kind='scatter', x='petal_length', y='petal_width')\nplt.show()\n\n\n\n\n\n\n\n\nAs can be seen from the above, pandas makes it easy to create a scatter plot with just a single line of code. We can also customize the plot by adding labels and a title:\n\niris.plot(kind='scatter', x='petal_length', y='petal_width')\nplt.xlabel('Petal Length')\nplt.ylabel('Petal Width')\nplt.title('Iris Petal Length vs. Width')\nplt.show()\n\n\n\n\n\n\n\n\nIn addition to scatter plots, pandas can also be used to create line plots, bar plots, and histograms. Let’s create a bar plot of the mean petal length for each of the three species of Iris:\n\niris.groupby('species')['petal_length'].mean().plot(kind='bar')\nplt.ylabel('Petal Length')\nplt.title('Mean Petal Length for Each Species of Iris')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nData visualization with matplotlib\nWhile pandas provides a number of convenient methods for data visualization, matplotlib provides a more low-level interface for creating visualizations. This makes it possible to create highly customized visualizations, but it requires a bit more effort. Let’s see how we can use matplotlib to create a scatter plot of the petal length and width for the Iris dataset.\n\nimport matplotlib.pyplot as plt\n# Create a scatter plot of petal length vs. petal width\nplt.scatter(iris['petal_length'], iris['petal_width'])\n# Add labels and a title\nplt.xlabel('Petal Length')\nplt.ylabel('Petal Width')\nplt.title('Iris Petal Length vs. Width')\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nFrom the above, it can be seen that creating a scatter plot with matplotlib requires a bit more code than with pandas, but it provides a high level of customization. We can also create a line plot of the mean petal length for each of the three species of Iris:\n\nimport numpy as np\n# Get the mean petal length for each species\nsetosa_mean = iris[iris['species'] == 'setosa']['petal_length'].mean()\nversicolor_mean = iris[iris['species'] == 'versicolor']['petal_length'].mean()\nvirginica_mean = iris[iris['species'] == 'virginica']['petal_length'].mean()\n# Create a bar chart of the mean petal length\nplt.bar(['setosa', 'versicolor', 'virginica'], [setosa_mean, versicolor_mean, virginica_mean])\n# Add a horizontal line at the overall mean petal length\nplt.axhline(y=iris['petal_length'].mean(), color='gray', linestyle='--')\n# Add labels and a title\nplt.xlabel('Species')\nplt.ylabel('Mean Petal Length')\nplt.title('Mean Petal Length for Each Species of Iris')\n# Show the plot\n\nplt.show()\n\n\n\n\n\n\n\n\nFrom the above plot, matplotlib provides a lot of flexibility for creating customized visualizations.\n\n\nData visualization with seaborn\nSeaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for creating attractive and informative statistical graphics. Seaborn makes it easy to create complex visualizations by providing a range of built-in styles and color palettes.\nTo create a scatter plot of the petal length and width for the Iris dataset with seaborn, we can use the scatterplot function:\n\nimport seaborn as sns\n# Create a scatter plot of petal length vs. petal width\nsns.scatterplot(data=iris, x='petal_length', y='petal_width')\n# Add labels and a title\nplt.xlabel('Petal Length')\nplt.ylabel('Petal Width')\nplt.title('Iris Petal Length vs. Width')\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nFrom the above plot, seaborn provides a clean and attractive visualization with just a single line of code.\nWe can also create a line plot of the mean petal length for each of the three species of Iris with the lineplot function:\n\n# Create a line plot of the mean petal length for each species\nsns.lineplot(data=iris, x='species', y='petal_length', estimator=np.mean)\n# Add labels and a title\nplt.xlabel('Species')\nplt.ylabel('Mean Petal Length')\nplt.title('Mean Petal Length for Each Species of Iris')\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nFrom the above plot, provides a more polished and informative visualization than matplotlib with very little code.\nSeaborn also provides many other types of visualizations, including heatmaps, histograms, and violin plots, that can be used to explore relationships in your data.\n\nConclusion\nIn this article, we have seen how to use Python libraries such as pandas, matplotlib, and seaborn for data visualization. Data visualization is an essential part of the data analysis process because it allows us to explore and understand our data more effectively.\nPandas provides a convenient interface for creating basic visualizations such as scatter plots and bar charts. Matplotlib provides a high level of customization and control over the appearance of your visualizations, but requires more code to create them. Seaborn provides a high-level interface for creating attractive and informative statistical graphics with minimal code.\nAs with any tool, it is important to choose the right visualization library for your specific needs. If you need to create simple visualizations quickly and easily, pandas may be the best choice. If you need a high level of control over the appearance of your visualizations, matplotlib may be the better option. If you want to create complex statistical graphics quickly and easily, seaborn may be the way to go. Seaborn is my current favourite. In any case, with the power of Python and these visualization libraries, you can create informative and attractive visualizations that help you better understand your data."
  },
  {
    "objectID": "posts/unet/index.html",
    "href": "posts/unet/index.html",
    "title": "U-Net Architecture: Revolutionizing Computer Vision Through Innovative Image Segmentation",
    "section": "",
    "text": "In the dynamic world of computer vision, where innovation is key, few architectures have made as profound an impact as U-Net. Published in 2015 by Olaf Ronneberger, Philipp Fischer, and Thomas Brox, the paper titled “U-Net: Convolutional Networks for Biomedical Image Segmentation” introduced a revolutionary approach to image segmentation. Its influence extends far beyond its biomedical origins, as U-Net’s innovative design and remarkable performance have shaped the landscape of computer vision. In this blog post, we’ll explore the motivations, architecture, and the far-reaching impact of the U-Net."
  },
  {
    "objectID": "posts/unet/index.html#introduction",
    "href": "posts/unet/index.html#introduction",
    "title": "U-Net Architecture: Revolutionizing Computer Vision Through Innovative Image Segmentation",
    "section": "",
    "text": "In the dynamic world of computer vision, where innovation is key, few architectures have made as profound an impact as U-Net. Published in 2015 by Olaf Ronneberger, Philipp Fischer, and Thomas Brox, the paper titled “U-Net: Convolutional Networks for Biomedical Image Segmentation” introduced a revolutionary approach to image segmentation. Its influence extends far beyond its biomedical origins, as U-Net’s innovative design and remarkable performance have shaped the landscape of computer vision. In this blog post, we’ll explore the motivations, architecture, and the far-reaching impact of the U-Net."
  },
  {
    "objectID": "posts/unet/index.html#motivation-and-problem-statement",
    "href": "posts/unet/index.html#motivation-and-problem-statement",
    "title": "U-Net Architecture: Revolutionizing Computer Vision Through Innovative Image Segmentation",
    "section": "Motivation and Problem Statement",
    "text": "Motivation and Problem Statement\nThe crux of U-Net’s inception lay in biomedical image segmentation. The challenge was clear: accurately assign each pixel in an image to a specific class or category. Traditional methods struggled with capturing fine details and object boundaries, relying heavily on handcrafted features. U-Net aimed to change this paradigm."
  },
  {
    "objectID": "posts/unet/index.html#architecture-overview",
    "href": "posts/unet/index.html#architecture-overview",
    "title": "U-Net Architecture: Revolutionizing Computer Vision Through Innovative Image Segmentation",
    "section": "Architecture Overview",
    "text": "Architecture Overview\n\n\n\nunet architecture\n\n\nU-Net’s architecture resembles the letter “U,” a design that would become iconic in the field. It comprises two primary components: the contracting path (left side) and the expansive path (right side).\n\nContracting Path: This part adopts a typical convolutional neural network (CNN) structure, with convolutional and pooling layers. These layers progressively reduce spatial dimensions while enhancing feature channels, enabling the network to learn hierarchical features.\nExpansive Path: The expansive path upscales learned features using transpose convolutions (or deconvolutions). What sets U-Net apart is the fusion of these upsampled features with those from the contracting path. This mechanism, called skip connections, merges high-level contextual information with detailed spatial data, mitigating information loss.\nSkip Connections: Skip connections emerged as a pivotal innovation in U-Net. They combat information loss during downsampling and upsampling, significantly improving accuracy, especially for capturing fine details and object boundaries.\nLoss Function: U-Net uses a pixel-wise cross-entropy loss function. It quantifies the disparity between predicted segmentation and ground truth on a per-pixel basis, facilitating accurate pixel-level learning.\nData Augmentation: Data augmentation plays a vital role in U-Net’s robustness. Techniques like random cropping, elastic deformations, and rotations artificially enhance training data diversity.\nRevolutionizing Impact: The U-Net architecture has left an indelible mark on computer vision, thanks to several key factors:\nEfficient Semantic Segmentation: U-Net introduced an efficient architecture for semantic segmentation. Its ability to capture both global context and fine details via skip connections proved invaluable for tasks requiring precise object boundaries and details.\nSkip Connections and Multi-scale Information: The integration of multi-scale information via skip connections elevated U-Net’s accuracy, surpassing traditional CNNs.\nState-of-the-Art Performance: U-Net quickly gained acclaim for its outstanding performance in biomedical image segmentation. It consistently outperformed existing methods, setting new accuracy benchmarks.\nBroad Applicability: While initially conceived for biomedical imaging, U-Net’s principles have transcended boundaries. It has been adapted for diverse computer vision tasks, including natural scene segmentation, satellite image analysis, self-driving cars and beyond."
  },
  {
    "objectID": "posts/unet/index.html#pytorch-implementation-of-u-net",
    "href": "posts/unet/index.html#pytorch-implementation-of-u-net",
    "title": "U-Net Architecture: Revolutionizing Computer Vision Through Innovative Image Segmentation",
    "section": "PyTorch Implementation of U-Net",
    "text": "PyTorch Implementation of U-Net\nNow, let’s dive into a PyTorch implementation of the U-Net architecture, complete with explanations and code snippets.These were adapted from here\n# importing relevant libraries\nimport torch\nimport torchvision.transforms.functional\nfrom torch import nn\nPadding will be used in this implementation to avoid cropping the final image. This is the key difference with the actual unet paper implementation.\n# Defining the layers\n\nclass DoubleConvolution(nn.Module):\n  '''Each step in the contraction path and expansive path have two 3×3 convolutional layers followed by ReLU activations.\n'''\n  def __init__(self,in_channels:int,out_channels:int):\n    super.__init__()\n    self.first = nn.Conv2d(in_channesls,out_channels,kernel_size=3,padding= 1)\n    self.act1 = nn.ReLU()\n    self.second = nn.Conv2d(in_channels,out_channels,kernel_size=3,padding=1)\n    self.act2 = nn.ReLU()\n\n    def forward(self,x:torch.Tensor):\n      # Apply the two convolution layers and activations\n      x = self.first(x)\n      x = self.act1(x)\n      x = self.second(x)\n      return self.act2(x)\n\n\nclass Downsample(nn.Module):\n  # Each step in the contracting path down-samples the feature map with a 2×2 max pooling layer.\n  def __init__(self):\n    super.__init__()\n    # max pooling layer\n    self.pool = nn.MaxPool2d(2)\n\n  def forward(self,x:torch.Tensor):\n    return self.pool(x)\n\nclass Upsample(nn.Module):\n  # Each step in the expansive path up-samples the feature map with a 2×2 up-convolution.\n  def __init__(self,in_channels:int,out_channels:int):\n    super.__init__()\n    # up convolution\n    self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n\n  def forward(self,x:torch.Tensor):\n    return self.up(x)\n\nclass CropAndConcat(nn.Module):\n   '''At every step in the expansive path the corresponding feature map from the contracting path is concatenated with the current feature map.'''\n  def forward(self, x: torch.Tensor, contracting_x: torch.Tensor):\n    # Crop the feature map from the contracting path to the size of the current feature map\n    contracting_x = torchvision.transforms.functional.center_crop(contracting_x, [x.shape[2], x.shape[3]])\n    # Concatenate the feature maps\n    x = torch.cat([x, contracting_x], dim=1)\n    return x\n\n# The Unet Architecture\nclass UNet(nn.Module):\n  def __init__(self,in_channels: int,out_channels: int):\n    super().__init()\n    # Double convolution layers for the contracting path. The number of features gets doubled at each step starting from 64.\n    self.down_conv = nn.ModuleList([DoubleConvolution(i, o) for i, o in [(in_channels, 64), (64, 128), (128, 256), (256, 512)]])\n    # Down sampling layers for the contracting path\n    self.down_sample = nn.ModuleList([DownSample() for _ in range(4)])\n    # The two convolution layers at the lowest resolution (the bottom of the U).\n    self.middle_conv = DoubleConvolution(512, 1024)\n    # Up sampling layers for the expansive path. The number of features is halved with up-sampling.\n    self.up_sample = nn.ModuleList([UpSample(i, o) for i, o in [(1024, 512), (512, 256), (256, 128), (128, 64)]])\n    # Double convolution layers for the expansive path. Their input is the concatenation of the current feature map and the feature map from the contracting path.\n    # Therefore, the number of input features is double the number of features from up-sampling.\n    self.up_conv = nn.ModuleList([DoubleConvolution(i, o) for i, o in [(1024, 512), (512, 256), (256, 128), (128, 64)]])\n    # Crop and concatenate layers for the expansive path.\n    self.concat = nn.ModuleList([CropAndConcat() for _ in range(4)])\n     #   Final 1×1 convolution layer to produce the output\n    self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n\n    def forward(self, x: torch.Tensor):\n      # To collect the outputs of contracting path for later concatenation with the expansive path.\n      pass_through = []\n      # contracting path\n      for i in range(len(self.down_conv)):\n        # Two 3×3 convolutional layers\n        # afford\n         x = self.down_conv[i](x)\n         # collect the output\n         pass_through.append(X)\n         # Down-sample\n         x = self.down_sample[i](x)\n         # Two 3×3 convolutional layers at the bottom of the U-Net\n      x = self.middle_conv(x)\n      for i in range(len(self.up_conv)):\n        x = self.up_sample[i](x)\n\n     # Concatenate the output of the contracting path\n\n        x = self.concat[i](x, pass_through.pop())\n\n       # Two 3×3 convolutional layers\n        x = self.up_conv[i](x)\n        # Final 1×1 convolution layer\n\n        x = self.final_conv(x)\n    return x\nIn conclusion, U-Net’s innovative architecture and exceptional performance have revolutionized computer vision, making it a cornerstone in the field. Its adaptability and versatility have expanded its influence far beyond its original domain, leaving an enduring legacy in the world of computer vision."
  },
  {
    "objectID": "posts/tensorflow-mentor/index.html",
    "href": "posts/tensorflow-mentor/index.html",
    "title": "A Journey from Learner to Leader: My Deep Learning.AI and Coursera Adventure",
    "section": "",
    "text": "On August 14, 2024, I reached a significant milestone in my AI career: I was officially designated as a mentor for the TensorFlow Developer Certificate program on Deep Learning.AI and Coursera. This comprehensive certification, consisting of four courses, introduces learners to the powerful TensorFlow framework. As I reflect on this achievement, I’m filled with pride and gratitude for how far I’ve come in such a short time."
  },
  {
    "objectID": "posts/tensorflow-mentor/index.html#the-tensorflow-developer-professional-certificate",
    "href": "posts/tensorflow-mentor/index.html#the-tensorflow-developer-professional-certificate",
    "title": "A Journey from Learner to Leader: My Deep Learning.AI and Coursera Adventure",
    "section": "The TensorFlow Developer Professional Certificate",
    "text": "The TensorFlow Developer Professional Certificate\nThe certificate program I now mentor comprises four essential courses:\n\nIntroduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning\nConvolutional Neural Networks in TensorFlow\nNatural Language Processing in TensorFlow\nSequences, Time Series and Prediction\n\nThese courses cover a broad spectrum of AI applications, from basic neural networks to advanced techniques in computer vision, natural language processing, and time series analysis."
  },
  {
    "objectID": "posts/tensorflow-mentor/index.html#a-serendipitous-beginning",
    "href": "posts/tensorflow-mentor/index.html#a-serendipitous-beginning",
    "title": "A Journey from Learner to Leader: My Deep Learning.AI and Coursera Adventure",
    "section": "A Serendipitous Beginning",
    "text": "A Serendipitous Beginning\nMy AI learning journey began with the Arewa Data Science Academy fellowship, a program that holds a special place in my heart. Simultaneously, I embarked on Andrew Ng’s Machine Learning Specialization on Coursera. This coincidence set the stage for my rapid progression in the AI community."
  },
  {
    "objectID": "posts/tensorflow-mentor/index.html#rising-through-the-ranks",
    "href": "posts/tensorflow-mentor/index.html#rising-through-the-ranks",
    "title": "A Journey from Learner to Leader: My Deep Learning.AI and Coursera Adventure",
    "section": "Rising Through the Ranks",
    "text": "Rising Through the Ranks\nMy involvement with Deep Learning.AI and Coursera deepened quickly:\n\nForum Moderator: In November 2023, just a few months into my journey, I became a moderator at the DeepLearning.AI forum, helping to guide discussions and support learners tackling challenges in various courses offered in the community.\nCourse Tester: I was invited to be part of the testing group for a major course upgrade, partly due to my active contribution as a moderator, where we transitioned from an older version of TensorFlow to the cutting-edge TensorFlow 2.16. My insightful feedback contributed to refining the course content and user experience across all four courses.\n\n\n\nMentor: The crowning achievement came in August 2024, when I was offered the opportunity to become a mentor for the TensorFlow Developer Certificate courses, in addition to my moderator role. This position allows me to guide learners through the intricacies of TensorFlow, from basic concepts to advanced applications in CNN, NLP, and time series prediction.\n\nThis rapid transition from learner to contributor to mentor and moderator has been both humbling and exhilarating."
  },
  {
    "objectID": "posts/tensorflow-mentor/index.html#celebrating-milestones-and-looking-ahead",
    "href": "posts/tensorflow-mentor/index.html#celebrating-milestones-and-looking-ahead",
    "title": "A Journey from Learner to Leader: My Deep Learning.AI and Coursera Adventure",
    "section": "Celebrating Milestones and Looking Ahead",
    "text": "Celebrating Milestones and Looking Ahead\nAs I continue to contribute to the AI community, I’m committed to making a lasting impact in all the places I volunteer, especially at Arewa Data Science Academy, where my journey began. Each step forward reminds me of the importance of celebrating small victories while keeping an eye on future goals.\nMy new role as a TensorFlow Developer Certificate mentor, alongside my moderator responsibilities, is not just a personal achievement; it’s an opportunity to give back to the community that has nurtured my growth. It’s a chance to inspire and guide the next generation of AI enthusiasts and developers across various domains of machine learning and deep learning."
  },
  {
    "objectID": "posts/tensorflow-mentor/index.html#a-message-to-aspiring-ai-developers",
    "href": "posts/tensorflow-mentor/index.html#a-message-to-aspiring-ai-developers",
    "title": "A Journey from Learner to Leader: My Deep Learning.AI and Coursera Adventure",
    "section": "A Message to Aspiring AI Developers",
    "text": "A Message to Aspiring AI Developers\nAs I forge ahead in pursuing my dreams in the AI field, I’m reminded of the power of persistence, continuous learning, and community support. To all aspiring AI developers out there: keep pushing, keep learning, and remember that today’s learner can be tomorrow’s leader.\nThe path from novice to mentor can be shorter than you think. With dedication and the right opportunities, you can quickly find yourself in a position to guide others. Embrace every chance to learn, contribute, and grow within the AI community.\nHere’s to the journey ahead, filled with neural networks, convolutional layers, natural language models, time series predictions, and the endless possibilities of artificial intelligence!"
  },
  {
    "objectID": "posts/tensorflow-dev/index.html",
    "href": "posts/tensorflow-dev/index.html",
    "title": "My Journey in Deep Learning: From TensorFlow to PyTorch and Back",
    "section": "",
    "text": "I recently completed the DeepLearning.AI TensorFlow Developer Professional Certificate, a milestone that marked a significant step in my deep learning journey. My exploration of TensorFlow began last year while taking the second course of the DeepLearning.AI Deep Learning Specialization, excellently taught by Andrew Ng.\nFollowing my initial foray into TensorFlow, I delved extensively into PyTorch during the Arewa Data Science Academy’s Deep Learning with PyTorch fellowship. The allure of PyTorch, with its state-of-the-art (SOTA) models, pythonic nature and high customizability, made me somewhat hesitant to further pursue TensorFlow. However, an unexpected opportunity arose when, as a moderator in the DeepLearning.AI Forum, I was invited to test a TensorFlow course. This experience reignited my interest and ultimately led me to pursue the Professional Certificate on Coursera.\nContrary to my initial reservations, I found TensorFlow to be quite user-friendly. While it may offer less customization compared to PyTorch, TensorFlow’s learning curve is significantly gentler, making it accessible to a wider audience.\nThe TensorFlow Developer Professional Certificate comprises four courses, each designed to quickly get learners up to speed with TensorFlow:\n\nIntroduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning\nConvolutional Neural Networks in TensorFlow\nNatural Language Processing in TensorFlow\nSequences, Time Series, and Prediction\n\nThe certificate is excellently taught by Laurence Moroney, who leads AI Advocacy at Google. His vision is to make AI easy for developers and widen access to machine learning careers for everyone. Laurence is a prolific author, with dozens of programming books to his name, including ‘AI and ML for Coders’ at O’Reilly. He is also an active member of the Science Fiction Writers of America, having authored several sci-fi novels, comic books, and a produced screenplay.\nThroughout the series of courses, I built on my existing TensorFlow skills, learning about regular dense layers, convolutional neural networks (CNNs), recurrent neural networks (RNNs), long short-term memory networks (LSTMs), gated recurrent units (GRUs), and more. I even explored lambda layers in TensorFlow. The culmination of the course involved applying deep learning and sequence models to time series data.\nCompleting this course has been an immensely rewarding experience. I am grateful to Coursera for providing a 90% discount, making it possible for me to embark on and complete this learning journey. The structured learning path and the expertise of the instructors have significantly enhanced my understanding and proficiency in using TensorFlow.\nIn conclusion, while my journey began with PyTorch, the experience with TensorFlow has been equally enriching. TensorFlow’s ease of use and comprehensive learning resources have made it a valuable tool in my deep learning toolkit. I am excited to apply these skills in future projects and continue exploring the evolving landscape of deep learning technologies."
  },
  {
    "objectID": "posts/statistics-in-r/index.html",
    "href": "posts/statistics-in-r/index.html",
    "title": "Celebrating a Milestone in Statistics: A Journey through DataCamp’s ‘Statistics Fundamentals with R’ Track",
    "section": "",
    "text": "It’s a moment of pride and immense satisfaction as I share the completion of the “Statistics Fundamentals with R” track offered by DataCamp, an educational journey made possible through the generous donations of DataCamp to Arewa Data Science Academy. This accomplishment is not just a testament to my dedication but also highlights the empowering vision of Arewa Data Science Academy and DataCamp in fostering data literacy and analytical skills among learners."
  },
  {
    "objectID": "posts/statistics-in-r/index.html#introduction",
    "href": "posts/statistics-in-r/index.html#introduction",
    "title": "Celebrating a Milestone in Statistics: A Journey through DataCamp’s ‘Statistics Fundamentals with R’ Track",
    "section": "",
    "text": "It’s a moment of pride and immense satisfaction as I share the completion of the “Statistics Fundamentals with R” track offered by DataCamp, an educational journey made possible through the generous donations of DataCamp to Arewa Data Science Academy. This accomplishment is not just a testament to my dedication but also highlights the empowering vision of Arewa Data Science Academy and DataCamp in fostering data literacy and analytical skills among learners."
  },
  {
    "objectID": "posts/statistics-in-r/index.html#the-pathway-to-mastery-delving-deep-into-statistical-analysis",
    "href": "posts/statistics-in-r/index.html#the-pathway-to-mastery-delving-deep-into-statistical-analysis",
    "title": "Celebrating a Milestone in Statistics: A Journey through DataCamp’s ‘Statistics Fundamentals with R’ Track",
    "section": "The Pathway to Mastery: Delving Deep into Statistical Analysis",
    "text": "The Pathway to Mastery: Delving Deep into Statistical Analysis\nThe track was meticulously designed to guide learners through the foundational to advanced concepts of statistics applied in the versatile R programming environment. It commenced with the “Introduction to Statistics in R”, a course that laid the groundwork by introducing statistical concepts and how they are implemented in R. This initial phase was crucial as it set the tone for the rigorous analytical skills I was about to develop.\nTransitioning from basics to more complex analyses, the “Introduction to Regression in R” and “Intermediate Regression in R” courses elevated my understanding of relationships between variables and how these relationships can be quantified and tested. Through these courses, I gained proficiency in building, diagnosing, and interpreting linear regression models, an indispensable skill in any data scientist’s toolkit.\nThe learning curve then ascended to “Sampling in R” and “Hypothesis Testing in R”, courses that are fundamental in statistical inference. These courses taught me the importance of sample design, the intricacies of drawing conclusions from data, and the procedures for testing hypotheses with confidence. The practical applications and real-world examples reinforced my understanding and applicability of these statistical methods."
  },
  {
    "objectID": "posts/statistics-in-r/index.html#solidifying-knowledge-and-practical-application",
    "href": "posts/statistics-in-r/index.html#solidifying-knowledge-and-practical-application",
    "title": "Celebrating a Milestone in Statistics: A Journey through DataCamp’s ‘Statistics Fundamentals with R’ Track",
    "section": "Solidifying Knowledge and Practical Application",
    "text": "Solidifying Knowledge and Practical Application\nThe culmination of the coursework was not merely theoretical; it demanded practical application of all the learned concepts. The project “Hypothesis Testing with Men’s and Women’s Soccer Matches” was a challenging yet thrilling part of the track. It tasked me with performing a hypothesis test to determine if there were differences in goals scored between women’s and men’s soccer matches. This project was not just an academic exercise but a real-world application that honed my analytical and critical thinking skills."
  },
  {
    "objectID": "posts/statistics-in-r/index.html#benchmarking-skills-and-achieving-excellence",
    "href": "posts/statistics-in-r/index.html#benchmarking-skills-and-achieving-excellence",
    "title": "Celebrating a Milestone in Statistics: A Journey through DataCamp’s ‘Statistics Fundamentals with R’ Track",
    "section": "Benchmarking Skills and Achieving Excellence",
    "text": "Benchmarking Skills and Achieving Excellence\nThe journey concluded with a skill assessment that tested the breadth and depth of my statistical knowledge and R programming skills. Achieving a rank in the 98th percentile was a moment of great pride and a testament to the quality of learning and understanding I had gained throughout the track."
  },
  {
    "objectID": "posts/statistics-in-r/index.html#a-journey-of-empowerment-and-future-aspirations",
    "href": "posts/statistics-in-r/index.html#a-journey-of-empowerment-and-future-aspirations",
    "title": "Celebrating a Milestone in Statistics: A Journey through DataCamp’s ‘Statistics Fundamentals with R’ Track",
    "section": "A Journey of Empowerment and Future Aspirations",
    "text": "A Journey of Empowerment and Future Aspirations\nCompleting the “Statistics Fundamentals with R” track is more than just an academic achievement; it’s an empowering journey that has equipped me with the skills to make data-driven decisions and insights. It has instilled in me a profound respect for data and its potential to influence real-world outcomes.\nI am immensely grateful to Arewa Data Science Academy and DataCamp for providing this incredible learning opportunity. This experience has not only enhanced my statistical analysis and R programming skills but has also inspired me to pursue further studies in data science and contribute to the field.\nAs I reflect on this journey, I am filled with gratitude and motivation to apply these skills in real-world scenarios, continue learning, and contribute to the growing field of data science. Here’s to a future where data and analytics lead the way to innovation and informed decision-making."
  },
  {
    "objectID": "posts/sigmoid/index.html",
    "href": "posts/sigmoid/index.html",
    "title": "The Crucial Role of the Sigmoid Function and its Derivative in Machine Learning",
    "section": "",
    "text": "In the realm of machine learning, the sigmoid function stands as a foundational and indispensable element, playing a pivotal role in various applications such as neural networks, logistic regression, and decision-making models. Its characteristic S-shaped curve enables it to map input data to a bounded output range, making it particularly valuable for tasks involving classification, probability estimation, and activation functions. This essay explores the significance of the sigmoid function in machine learning and delves into the techniques for solving its derivative, elucidating its importance in optimizing learning processes."
  },
  {
    "objectID": "posts/sigmoid/index.html#introduction",
    "href": "posts/sigmoid/index.html#introduction",
    "title": "The Crucial Role of the Sigmoid Function and its Derivative in Machine Learning",
    "section": "",
    "text": "In the realm of machine learning, the sigmoid function stands as a foundational and indispensable element, playing a pivotal role in various applications such as neural networks, logistic regression, and decision-making models. Its characteristic S-shaped curve enables it to map input data to a bounded output range, making it particularly valuable for tasks involving classification, probability estimation, and activation functions. This essay explores the significance of the sigmoid function in machine learning and delves into the techniques for solving its derivative, elucidating its importance in optimizing learning processes."
  },
  {
    "objectID": "posts/sigmoid/index.html#the-sigmoid-function-a-cornerstone-of-machine-learning",
    "href": "posts/sigmoid/index.html#the-sigmoid-function-a-cornerstone-of-machine-learning",
    "title": "The Crucial Role of the Sigmoid Function and its Derivative in Machine Learning",
    "section": "The Sigmoid Function: A Cornerstone of Machine Learning",
    "text": "The Sigmoid Function: A Cornerstone of Machine Learning\nThe sigmoid function, mathematically represented as \\(\\sigma\\left(x\\right) = \\frac{1}{1+e^{-x}}\\), transforms any input value to a range between 0 and 1. This property is particularly useful in binary classification problems, where the output is interpreted as a probability score for a particular class. In the context of neural networks, sigmoid activation functions allow neurons to fire selectively, enabling complex patterns to be captured through the network’s layers. Its continuous and differentiable nature makes it suitable for various optimization algorithms, including gradient descent."
  },
  {
    "objectID": "posts/sigmoid/index.html#derivative-of-the-sigmoid-function-unraveling-the-complexity",
    "href": "posts/sigmoid/index.html#derivative-of-the-sigmoid-function-unraveling-the-complexity",
    "title": "The Crucial Role of the Sigmoid Function and its Derivative in Machine Learning",
    "section": "Derivative of the Sigmoid Function: Unraveling the Complexity",
    "text": "Derivative of the Sigmoid Function: Unraveling the Complexity\nOne of the key aspects of utilizing the sigmoid function in machine learning is the ability to compute its derivative. The derivative provides insights into how the output of the function changes concerning variations in the input. The straightforward yet intricate nature of the sigmoid derivative, often expressed as \\(\\sigma ^{,}\\left(x\\right) = \\sigma\\left(x\\right) \\times (1-\\sigma\\left(x\\right))\\), has far-reaching implications for the training of neural networks and other machine learning models."
  },
  {
    "objectID": "posts/sigmoid/index.html#solving-the-sigmoid-derivative-chain-rule-in-action",
    "href": "posts/sigmoid/index.html#solving-the-sigmoid-derivative-chain-rule-in-action",
    "title": "The Crucial Role of the Sigmoid Function and its Derivative in Machine Learning",
    "section": "Solving the Sigmoid Derivative: Chain Rule in Action",
    "text": "Solving the Sigmoid Derivative: Chain Rule in Action\nTo solve the derivative of the sigmoid function, the chain rule comes into play. The chain rule is a fundamental concept in calculus that allows us to calculate the derivative of composite functions. Applying the chain rule to the sigmoid derivative involves breaking down the complex expression into manageable components and systematically calculating their derivatives. The result,\\(\\sigma ^{,}\\left(x\\right) = \\sigma\\left(x\\right) \\times(1-\\sigma\\left(x\\right))\\) , embodies the sigmoid function’s self-adjusting behavior, where the derivative is maximized at the inflection point (0.5) and approaches zero as the input approaches positive or negative infinity. On this note, I will like to indulge in deriving the derivative of a sigmoid, just for fun.\n\\(\\sigma\\left(x\\right) = \\frac{1}{1+e^{-x}}\\)\nThis can also be expressed as:\n\\(\\sigma\\left(x\\right) = (1+e^{-x})^{-1}\\)\nwhich we know differentiates, according to the chain rule, to:\n\\(-(1+e^{-x})^{-2} \\times e^{-x} \\times-1\\)\nwhich easily becomes:\n\\(e^{-x}(1+e^{-x})^{-2}\\)\nsame as:\n\\(\\frac{e^{-x}}{\\left(1 + e ^{-x}\\right)^{2}}\\)\nthen we can rearrange by adding and subtracting 1 which in essence does nothing but add a zero:\n\\(\\frac{e^{-x}}{\\left(1 + e ^{-x}\\right)^{2}}+ 1 -1\\)\nthis easily becomes:\n\\(\\frac{1 + e^{-x}-1}{\\left(1+e^{-x}\\right)^{-2}}\\)\nwhich is simply:\n\\(\\frac{1 + e^{-x}}{\\left(1 + e ^{-x}\\right)^{2}} - \\frac{1}{\\left(1 + e ^{-x}\\right)^{2}}\\)\nwhich simplifies to:\n\\(\\frac{1}{1 + e ^{-x}} - \\frac{1}{\\left(1 + e ^{-x}\\right)^{2}}\\)\nfactorizing we get:\n\\(\\frac{1}{1 + e ^{-x}}\\left(1 - \\frac{1}{1 + e ^{-x}}\\right)\\)\nrecalling that:\n\\(\\sigma\\left(x\\right) = \\frac{1}{1+e^{-x}}\\)\nour derivative is now proven to be\n\\(\\sigma ^{,}\\left(x\\right) = \\sigma\\left(x\\right) \\times(1-\\sigma\\left(x\\right))\\)"
  },
  {
    "objectID": "posts/sigmoid/index.html#importance-of-the-sigmoid-derivative-in-optimization",
    "href": "posts/sigmoid/index.html#importance-of-the-sigmoid-derivative-in-optimization",
    "title": "The Crucial Role of the Sigmoid Function and its Derivative in Machine Learning",
    "section": "Importance of the Sigmoid Derivative in Optimization",
    "text": "Importance of the Sigmoid Derivative in Optimization\nUnderstanding and utilizing the sigmoid derivative is of paramount importance in optimizing machine learning models. In neural network training, the backpropagation algorithm relies heavily on derivatives to adjust the model’s weights and biases. The sigmoid derivative guides the network’s learning process, indicating the direction and magnitude of weight adjustments needed for minimizing error, particularly in models where the output layer is sigmoid."
  },
  {
    "objectID": "posts/sigmoid/index.html#conclusion",
    "href": "posts/sigmoid/index.html#conclusion",
    "title": "The Crucial Role of the Sigmoid Function and its Derivative in Machine Learning",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, the sigmoid function is a cornerstone of machine learning, serving as a fundamental tool for a wide range of applications. Its ability to map inputs to a bounded output range makes it indispensable in tasks such as binary classification, probability estimation, and activation functions within neural networks. Solving the derivative of the sigmoid function through the chain rule unravels its intricate nature, enabling its effective utilization in optimization processes. As machine learning continues to evolve, the sigmoid function and its derivative will undoubtedly continue to play a pivotal role in shaping the landscape of artificial intelligence and data analysis."
  },
  {
    "objectID": "posts/rprogramming/index.html",
    "href": "posts/rprogramming/index.html",
    "title": "Data Science: Foundations using R Specialization",
    "section": "",
    "text": "I recently wrapped up the Data Science: Foundations using R specialization on Coursera, a comprehensive program offered by John Hopkins University and led by Professors Roger Peng and Jeff Leek. Comprising five enriching courses, this specialization serves as the initial segment of the highly popular Data Science Specialization. Let’s delve into the key components of this journey:\n\nThe Data Scientist’s Toolbox: Inaugurating the specialization, the first course, “The Data Scientist’s Toolbox,” provides a solid foundation. Here, we explore the essential tools and resources essential for data scientists. We delve into the world of data science, defining its scope and importance. Additionally, we get hands-on experience with version control, particularly Git. The course also introduces us to R and RStudio, offering guidance on seamlessly integrating version control within RStudio. Moreover, it acquaints us with RMarkdown, a powerful tool for effective communication in data science.\nR Programming: The second course, “R Programming,” is a deep dive into the R programming language. We learn the fundamentals of R as a programming language, with practical insights gained through the use of swirl courses.\nGetting and Cleaning Data: Courses three and four further solidify the concepts introduced in the second course. These courses intensify our learning through interactive swirl lessons and real-world examples. “Getting and Cleaning Data” equips us with the skills necessary for data cleaning, a crucial step in any data science project.\nExploratory Data Analysis: In the fourth course, “Exploratory Data Analysis,” we build on our knowledge to perform insightful exploratory data analysis. The course guides us through specific examples, providing a hands-on approach to this critical aspect of data science.\nReproducible Research: The final course, “Reproducible Research,” emphasizes the importance of clear research communication. It equips us with the tools and techniques needed to ensure our research is not only transparent but also reproducible. RMarkdown plays a central role in this course, enabling us to effectively communicate our findings.\n\nOne noteworthy aspect of this specialization is the peer review assignments, with at least one per course. These assignments not only reinforce the concepts we’ve learned but also offer a platform for applying them in real-world scenarios. They are an invaluable part of the learning experience.\nIn conclusion, the Data Science: Foundations using R specialization is a rewarding journey that equips students with essential data science skills. I am immensely grateful to Professors Roger Peng and Jeff Leek for their guidance throughout this program. As someone in the field of medical sciences, I firmly believe that learning R programming is a crucial step for anyone serious about research, and this specialization has been an enlightening and enriching experience."
  },
  {
    "objectID": "posts/r-vectorized-code/index.html",
    "href": "posts/r-vectorized-code/index.html",
    "title": "Mastering Vectorized Code: Boosting Efficiency in R",
    "section": "",
    "text": "In the world of programming, efficiency is a golden goal. One clever technique to achieve this is using vectorized code. Vectorized code might sound complex, but it’s like a superhero power for your programs. Imagine if you could do tasks on a bunch of numbers all at once, without going through them one by one. That’s the magic of vectorized code! In this article, we’ll explore what vectorized code is, why it’s awesome, and how it makes your R programming faster and smoother.\nSo, what’s the secret sauce of vectorized code? It’s like having a bunch of superpowers rolled into one. Vectorized code is all about doing things at once – it’s like multitasking for your computer. Instead of looping over each number separately, you perform operations on whole groups of numbers in one go. This can make your R code run way faster and feel snappier.\nLet’s imagine we’re dealing with a bunch of numbers and we want to transform them. We’ll use two functions to do this, each with its own style. One function will be the superhero – it’ll use vectorized code. The other function will be the regular person – it won’t use vectorized code, and it’ll take longer to do the same job.\n\nThe Regular Approach: No Superpowers\n\nabs_loop &lt;- function(vec){\n  for (i in 1:length(vec)) {\n    if (vec[i] &lt; 0) {\n      vec[i] &lt;- -vec[i]\n    }\n  }\n  vec\n}\n\n\n\nEmbracing Vectorization: The Superhero\n\nabs_sets &lt;- function(vec){\n  negs &lt;- vec &lt; 0\n  vec[negs] &lt;- vec[negs] * -1\n  vec\n}\n\nNow, let’s put these functions to the test. We’ve got a bunch of numbers in a variable called long.\n\nlong &lt;- rep(c(-1, 1), 5000000)\n\nIt’s time for the showdown! We’ll measure how fast each function works.\n\n# Non vectorized\nprint(\"Nonvectorized\")\n\n[1] \"Nonvectorized\"\n\nsystem.time(abs_loop(long))\n\n   user  system elapsed \n  0.744   0.059   0.803 \n\n# Vectorized\n\nprint(\"Vectorized\")\n\n[1] \"Vectorized\"\n\nsystem.time(abs_sets(long))\n\n   user  system elapsed \n  0.198   0.091   0.289 \n\n\nThe result is in – the vectorized abs_sets() function wins the race by being around three times faster! This difference gets even bigger as we deal with larger batches of numbers. Think about the time you can save when working with big sets of data. Your code becomes a speedster, and your computer can focus on more exciting things.\nIn the world of R programming, vectorization is like a secret ingredient that gives you superpowers. Since R loves working with vectors, most R functions are already set up for vectorized operations. This means using vectorization can level up your programming skills and save you time and effort.\nTo wrap it up, diving into vectorized code is like discovering a hidden treasure. Its power to tackle tasks in one swift move, without tediously looping through each item, is a game-changer. As R programming evolves, vectorization remains a steadfast companion – a guiding star in the quest for efficient and elegant coding. So, why wait? Unleash the power of vectorized code and watch your R programming prowess soar!"
  },
  {
    "objectID": "posts/pytorch-nlp/index.html",
    "href": "posts/pytorch-nlp/index.html",
    "title": "PyTorch for Natural Language Processing",
    "section": "",
    "text": "In the dynamic field of Natural Language Processing (NLP), the quest for more robust and scalable models has led researchers and developers towards powerful, flexible tools that can handle the complexity and size of modern datasets. One such tool that has risen to prominence is PyTorch. Developed by Facebook’s AI Research lab, PyTorch offers a compelling blend of flexibility, speed, and ease of use, making it an ideal choice for NLP tasks. Having recently completed a track on DataCamp on NLP with PyTorch, I find it useful to pen down how PyTorch changes the game."
  },
  {
    "objectID": "posts/pytorch-nlp/index.html#introduction",
    "href": "posts/pytorch-nlp/index.html#introduction",
    "title": "PyTorch for Natural Language Processing",
    "section": "",
    "text": "In the dynamic field of Natural Language Processing (NLP), the quest for more robust and scalable models has led researchers and developers towards powerful, flexible tools that can handle the complexity and size of modern datasets. One such tool that has risen to prominence is PyTorch. Developed by Facebook’s AI Research lab, PyTorch offers a compelling blend of flexibility, speed, and ease of use, making it an ideal choice for NLP tasks. Having recently completed a track on DataCamp on NLP with PyTorch, I find it useful to pen down how PyTorch changes the game."
  },
  {
    "objectID": "posts/pytorch-nlp/index.html#what-is-pytorch",
    "href": "posts/pytorch-nlp/index.html#what-is-pytorch",
    "title": "PyTorch for Natural Language Processing",
    "section": "What is PyTorch?",
    "text": "What is PyTorch?\nPyTorch is an open-source machine learning library based on the Torch library, widely recognized for its simplicity and interface clarity. It excels in areas requiring automatic differentiation and dynamic neural networks, particularly in complex, evolving projects where versatility is as critical as performance. This is largely attributed to its use of dynamic computation graphs (called “define-by-run” schema), which allow changes to be made on-the-fly and graphs to be built during runtime."
  },
  {
    "objectID": "posts/pytorch-nlp/index.html#pytorchs-advantages-for-nlp",
    "href": "posts/pytorch-nlp/index.html#pytorchs-advantages-for-nlp",
    "title": "PyTorch for Natural Language Processing",
    "section": "PyTorch’s Advantages for NLP",
    "text": "PyTorch’s Advantages for NLP\nThe characteristics of PyTorch particularly beneficial for NLP include its intuitive design, ease of debugging, and seamless integration with the Python programming environment. Unlike static graphs, which need to define and optimize the entire model architecture before running, PyTorch’s dynamic graphs enable developers to alter their models as inputs change, which is especially useful for the varying sequence lengths in text data.\n\nFlexibility and Ease of Use\nFor NLP, models often need to experiment with novel ideas or hybrid architectures, and PyTorch’s flexibility ensures that researchers can implement changes almost as quickly as they can think of them. It integrates seamlessly with the Python data science stack, making it easier to turn research prototypes into production-ready code.\n\n\nRich Prebuilt Libraries\nPyTorch is supported by a rich ecosystem of libraries and extensions. Libraries such as torchtext simplify text processing and provide utilities for common tasks like tokenization, vocabulary creation, and sequence padding, allowing researchers to handle preprocessing efficiently and focus more on model development.\n\n\ntorchtext for Streamlined Text Preprocessing\nA key component in PyTorch’s NLP capabilities is torchtext, a library designed to streamline preprocessing pipelines. torchtext offers utilities for batch processing of text, making it easier to load and handle large datasets efficiently. With functionalities such as built-in vocabularies, pre-trained word vectors, and support for common datasets, torchtext significantly reduces the boilerplate code required in text preprocessing. This allows developers to rapidly experiment with different NLP models and techniques, enhancing productivity and innovation.\n\n\nCommunity and Support\nThe PyTorch community is a vibrant and growing ecosystem. With extensive documentation, tutorials, and forums, developers and researchers can easily find help and resources. The community not only contributes to the core library but also continuously adds to a growing repository of models and tools, which accelerates development and fosters innovation in NLP."
  },
  {
    "objectID": "posts/pytorch-nlp/index.html#pytorch-in-action-nlp-applications",
    "href": "posts/pytorch-nlp/index.html#pytorch-in-action-nlp-applications",
    "title": "PyTorch for Natural Language Processing",
    "section": "PyTorch in Action: NLP Applications",
    "text": "PyTorch in Action: NLP Applications\nIn NLP, PyTorch has been used to achieve state-of-the-art results in several areas:\n\nText Classification: PyTorch provides a straightforward approach for developing models for sentiment analysis, topic classification, and more.\nMachine Translation: The flexibility in sequence-to-sequence models, attention mechanisms, and memory networks has made PyTorch a popular choice for researchers working on language translation applications.\nLanguage Modeling and Generation: PyTorch supports advanced models like Transformers and GPT-n series, which are crucial for tasks that require understanding context and generating text."
  },
  {
    "objectID": "posts/pytorch-nlp/index.html#conclusion",
    "href": "posts/pytorch-nlp/index.html#conclusion",
    "title": "PyTorch for Natural Language Processing",
    "section": "Conclusion",
    "text": "Conclusion\nPyTorch’s impact on NLP is undeniable. Its design inherently supports the rapid prototyping and iterative refinement that NLP models often require. Whether you’re a seasoned data scientist or a novice in machine learning, PyTorch provides the tools to innovate and scale NLP applications effectively. As NLP continues to evolve, PyTorch’s role in driving forward the boundaries of what machines understand about human language is likely only to grow, making it an invaluable asset in any NLP developer’s toolkit."
  },
  {
    "objectID": "posts/pytorch-fellowship/index.html",
    "href": "posts/pytorch-fellowship/index.html",
    "title": "Embarking on a Deep Learning Journey with Arewa Data Science Academy’s Deep Learning with PyTorch Fellowship",
    "section": "",
    "text": "This December marked the exciting kickoff of the “Deep Learning with PyTorch” fellowship, hosted by Arewa Data Science Academy. I’m thrilled to share my journey and insights as we delve into the fascinating world of deep learning."
  },
  {
    "objectID": "posts/pytorch-fellowship/index.html#introduction",
    "href": "posts/pytorch-fellowship/index.html#introduction",
    "title": "Embarking on a Deep Learning Journey with Arewa Data Science Academy’s Deep Learning with PyTorch Fellowship",
    "section": "",
    "text": "This December marked the exciting kickoff of the “Deep Learning with PyTorch” fellowship, hosted by Arewa Data Science Academy. I’m thrilled to share my journey and insights as we delve into the fascinating world of deep learning."
  },
  {
    "objectID": "posts/pytorch-fellowship/index.html#the-fellowship-begins",
    "href": "posts/pytorch-fellowship/index.html#the-fellowship-begins",
    "title": "Embarking on a Deep Learning Journey with Arewa Data Science Academy’s Deep Learning with PyTorch Fellowship",
    "section": "The Fellowship Begins",
    "text": "The Fellowship Begins\nThe fellowship’s curriculum is centered around the comprehensive PyTorch deep learning book, a resource that’s freely available and highly recommended for enthusiasts in the field (find it here: PyTorch Deep Learning by Daniel Bourke)."
  },
  {
    "objectID": "posts/pytorch-fellowship/index.html#inspirational-instruction",
    "href": "posts/pytorch-fellowship/index.html#inspirational-instruction",
    "title": "Embarking on a Deep Learning Journey with Arewa Data Science Academy’s Deep Learning with PyTorch Fellowship",
    "section": "Inspirational Instruction",
    "text": "Inspirational Instruction\nA key highlight of this program is our instructor, Mustapha Abdullahi. Fresh from his commendable achievement of completing his master’s with honors from Queen Mary University, UK, Mustapha brings a blend of youthful vigor and profound knowledge to our virtual classroom. His ability to break down complex concepts into digestible bits has been nothing short of remarkable."
  },
  {
    "objectID": "posts/pytorch-fellowship/index.html#progress-and-personalization",
    "href": "posts/pytorch-fellowship/index.html#progress-and-personalization",
    "title": "Embarking on a Deep Learning Journey with Arewa Data Science Academy’s Deep Learning with PyTorch Fellowship",
    "section": "Progress and Personalization",
    "text": "Progress and Personalization\nOver the past three weeks, we’ve journeyed through the initial three lectures of the course material, each session bringing new insights and challenges. The structure of the program encourages continuous learning and application, with weekly assignments that are meticulously reviewed by Mustapha. His personalized feedback has been instrumental in enhancing our understanding and skills."
  },
  {
    "objectID": "posts/pytorch-fellowship/index.html#a-community-of-learners",
    "href": "posts/pytorch-fellowship/index.html#a-community-of-learners",
    "title": "Embarking on a Deep Learning Journey with Arewa Data Science Academy’s Deep Learning with PyTorch Fellowship",
    "section": "A Community of Learners",
    "text": "A Community of Learners\nWhat makes this fellowship stand out is the sense of community and collective learning. Each of us brings a unique perspective to the table, enriching discussions and collaborations. It’s been a privilege to be part of such a vibrant and passionate group of learners."
  },
  {
    "objectID": "posts/pytorch-fellowship/index.html#looking-ahead",
    "href": "posts/pytorch-fellowship/index.html#looking-ahead",
    "title": "Embarking on a Deep Learning Journey with Arewa Data Science Academy’s Deep Learning with PyTorch Fellowship",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nAs we progress through the program, I plan to document and share my experiences and learnings in this blog. Whether it’s tackling a challenging concept, celebrating a breakthrough, or sharing useful resources, I hope to provide a window into the dynamic and ever-evolving world of deep learning with PyTorch."
  },
  {
    "objectID": "posts/pytorch-fellowship/index.html#stay-connected",
    "href": "posts/pytorch-fellowship/index.html#stay-connected",
    "title": "Embarking on a Deep Learning Journey with Arewa Data Science Academy’s Deep Learning with PyTorch Fellowship",
    "section": "Stay Connected",
    "text": "Stay Connected\nFor those interested in following this journey, stay tuned to this blog. Your comments, questions, and insights are always welcome as we explore the frontiers of deep learning together."
  },
  {
    "objectID": "posts/practice-concepts/index.html",
    "href": "posts/practice-concepts/index.html",
    "title": "Practising Concepts Learnt So far in ArewaDS",
    "section": "",
    "text": "What is the output of this code and why?\n\n\n\n\npractice\n\n\n\nI selected this question as it covers so many of concepts we have covered in our Arewa Data Science 30 days of Python so far.\nIt can be summarised as follows: 1. range(1,6,2) is the sequence 1,3,5\n\narr is [1,3,5]\nThe change made to arr1 reflects in arr. So arr also becomes [1,3,5,10]\n*arr, converts arr to tuple and returns (1,3,5,10)\n\nThe question covers the following topics: - range (day 10, introduced during loops) - list comprehension (day 13) - mutation (day 5, lists) - unpacking (day 17)\nrange is a built-in function that helps in creating sequence of integers. It accepts three arguments; start, stop and step. Start is inclusive but stop is exclusive. In this question, range(1,6,2) gives the sequence of 1,3,5.\nList comprehension: The list comprehension statement, arr = [x for x in range(1,6,2)], goes through each element in the sequence, range(1,6,2), and includes it in a list. Since it has been established that range(1,6,2) is 1,3,5 then arr is [1,3,5].\nMutation: This is a concept that is hard for most Python beginners like me to grasp. arr1 = arr does not make a copy of arr and put it in arr1, rather it gives an additional name, arr1 to the object named arr. This implies that the list [1,3,5] now has two names, arr and arr1. This is how assignment works in Python. So any change done to arr1 will reflect in arr and vice versa. Hence arr1.append(10) will add ten to the same list object referenced by both arr and arr1.\nUnpacking: The final ingredient is the unpacking operator- asterisk. The functions returns arr, and this is what we need to define. Unpacking means separating the items of an iterable, in this case the elements of the list arr. The question is then, what happens to the separated elements? If only arr is run, an error is raised as the separated elements need to go into something e.g. list, tuple etc. So arr will not work but [*arr] will work as the elements will be unpacked into a list. The code surrounding the operator determines where the output will be unpacked into. In our case, arr, is given and the syntax for a tuple is an item followed by a comma. Therefore a tuple will be returned.\nIn conclusion, the answer is C for all the reasons above.\n\nEveryday I learn something new in my Data Science journey with Arewa Data Science Fellowship. Less than a month ago, I knew almost nothing in Python but right now, my confidence is increasing with each new day when it comes to my Python skills. It is a pleasure to be a part of this and I am incredibly grateful to the mentors for the opportunity to be a part of this amazing journey."
  },
  {
    "objectID": "posts/pandas2/index.html",
    "href": "posts/pandas2/index.html",
    "title": "Overview of Data Processing Using Pandas",
    "section": "",
    "text": "Data processing is an important aspect of data analysis and provides a way to generate insights and information from large datasets, and also to prepare data for use in, say, training a machine learning model. In this article, I will briefly provide an overview of how to use Pandas - a popular data manipulation library in Python - for data processing.\nPandas is a high-performance library that provides easy-to-use data structures and data analysis tools for Python. It offers powerful capabilities for data processing, including data cleaning, transformation, merging, and analysis, among others.\nOne of the core data structures in Pandas is the DataFrame, which is essentially a two-dimensional table of data. This table is indexed by rows and columns and can be thought of as a spreadsheet or a SQL table. Pandas also provides a Series data structure, which is a one-dimensional labelled array capable of holding any data type.\nThe first step in data processing with Pandas is to load data into a DataFrame. Pandas supports reading data from a variety of sources, including CSV, Excel, SQL databases, and JSON, among others. Once the data is loaded into a DataFrame, we can start exploring, cleaning, and transforming it.\nData cleaning is an important step in data processing, as datasets in most cases contain missing values, duplicate records, or incorrect data types. Pandas provides several methods for cleaning data, including dropping missing values, filling in missing values, and removing duplicates. For example, the dropna() method can be used to remove rows with missing values from a DataFrame, while the fillna() method can be used to fill in missing values with a specified value or method. The drop_duplicates() method can be used to remove duplicate records from a DataFrame based on specified columns.\nOnce the data is cleaned, we can start transforming it to extract useful information. Pandas provides several methods for data transformation, including filtering, grouping, and aggregating data. For example, the loc[] method can be used to filter rows based on a given condition, while the groupby() method can be used to group data by one or more columns and apply aggregation functions such as sum(), mean(), and count().\nMerging and joining datasets is another common task in data processing, especially when dealing with data from multiple sources. Pandas provides several methods for merging and joining datasets, including concat(), merge(), and join(). The concat() method can be used to concatenate two or more DataFrames vertically or horizontally, while the merge() method can be used to merge two DataFrames based on one or more common columns. The join() method is similar to merge(), but it is used to join two DataFrames based on their index.\nFinally, once the data is processed, we can analyze it to extract insights and information. Pandas provides several methods for data analysis, including statistical analysis, visualization, and machine learning. For example, the describe() method can be used to generate summary statistics for a DataFrame, while the plot() method can be used to create various types of visualizations, such as line charts, bar charts, and scatter plots. Pandas also integrates well with machine learning libraries such as scikit-learn, allowing us to perform various machine learning tasks on our data.\nIn conclusion, in this article, I have provided an overview of how Pandas can be used for data processing and analysis in Python. I have highlighted some of the capabilities it provides for data cleaning, transformation, merging, and analysis. Using Pandas, we can easily load, clean, transform, and analyze data, making it a valuable tool for data scientists and analysts."
  },
  {
    "objectID": "posts/optimization/index.html",
    "href": "posts/optimization/index.html",
    "title": "Advanced Optimization Techniques in Deep Learning: Mastering PyTorch",
    "section": "",
    "text": "In the journey of training deep learning models, choosing the right optimization algorithm is crucial to achieve faster convergence and better final results. While Gradient Descent has been the go-to method, there are more advanced techniques that can significantly improve model performance. In this article, we will explore these advanced optimization methods, including Stochastic Gradient Descent (SGD), Momentum, RMSProp, and Adam, and how they can be applied effectively in PyTorch."
  },
  {
    "objectID": "posts/optimization/index.html#introduction",
    "href": "posts/optimization/index.html#introduction",
    "title": "Advanced Optimization Techniques in Deep Learning: Mastering PyTorch",
    "section": "",
    "text": "In the journey of training deep learning models, choosing the right optimization algorithm is crucial to achieve faster convergence and better final results. While Gradient Descent has been the go-to method, there are more advanced techniques that can significantly improve model performance. In this article, we will explore these advanced optimization methods, including Stochastic Gradient Descent (SGD), Momentum, RMSProp, and Adam, and how they can be applied effectively in PyTorch."
  },
  {
    "objectID": "posts/optimization/index.html#understanding-optimization-in-deep-learning",
    "href": "posts/optimization/index.html#understanding-optimization-in-deep-learning",
    "title": "Advanced Optimization Techniques in Deep Learning: Mastering PyTorch",
    "section": "Understanding Optimization in Deep Learning",
    "text": "Understanding Optimization in Deep Learning\nOptimization in deep learning is about finding the best parameters (weights and biases) of a neural network. It involves minimizing a cost function, analogous to finding the lowest point in a hilly landscape. This process is iterative and requires updating the model’s parameters in a certain direction at each training step.\n\nStochastic Gradient Descent (SGD): SGD is a variation of the gradient descent algorithm. It updates the model’s parameters using only a single training example at a time. This makes the updates faster but causes the parameter path to oscillate, taking a bit longer to converge.\nMini-Batch Gradient Descent: This method strikes a balance by using a subset of the training set to perform each update. It’s faster than batch gradient descent and more stable than SGD. The mini-batches are randomly selected, ensuring that the model doesn’t see the same examples in each iteration.\nMomentum: Momentum optimization is like pushing a ball down a hill; it accumulates velocity as it rolls downhill, becoming faster and more powerful. This method helps accelerate SGD by navigating along relevant directions and dampening oscillations. It achieves this by taking into account the past gradients to update the weights. Essentially, it adds a fraction (denoted as the momentum term) of the update vector of the past step to the current update vector.\nRMSProp: Root Mean Square Propagation (RMSProp) is an adaptive learning rate method. It divides the learning rate for a weight by a running average of the magnitudes of recent gradients for that weight. This means that the learning rate gets adjusted automatically and is different for each parameter. RMSProp performs well in situations where the optimization landscape is very uneven, as it can adapt to the changing landscape accelerating the training process.\nAdam: Adaptive Moment Estimation (Adam) combines the ideas from RMSProp and Momentum. It keeps track of an exponentially decaying average of past gradients (like Momentum) and an exponentially decaying average of past squared gradients (like RMSProp). Adam calculates the adaptive learning rates for each parameter. This method is often recommended as the default optimizer due to its effectiveness in handling sparse gradients on noisy problems."
  },
  {
    "objectID": "posts/optimization/index.html#implementing-optimization-methods-in-pytorch",
    "href": "posts/optimization/index.html#implementing-optimization-methods-in-pytorch",
    "title": "Advanced Optimization Techniques in Deep Learning: Mastering PyTorch",
    "section": "Implementing Optimization Methods in PyTorch",
    "text": "Implementing Optimization Methods in PyTorch\nIn PyTorch, these optimization methods are readily available and can be easily implemented. Here’s a brief guide on how to use them:\nGradient Descent: Use torch.optim.SGD with a learning rate parameter.\nMomentum: Add the momentum parameter to the torch.optim.SGD optimizer.\nRMSProp: Use torch.optim.RMSprop, providing parameters such as learning rate and decay rate.\nAdam: Simply use torch.optim.Adam, which requires minimal tuning."
  },
  {
    "objectID": "posts/optimization/index.html#learning-rate-decay-and-scheduling",
    "href": "posts/optimization/index.html#learning-rate-decay-and-scheduling",
    "title": "Advanced Optimization Techniques in Deep Learning: Mastering PyTorch",
    "section": "Learning Rate Decay and Scheduling",
    "text": "Learning Rate Decay and Scheduling\nOver time, reducing the learning rate can help the model converge by taking smaller steps. This is particularly important in the later stages of training. PyTorch provides scheduling utilities (e.g., torch.optim.lr_scheduler) to adjust the learning rate during training. By combining these schedulers with the optimization methods, you can achieve more robust and faster convergence."
  },
  {
    "objectID": "posts/optimization/index.html#conclusion",
    "href": "posts/optimization/index.html#conclusion",
    "title": "Advanced Optimization Techniques in Deep Learning: Mastering PyTorch",
    "section": "Conclusion",
    "text": "Conclusion\nSelecting the right optimizer and learning rate schedule can drastically improve the performance of your deep learning models. While Adam is a safe and effective default choice, exploring other optimizers like Momentum and RMSProp can provide better insights into the model’s learning dynamics. Always remember, the choice of optimizer might depend on the specific characteristics of your neural network and the nature of your problem. Experimenting with different optimizers and learning rate schedules is key to finding the most efficient path to training successful deep learning models. Stay tuned for more insights and tutorials on deep learning with PyTorch.\nOur journey into mastering deep learning is well underway as we continue to participate in the Deep Learning with PyTorch fellowship with the Arewa Data Science Academy!"
  },
  {
    "objectID": "posts/numpy/index.html",
    "href": "posts/numpy/index.html",
    "title": "NumPy ndarrays: Introduction to a Powerful Tool for Scientific Computing in Python",
    "section": "",
    "text": "NumPy which is short for Numerical Python is the one of the most foundational tools for scientific computing with Python today. It is so important that many other tools like pandas utilise NumPy array objects as their basis. The most important tool in NumPy is the ndarray which is usually a fixed-size multidimensional container of items of the same type and size. Other useful tools in NumPy are the mathematical functions that support fast operations on the ndarrays without using loops as required in pure Python, and even a C API that connects NumPy with libraries written in C, C ++ or FORTRAN.\nIn this short piece, I attempt to highlight some important NumPy features that may be useful to individuals interested in scientific computing.\nFirst before using NumPy, it is important to install it first, after installing Python of course. This can be done using conda or pip. The specific codes can be found in the documentation and is dependent on the individual’s setup eg windows, linux, Mac etc. After installation, NumPy can be imported in the following way before it can be used:\nimport numpy as np\nThere are other ways of importing NumPy but the syntax above is the most widely accepted and it is advisable as it can foster collaboration. To check the package version, the following can be used:\nprint('numpy:', np.__version__)\n\nnumpy: 1.22.3"
  },
  {
    "objectID": "posts/numpy/index.html#the-numpy-array-object-the-basis-of-numpy",
    "href": "posts/numpy/index.html#the-numpy-array-object-the-basis-of-numpy",
    "title": "NumPy ndarrays: Introduction to a Powerful Tool for Scientific Computing in Python",
    "section": "The NumPy array object, the basis of NumPy",
    "text": "The NumPy array object, the basis of NumPy\nN-dimensional array object, or ndarray, is a fast, flexible container for large datasets in Python. Arrays enable performance of mathematical operations on whole blocks of data using similar syntax to the equivalent operations between scalar elements. It enables one to perform computations on indivudual elements of an ndarray without the need for a loop or list comprehension as may be required while trying to perform a similar computation using lets say a list in pure python. In addition, computations in NumPy are significantly faster than those in pure Python eg:\n\n# Comparing speed with pure Python\n\nmy_arr = np.arange(1_000_000)\n\nmy_list = list(range(1_000_000))\n\nAn ndarray and a list of the same size are declared and the below, I time a similar computation on the array and the list.\n\n%timeit my_arr2 = my_arr * 2\n\n\n%timeit my_list2 = [x * 2 for x in my_list]\n\n1.08 ms ± 22.4 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n52.4 ms ± 786 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nThe result above shows that the computation on the array is many times faster than same on the list and this is due to the power of NumPy.\nContinuing the discussion on the ndarray, it contains elements of the same data type. In addition, every array has a shape which is a tuple that gives the dimensions of the array (ie size of each dimension) , and also a dtype (data type, since the array has the same data type).\n\nCreating ndarrays\nndarrays can be created by using the array function and they can be initiated from lists or other data structures as shown below\n\n# from lists\ndata = [1,2,3,4,5,6]\narr1 = np.array(data)\narr1\n\narray([1, 2, 3, 4, 5, 6])\n\n\nThis creates an array object that contains the elements in the data list.\nWe can also input the elements directly as a list as follows:\n\narr2 = np.array([1,2,3,4,5,6])\narr2\n\narray([1, 2, 3, 4, 5, 6])\n\n\nSame array object will be created as arr1 above.\nThe functions and methods available in NumPy can be accessed by the following code:\n\ndir(np)  # assuming numpy is imported as np\n\n['ALLOW_THREADS',\n 'AxisError',\n 'BUFSIZE',\n 'CLIP',\n 'ComplexWarning',\n 'DataSource',\n 'ERR_CALL',\n 'ERR_DEFAULT',\n 'ERR_IGNORE',\n 'ERR_LOG',\n 'ERR_PRINT',\n 'ERR_RAISE',\n 'ERR_WARN',\n 'FLOATING_POINT_SUPPORT',\n 'FPE_DIVIDEBYZERO',\n 'FPE_INVALID',\n 'FPE_OVERFLOW',\n 'FPE_UNDERFLOW',\n 'False_',\n 'Inf',\n 'Infinity',\n 'MAXDIMS',\n 'MAY_SHARE_BOUNDS',\n 'MAY_SHARE_EXACT',\n 'ModuleDeprecationWarning',\n 'NAN',\n 'NINF',\n 'NZERO',\n 'NaN',\n 'PINF',\n 'PZERO',\n 'RAISE',\n 'RankWarning',\n 'SHIFT_DIVIDEBYZERO',\n 'SHIFT_INVALID',\n 'SHIFT_OVERFLOW',\n 'SHIFT_UNDERFLOW',\n 'ScalarType',\n 'Tester',\n 'TooHardError',\n 'True_',\n 'UFUNC_BUFSIZE_DEFAULT',\n 'UFUNC_PYVALS_NAME',\n 'VisibleDeprecationWarning',\n 'WRAP',\n '_CopyMode',\n '_NoValue',\n '_UFUNC_API',\n '__NUMPY_SETUP__',\n '__all__',\n '__builtins__',\n '__cached__',\n '__config__',\n '__deprecated_attrs__',\n '__dir__',\n '__doc__',\n '__expired_functions__',\n '__file__',\n '__getattr__',\n '__git_version__',\n '__loader__',\n '__name__',\n '__package__',\n '__path__',\n '__spec__',\n '__version__',\n '_add_newdoc_ufunc',\n '_distributor_init',\n '_financial_names',\n '_from_dlpack',\n '_globals',\n '_mat',\n '_pytesttester',\n '_version',\n 'abs',\n 'absolute',\n 'add',\n 'add_docstring',\n 'add_newdoc',\n 'add_newdoc_ufunc',\n 'alen',\n 'all',\n 'allclose',\n 'alltrue',\n 'amax',\n 'amin',\n 'angle',\n 'any',\n 'append',\n 'apply_along_axis',\n 'apply_over_axes',\n 'arange',\n 'arccos',\n 'arccosh',\n 'arcsin',\n 'arcsinh',\n 'arctan',\n 'arctan2',\n 'arctanh',\n 'argmax',\n 'argmin',\n 'argpartition',\n 'argsort',\n 'argwhere',\n 'around',\n 'array',\n 'array2string',\n 'array_equal',\n 'array_equiv',\n 'array_repr',\n 'array_split',\n 'array_str',\n 'asanyarray',\n 'asarray',\n 'asarray_chkfinite',\n 'ascontiguousarray',\n 'asfarray',\n 'asfortranarray',\n 'asmatrix',\n 'asscalar',\n 'atleast_1d',\n 'atleast_2d',\n 'atleast_3d',\n 'average',\n 'bartlett',\n 'base_repr',\n 'binary_repr',\n 'bincount',\n 'bitwise_and',\n 'bitwise_not',\n 'bitwise_or',\n 'bitwise_xor',\n 'blackman',\n 'block',\n 'bmat',\n 'bool8',\n 'bool_',\n 'broadcast',\n 'broadcast_arrays',\n 'broadcast_shapes',\n 'broadcast_to',\n 'busday_count',\n 'busday_offset',\n 'busdaycalendar',\n 'byte',\n 'byte_bounds',\n 'bytes0',\n 'bytes_',\n 'c_',\n 'can_cast',\n 'cast',\n 'cbrt',\n 'cdouble',\n 'ceil',\n 'cfloat',\n 'char',\n 'character',\n 'chararray',\n 'choose',\n 'clip',\n 'clongdouble',\n 'clongfloat',\n 'column_stack',\n 'common_type',\n 'compare_chararrays',\n 'compat',\n 'complex128',\n 'complex256',\n 'complex64',\n 'complex_',\n 'complexfloating',\n 'compress',\n 'concatenate',\n 'conj',\n 'conjugate',\n 'convolve',\n 'copy',\n 'copysign',\n 'copyto',\n 'core',\n 'corrcoef',\n 'correlate',\n 'cos',\n 'cosh',\n 'count_nonzero',\n 'cov',\n 'cross',\n 'csingle',\n 'ctypeslib',\n 'cumprod',\n 'cumproduct',\n 'cumsum',\n 'datetime64',\n 'datetime_as_string',\n 'datetime_data',\n 'deg2rad',\n 'degrees',\n 'delete',\n 'deprecate',\n 'deprecate_with_doc',\n 'diag',\n 'diag_indices',\n 'diag_indices_from',\n 'diagflat',\n 'diagonal',\n 'diff',\n 'digitize',\n 'disp',\n 'divide',\n 'divmod',\n 'dot',\n 'double',\n 'dsplit',\n 'dstack',\n 'dtype',\n 'e',\n 'ediff1d',\n 'einsum',\n 'einsum_path',\n 'emath',\n 'empty',\n 'empty_like',\n 'equal',\n 'errstate',\n 'euler_gamma',\n 'exp',\n 'exp2',\n 'expand_dims',\n 'expm1',\n 'expm1x',\n 'extract',\n 'eye',\n 'fabs',\n 'fastCopyAndTranspose',\n 'fft',\n 'fill_diagonal',\n 'find_common_type',\n 'finfo',\n 'fix',\n 'flatiter',\n 'flatnonzero',\n 'flexible',\n 'flip',\n 'fliplr',\n 'flipud',\n 'float128',\n 'float16',\n 'float32',\n 'float64',\n 'float_',\n 'float_power',\n 'floating',\n 'floor',\n 'floor_divide',\n 'fmax',\n 'fmin',\n 'fmod',\n 'format_float_positional',\n 'format_float_scientific',\n 'format_parser',\n 'frexp',\n 'frombuffer',\n 'fromfile',\n 'fromfunction',\n 'fromiter',\n 'frompyfunc',\n 'fromregex',\n 'fromstring',\n 'full',\n 'full_like',\n 'gcd',\n 'generic',\n 'genfromtxt',\n 'geomspace',\n 'get_array_wrap',\n 'get_include',\n 'get_printoptions',\n 'getbufsize',\n 'geterr',\n 'geterrcall',\n 'geterrobj',\n 'gradient',\n 'greater',\n 'greater_equal',\n 'half',\n 'hamming',\n 'hanning',\n 'heaviside',\n 'histogram',\n 'histogram2d',\n 'histogram_bin_edges',\n 'histogramdd',\n 'hsplit',\n 'hstack',\n 'hypot',\n 'i0',\n 'identity',\n 'iinfo',\n 'imag',\n 'in1d',\n 'index_exp',\n 'indices',\n 'inexact',\n 'inf',\n 'info',\n 'infty',\n 'inner',\n 'insert',\n 'int0',\n 'int16',\n 'int32',\n 'int64',\n 'int8',\n 'int_',\n 'intc',\n 'integer',\n 'interp',\n 'intersect1d',\n 'intp',\n 'invert',\n 'is_busday',\n 'isclose',\n 'iscomplex',\n 'iscomplexobj',\n 'isfinite',\n 'isfortran',\n 'isin',\n 'isinf',\n 'isnan',\n 'isnat',\n 'isneginf',\n 'isposinf',\n 'isreal',\n 'isrealobj',\n 'isscalar',\n 'issctype',\n 'issubclass_',\n 'issubdtype',\n 'issubsctype',\n 'iterable',\n 'ix_',\n 'kaiser',\n 'kernel_version',\n 'kron',\n 'lcm',\n 'ldexp',\n 'left_shift',\n 'less',\n 'less_equal',\n 'lexsort',\n 'lib',\n 'linalg',\n 'linspace',\n 'little_endian',\n 'load',\n 'loadtxt',\n 'log',\n 'log10',\n 'log1p',\n 'log2',\n 'logaddexp',\n 'logaddexp2',\n 'logical_and',\n 'logical_not',\n 'logical_or',\n 'logical_xor',\n 'logspace',\n 'longcomplex',\n 'longdouble',\n 'longfloat',\n 'longlong',\n 'lookfor',\n 'ma',\n 'mask_indices',\n 'mat',\n 'math',\n 'matmul',\n 'matrix',\n 'matrixlib',\n 'max',\n 'maximum',\n 'maximum_sctype',\n 'may_share_memory',\n 'mean',\n 'median',\n 'memmap',\n 'meshgrid',\n 'mgrid',\n 'min',\n 'min_scalar_type',\n 'minimum',\n 'mintypecode',\n 'mod',\n 'modf',\n 'moveaxis',\n 'msort',\n 'multiply',\n 'nan',\n 'nan_to_num',\n 'nanargmax',\n 'nanargmin',\n 'nancumprod',\n 'nancumsum',\n 'nanmax',\n 'nanmean',\n 'nanmedian',\n 'nanmin',\n 'nanpercentile',\n 'nanprod',\n 'nanquantile',\n 'nanstd',\n 'nansum',\n 'nanvar',\n 'nbytes',\n 'ndarray',\n 'ndenumerate',\n 'ndim',\n 'ndindex',\n 'nditer',\n 'negative',\n 'nested_iters',\n 'newaxis',\n 'nextafter',\n 'nonzero',\n 'not_equal',\n 'numarray',\n 'number',\n 'obj2sctype',\n 'object0',\n 'object_',\n 'ogrid',\n 'oldnumeric',\n 'ones',\n 'ones_like',\n 'os',\n 'outer',\n 'packbits',\n 'pad',\n 'partition',\n 'percentile',\n 'pi',\n 'piecewise',\n 'place',\n 'poly',\n 'poly1d',\n 'polyadd',\n 'polyder',\n 'polydiv',\n 'polyfit',\n 'polyint',\n 'polymul',\n 'polynomial',\n 'polysub',\n 'polyval',\n 'positive',\n 'power',\n 'printoptions',\n 'prod',\n 'product',\n 'promote_types',\n 'ptp',\n 'put',\n 'put_along_axis',\n 'putmask',\n 'quantile',\n 'r_',\n 'rad2deg',\n 'radians',\n 'random',\n 'ravel',\n 'ravel_multi_index',\n 'real',\n 'real_if_close',\n 'rec',\n 'recarray',\n 'recfromcsv',\n 'recfromtxt',\n 'reciprocal',\n 'record',\n 'remainder',\n 'repeat',\n 'require',\n 'reshape',\n 'resize',\n 'result_type',\n 'right_shift',\n 'rint',\n 'roll',\n 'rollaxis',\n 'roots',\n 'rot90',\n 'round',\n 'round_',\n 'row_stack',\n 's_',\n 'safe_eval',\n 'save',\n 'savetxt',\n 'savez',\n 'savez_compressed',\n 'sctype2char',\n 'sctypeDict',\n 'sctypes',\n 'searchsorted',\n 'select',\n 'set_numeric_ops',\n 'set_printoptions',\n 'set_string_function',\n 'setbufsize',\n 'setdiff1d',\n 'seterr',\n 'seterrcall',\n 'seterrobj',\n 'setxor1d',\n 'shape',\n 'shares_memory',\n 'short',\n 'show_config',\n 'sign',\n 'signbit',\n 'signedinteger',\n 'sin',\n 'sinc',\n 'single',\n 'singlecomplex',\n 'sinh',\n 'size',\n 'sometrue',\n 'sort',\n 'sort_complex',\n 'source',\n 'spacing',\n 'split',\n 'sqrt',\n 'square',\n 'squeeze',\n 'stack',\n 'std',\n 'str0',\n 'str_',\n 'string_',\n 'subtract',\n 'sum',\n 'swapaxes',\n 'sys',\n 'take',\n 'take_along_axis',\n 'tan',\n 'tanh',\n 'tensordot',\n 'test',\n 'testing',\n 'tile',\n 'timedelta64',\n 'trace',\n 'tracemalloc_domain',\n 'transpose',\n 'trapz',\n 'tri',\n 'tril',\n 'tril_indices',\n 'tril_indices_from',\n 'trim_zeros',\n 'triu',\n 'triu_indices',\n 'triu_indices_from',\n 'true_divide',\n 'trunc',\n 'typecodes',\n 'typename',\n 'ubyte',\n 'ufunc',\n 'uint',\n 'uint0',\n 'uint16',\n 'uint32',\n 'uint64',\n 'uint8',\n 'uintc',\n 'uintp',\n 'ulonglong',\n 'unicode_',\n 'union1d',\n 'unique',\n 'unpackbits',\n 'unravel_index',\n 'unsignedinteger',\n 'unwrap',\n 'use_hugepage',\n 'ushort',\n 'vander',\n 'var',\n 'vdot',\n 'vectorize',\n 'version',\n 'void',\n 'void0',\n 'vsplit',\n 'vstack',\n 'warnings',\n 'where',\n 'who',\n 'zeros',\n 'zeros_like']\n\n\nUsing the dir function is a handy way of finding the methods and functions associated with a package or even an object.\n\n\nData types for ndarrays\nIt has been established that elements in an ndarray are usually of the same data type. As such, the data type of an array can be declared while creating the array as follows:\n\narr = np.array([4,3,5,6,7], dtype = np.float64)\narr\n\narray([4., 3., 5., 6., 7.])\n\n\nAlso, conversion of data types can also be done:\n\nint_arr = arr.astype(np.int64)\nint_arr\n\narray([4, 3, 5, 6, 7])\n\n\nThe following is a list of NumPy data types: 1. int 2. float 3. bool 4. complex 5. object 6. string 7. unicode We can also specify the number of bits for some of them e.g. int8, int32, float64, complex64 etc.\n\n\nMathematical Calculations in NumPy\nNumPy has two features on ndarrays that reflect the powerful nature of the library, vectorization and broadcasting. These are not available on traditional data structures like Python lists. Vectorization to refers to element-wise calculations done on entire arrays by using simple syntax same as will be used for scalar calculations:\n\narr = np.array([[1., 2., 3.], [4., 5., 6.]])\narr\n\narray([[1., 2., 3.],\n       [4., 5., 6.]])\n\n\n\narr * arr \n\narray([[ 1.,  4.,  9.],\n       [16., 25., 36.]])\n\n\nArithmetic with scalars propagates the scalar to each element in the array:\n\n1 / arr\n\narray([[1.        , 0.5       , 0.33333333],\n       [0.25      , 0.2       , 0.16666667]])\n\n\nIn the above, 1 is divided by each of the elements in arr to return a new array of the same size as arr. This is the beauty of NumPy.\nApart from regular calculations, NumPy also supports ufuncs (universal functions) that allow element-wise computations on ndarrays. Examples include numpy.sqrt and numpy.exp. Ufuncs can be unary (acts on elements of a single array) eg numpy.sqrt or binary (takes two arrays as arguments) eg numpy.multiply, numpy.mod etc. Ufuncs are powerful and I have seen them used in a simple implementation of logistic regression.\n\n\nIndexing ndarrays\nThere are a variety of ways to index NumPy arrays but in the basic form, indexing one dimensional arrays is very similar to indexing lists in pure Python. However, since the dimensions of an ndarray can be multidimensional, it becomes more complex with increase in dimension. In addition, numpy allows for boolean indexing (using a conditional) and fancy indexing (using integer arrays). Both are not available in pure Python. An important first distinction from Python’s built-in lists is that array slices are views on the original array. This means that the data is not copied, and any modifications to the view will be reflected in the source array."
  },
  {
    "objectID": "posts/numpy/index.html#conclusion",
    "href": "posts/numpy/index.html#conclusion",
    "title": "NumPy ndarrays: Introduction to a Powerful Tool for Scientific Computing in Python",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, NumPy provides powerful tools that can be harnessed for scientific computing and this is an attempt to provide a brief intro. As I begin my data science journey, I try to document my learnings and this is one of such attempts at documentation. This is a very skeletal view of the potential possibilities with NumPy. I have barely begun to scratch the surface. I recommend checking the NumPy documentation and also practicing the concepts as we learn."
  },
  {
    "objectID": "posts/ml-ai/index.html",
    "href": "posts/ml-ai/index.html",
    "title": "Machine-Learning: Relationship With Deep Learning, AI ,and Data Science",
    "section": "",
    "text": "ml-ai-data-science\n\n\nThe picture above,taken from Microsoft’s free introduction to machine learning course, available on github, succinctly expresses how the concepts relate to one another.\nArtificial Intelligence (AI), Machine Learning (ML), Deep Learning (DL), and Data Science are related but distinct fields in computer science that involve different techniques and applications.\nAI refers to the ability of machines to perform tasks that would normally require human intelligence. These include activties such as visual perception, speech recognition, decision-making, and natural language processing. AI is a broad field that encompasses a range of techniques and applications, including rule-based systems, expert systems, neural networks, and evolutionary algorithms.\nMachine Learning (ML) is a subfield of AI that involves developing algorithms that can learn from and make predictions using data. Rather than being programmed directly with explicit rules, ML algorithms learn patterns and relationships in data through experience and iteration. Examples of ML algorithms include decision trees, random forests, and support vector machines.\nDeep Learning (DL) is a subset of ML that involves training deep neural networks, which are complex mathematical models inspired by the structure and function of the human brain. DL has enabled breakthroughs in areas such as computer vision, natural language processing, and speech recognition. Examples of DL algorithms include convolutional neural networks, recurrent neural networks, and generative adversarial networks.\nData Science is a broader field that involves extracting insights and knowledge from data using a combination of statistical analysis, machine learning, and domain expertise. Data scientists use techniques such as data mining, data visualization, and predictive modeling to extract actionable insights from data. Data science has applications in a wide range of fields, including healthcare, finance, marketing, and social science.\nIn summary, AI refers to the broader goal of creating machines that can perform human-like tasks, while machine learning and deep learning are specific techniques for achieving that goal. Data science is a broader field that involves using statistical analysis and machine learning to extract insights and knowledge from data."
  },
  {
    "objectID": "posts/matrix/index.html",
    "href": "posts/matrix/index.html",
    "title": "Matrix Multiplication: Deep Learning Insights, Implementation, and Performance in Python and R",
    "section": "",
    "text": "Matrix multiplication is a fundamental mathematical operation with wide-ranging applications in various fields, including linear algebra, computer graphics, and machine learning. In the realm of deep learning, matrix multiplication takes on a central role, serving as the backbone for critical computations within neural networks. This article explores the significance of matrix multiplication in deep learning, its intrinsic connection to neural networks, and provides comprehensive implementations using both the R and Python programming languages."
  },
  {
    "objectID": "posts/matrix/index.html#introduction",
    "href": "posts/matrix/index.html#introduction",
    "title": "Matrix Multiplication: Deep Learning Insights, Implementation, and Performance in Python and R",
    "section": "",
    "text": "Matrix multiplication is a fundamental mathematical operation with wide-ranging applications in various fields, including linear algebra, computer graphics, and machine learning. In the realm of deep learning, matrix multiplication takes on a central role, serving as the backbone for critical computations within neural networks. This article explores the significance of matrix multiplication in deep learning, its intrinsic connection to neural networks, and provides comprehensive implementations using both the R and Python programming languages."
  },
  {
    "objectID": "posts/matrix/index.html#matrix-multiplication-and-its-deep-learning-role",
    "href": "posts/matrix/index.html#matrix-multiplication-and-its-deep-learning-role",
    "title": "Matrix Multiplication: Deep Learning Insights, Implementation, and Performance in Python and R",
    "section": "Matrix Multiplication and Its Deep Learning Role:",
    "text": "Matrix Multiplication and Its Deep Learning Role:\nMatrix multiplication forms the core of many deep learning operations by efficiently transforming data. In neural networks, each layer involves a matrix of weights that is multiplied with input data, followed by activation functions. The resulting weight updates facilitate the network’s ability to discern patterns and features, underscoring the essential role of matrix multiplication in the efficiency and effectiveness of deep learning algorithms."
  },
  {
    "objectID": "posts/matrix/index.html#r-implementation",
    "href": "posts/matrix/index.html#r-implementation",
    "title": "Matrix Multiplication: Deep Learning Insights, Implementation, and Performance in Python and R",
    "section": "R Implementation:",
    "text": "R Implementation:\nThe presented R implementation of matrix multiplication offers an intuitive insight into the concept. However, its nested loop structure can be computationally inefficient for larger matrices. This implementation serves as an educational example illustrating the mechanics of matrix multiplication:\n\nrmatmul &lt;- function(a,b){\n    rows_mat1 &lt;- nrow(a)\n    cols_mat1 &lt;- ncol(a)\n    cols_mat2 &lt;- ncol(b)\n    # initialize result matrix\n    c &lt;- matrix(0,nrow=rows_mat1,ncol = cols_mat2)\n    # perform the matrix multiplication using nested loops\n    for (i in 1:rows_mat1){\n        for (j in 1:cols_mat2){\n            for (k in 1:cols_mat1){\n                c[i,j] &lt;- c[i,j] + a[i,k] * b[k,j]\n            }\n        }\n    }\n c\n}\n\nWe can now test out our new matmul function by multiplying a simple matrix with itself.\n\na &lt;- matrix(seq(1:4),nrow=2)\nb &lt;- rmatmul(a,a)\nb\n\n     [,1] [,2]\n[1,]    7   15\n[2,]   10   22\n\n\nNow we perform R regular matrix multiplication to test out the accuracy of the function\n\na %*% a\n\n     [,1] [,2]\n[1,]    7   15\n[2,]   10   22\n\n\nAnd the results are the same"
  },
  {
    "objectID": "posts/matrix/index.html#python-implementation-using-numpy",
    "href": "posts/matrix/index.html#python-implementation-using-numpy",
    "title": "Matrix Multiplication: Deep Learning Insights, Implementation, and Performance in Python and R",
    "section": "Python Implementation using NumPy:",
    "text": "Python Implementation using NumPy:\nIn the Python ecosystem, the NumPy library provides a powerful toolset for optimized matrix operations. The Python implementation showcases the usage of NumPy for efficient matrix multiplication:\n\nimport numpy as np\ndef npmatmul(a,b):\n    # Get the dimensions of the matrices\n    rows1, cols1 = a.shape\n    rows2,cols2 = b.shape\n    # Check if matrix multiplication is possible\n    if cols1 != rows2:\n        print(\"Matrix multiplication is not possible\")\n    else: \n\n    # define a matrix for the multiplication product\n        c = np.zeros((rows1,cols2))\n\n        for i in range(rows1):\n            for j in range(cols2):\n                for k in range(cols1):\n                    c[i,j] += a[i,k] * b[k,j]\n        \n    \n    return c\n\nNow it’s time to test out the numpy matmul function alongside the regular numpy matrix multiplication function, np.matmul():\n\n#  New matrix\na = np.array([[1,2],[3,4]])\nc = npmatmul(a,a)\nprint(\"\\n\", c)\n\n\n [[ 7. 10.]\n [15. 22.]]\n\nnp.matmul(a,a)\n\narray([[ 7, 10],\n       [15, 22]])"
  },
  {
    "objectID": "posts/matrix/index.html#comparing-implementations",
    "href": "posts/matrix/index.html#comparing-implementations",
    "title": "Matrix Multiplication: Deep Learning Insights, Implementation, and Performance in Python and R",
    "section": "Comparing Implementations:",
    "text": "Comparing Implementations:\n\nEase of Use:\nBoth R and Python implementations are clear and readable. However, Python’s concise syntax and libraries like NumPy make it more user-friendly.\n\n\nOptimization and Performance:\nNumPy’s implementation in Python is highly optimized, leveraging efficient low-level operations for faster computation compared to the nested loops in R.\n\n\nConciseness:\nPython’s NumPy implementation is more concise due to its vectorized operations, enabling shorter and more expressive code.\n\n\nError Handling:\nPython’s implementation includes error handling to check if matrix multiplication is feasible, offering better user feedback."
  },
  {
    "objectID": "posts/matrix/index.html#performance-comparison",
    "href": "posts/matrix/index.html#performance-comparison",
    "title": "Matrix Multiplication: Deep Learning Insights, Implementation, and Performance in Python and R",
    "section": "Performance Comparison:",
    "text": "Performance Comparison:\nFor a performance comparison, let’s consider matrix multiplication of larger matrices (e.g., 1000x1000) using both native operations and the respective libraries.\n\nR Performance:\n\n# Creating large matrices\nlarge_a &lt;- matrix(1:1000000, nrow = 1000)\nlarge_b &lt;- matrix(1000000:1, nrow = 1000)\n\n# Timing the R native matrix multiplication\nsystem.time(large_result_r &lt;- large_a %*% large_b)\n\n   user  system elapsed \n  0.766   0.010   0.775 \n\n\n\n\nPython Performance:\n\n# Creating large matrices\nlarge_a = np.arange(1, 1000001).reshape(1000, 1000)\nlarge_b = np.arange(1000000, 0, -1).reshape(1000, 1000)\n\n# Timing the NumPy matrix multiplication\nimport time\nstart_time = time.time()\nlarge_result_np = np.matmul(large_a, large_b)\nprint(\"Time taken for NumPy matrix multiplication:\", time.time() - start_time)\n\nTime taken for NumPy matrix multiplication: 3.2793290615081787"
  },
  {
    "objectID": "posts/matrix/index.html#conclusion",
    "href": "posts/matrix/index.html#conclusion",
    "title": "Matrix Multiplication: Deep Learning Insights, Implementation, and Performance in Python and R",
    "section": "Conclusion:",
    "text": "Conclusion:\nMatrix multiplication is pivotal across diverse fields, with deep learning embracing it as a fundamental building block. While both R and Python offer matrix multiplication implementations, Python’s NumPy library stands out due to its performance optimization and user-friendly interface. The nested loop approach in R is instructive but less efficient for real-world applications. Embracing optimized libraries such as NumPy in Python is crucial for achieving efficient and scalable matrix multiplication, particularly in contexts like deep learning. This article underscores the intersection of matrix multiplication, deep learning, and programming, shedding light on their interconnectedness and significance."
  },
  {
    "objectID": "posts/list-comprehensions/index.html",
    "href": "posts/list-comprehensions/index.html",
    "title": "Speed and Elegance: How Python’s List Comprehensions Outshine Traditional Loops",
    "section": "",
    "text": "Python is celebrated for its elegant syntax and the ability to express complex ideas in a few lines of code, with list comprehensions being a shining example. These powerful expressions streamline the process of creating new lists by transforming and filtering data seamlessly. While the trusty for loop has its merits, list comprehensions bring efficiency and clarity to the forefront of Python programming. Join me as we delve into the reasons that elevate list comprehensions above traditional loops, showcasing why they’re not just a tool but an essential Python idiom for any coder’s toolkit."
  },
  {
    "objectID": "posts/list-comprehensions/index.html#introduction",
    "href": "posts/list-comprehensions/index.html#introduction",
    "title": "Speed and Elegance: How Python’s List Comprehensions Outshine Traditional Loops",
    "section": "",
    "text": "Python is celebrated for its elegant syntax and the ability to express complex ideas in a few lines of code, with list comprehensions being a shining example. These powerful expressions streamline the process of creating new lists by transforming and filtering data seamlessly. While the trusty for loop has its merits, list comprehensions bring efficiency and clarity to the forefront of Python programming. Join me as we delve into the reasons that elevate list comprehensions above traditional loops, showcasing why they’re not just a tool but an essential Python idiom for any coder’s toolkit."
  },
  {
    "objectID": "posts/list-comprehensions/index.html#readability-and-conciseness",
    "href": "posts/list-comprehensions/index.html#readability-and-conciseness",
    "title": "Speed and Elegance: How Python’s List Comprehensions Outshine Traditional Loops",
    "section": "Readability and Conciseness",
    "text": "Readability and Conciseness\nOne of the main advantages of list comprehensions is their readability and conciseness. A list comprehension allows you to write a loop in a single line of code. For example, the code example below uses a list comprehension to square each number. To begin, let’s define a list of numbers.\n\nmain_list = list(range(10000000))\n\n\nopt_squared = [num**2 for num in main_list if num]\nopt_squared[-5:]\n\n[99999900000025,\n 99999920000016,\n 99999940000009,\n 99999960000004,\n 99999980000001]\n\n\nThis is not only more readable but also reduces the chance of coding errors because it’s all in one compact line.\nOn the other hand, the traditional for loop is more verbose:\n\nsquared = []\nfor i in range(len(main_list)):\n    main_list_i = main_list[i] ** 2\n    squared.append(main_list_i)\nsquared[-5:]\n\n[99999900000025,\n 99999920000016,\n 99999940000009,\n 99999960000004,\n 99999980000001]\n\n\nThe answer gotten is the same in both methods. However, more lines are involved in the traditional for loop, which increases the complexity and the potential for bugs."
  },
  {
    "objectID": "posts/list-comprehensions/index.html#performance",
    "href": "posts/list-comprehensions/index.html#performance",
    "title": "Speed and Elegance: How Python’s List Comprehensions Outshine Traditional Loops",
    "section": "Performance",
    "text": "Performance\nWhen evaluating algorithms, performance is often equated with speed—how swiftly can the algorithm accomplish its task? To quantitatively assess this aspect, we turn to the timeit module, a robust Python tool that meticulously measures the execution time of small code snippets. This approach allows us to compare the speed of list comprehensions with traditional loops under a precise and controlled benchmark.\nList comprehension performance\n\n%%timeit\nopt_squared = [num**2 for num in main_list if num]\nopt_squared[-5:]\n\n2.41 s ± 431 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\nTraditional loop performance\n\n%%timeit\nsquared = []\nfor i in range(len(main_list)):\n    main_list_i = main_list[i] ** 2\n    squared.append(main_list_i)\nsquared[-5:]\n\n2.66 s ± 72.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\nList comprehensions are generally faster than traditional for loops because they are optimized by Python’s internal C-based engine. This optimization leads to better performance, particularly noticeable when dealing with large datasets, as seen above.\nThe timeit results indicate that the list comprehension approach is faster than the traditional for loop. While the difference in this case might seems small, it becomes significant as the complexity and size of the data grow."
  },
  {
    "objectID": "posts/list-comprehensions/index.html#memory-efficiency",
    "href": "posts/list-comprehensions/index.html#memory-efficiency",
    "title": "Speed and Elegance: How Python’s List Comprehensions Outshine Traditional Loops",
    "section": "Memory Efficiency",
    "text": "Memory Efficiency\nList comprehensions can be more memory-efficient than traditional loops. They generate the required list in a single expression, which allows Python’s memory allocator to optimize its strategy. This advantage can lead to better performance in memory usage, which is critical in large-scale applications."
  },
  {
    "objectID": "posts/list-comprehensions/index.html#expressiveness-and-flexibility",
    "href": "posts/list-comprehensions/index.html#expressiveness-and-flexibility",
    "title": "Speed and Elegance: How Python’s List Comprehensions Outshine Traditional Loops",
    "section": "Expressiveness and Flexibility",
    "text": "Expressiveness and Flexibility\nList comprehensions can incorporate complex expressions and multiple conditions in a single line. The inclusion of an if statement in the list comprehension above is an excellent example of this expressiveness:\nopt_squared = [num**2 for num in main_list if num]\nHere, the if num serves as a filter to exclude falsy values (like 0) before the squaring operation. To achieve the same with a traditional loop, additional lines and conditions would be necessary."
  },
  {
    "objectID": "posts/list-comprehensions/index.html#pythonic-idiom",
    "href": "posts/list-comprehensions/index.html#pythonic-idiom",
    "title": "Speed and Elegance: How Python’s List Comprehensions Outshine Traditional Loops",
    "section": "Pythonic Idiom",
    "text": "Pythonic Idiom\nUsing list comprehensions is considered more “Pythonic,” a term that refers to the idiomatic use of Python. Python’s philosophy emphasizes simplicity and the importance of writing clear, readable, and concise code as specified in the Zen of Python. List comprehensions align perfectly with this philosophy, encouraging clean and maintainable code."
  },
  {
    "objectID": "posts/list-comprehensions/index.html#conclusion",
    "href": "posts/list-comprehensions/index.html#conclusion",
    "title": "Speed and Elegance: How Python’s List Comprehensions Outshine Traditional Loops",
    "section": "Conclusion",
    "text": "Conclusion\nWhile traditional for loops have their place and are sometimes necessary, list comprehensions offer a more elegant and efficient way to create lists based on existing iterables. They provide better performance, improved readability, and a more Pythonic approach to coding. The code example given is a practical demonstration of how a seemingly small change in syntax and approach can lead to better code performance and maintainability, which is why list comprehensions are generally preferred in Python development."
  },
  {
    "objectID": "posts/ibm-ai-eng/index.html",
    "href": "posts/ibm-ai-eng/index.html",
    "title": "Preparing for the Future of AI: My Journey with IBM’s AI Engineering Professional Certificate",
    "section": "",
    "text": "As I continue to navigate through the rich and complex landscape of artificial intelligence, I had the pleasure of completing IBM’s esteemed AI Engineering Professional Certificate program, hosted on Coursera. This immersive experience, spanning six comprehensive courses, was a deep dive into the intricacies of machine learning algorithms, TensorFlow, PyTorch, Keras, and the fascinating world of computer vision.\nMy journey was particularly steeped in the latter, culminating in a capstone project that put my newfound expertise to the test. I take immense pride in weaving another layer of accomplishment into my burgeoning portfolio of certifications, each course further endorsed by distinct skill badges, with a special one recognizing the entire professional certificate achievement.\nThis program marks my second venture with IBM, building upon the foundational knowledge I acquired in their Data Science certificate. The initial course, “Machine Learning for Python,” served as a welcome refresher, reinforcing concepts I had previously encountered.\nAs the curriculum progressed, it transitioned into the realm of deep learning, commencing with neural networks and employing the Keras framework. The choice of Keras was deliberate; its simplicity and beginner-friendly nature are perfect for those keen on rapid development and prototyping.\nSubsequent courses whisked me through the mechanics of Computer Vision and image processing. The revelation of how to deftly manipulate images using PIL and OpenCV libraries was nothing short of revelatory.\nMy skill set expanded further as I delved into constructing sophisticated deep neural networks, courtesy of PyTorch and TensorFlow. The program’s thoughtful design ensured that by the finale—a challenging capstone project—I had amassed a trove of knowledge, a nuanced understanding of frameworks like Keras, TensorFlow, and PyTorch, as well as a portfolio of mini-projects and skill badges to showcase my competencies.\nThe experience was immensely rewarding, with each course’s Jupyter notebooks serving as a solid foundation that will underpin my future endeavors in AI. For enthusiasts and aspiring professionals, I wholeheartedly endorse this professional certification. While the material might hail from an earlier era in this fast-evolving field, a commitment to refreshing the codes and staying abreast of the latest trends will ground you firmly in the essential principles of AI.\nEmbark on this journey, and you too can unlock the transformative power of AI, paving the way for a future rich with possibility."
  },
  {
    "objectID": "posts/health-informatics/index.html",
    "href": "posts/health-informatics/index.html",
    "title": "Completing the Health Informatics Specialization on Coursera",
    "section": "",
    "text": "I am excited to announce that I have completed the Health Informatics Specialization offered by Johns Hopkins University on Coursera. This learning journey, which began in November while I was in Rwanda, has taken me across different geographies—from Kaduna to Ogoja in Cross River State, and even to Senegal—and has finally concluded in Nigeria.\nThe specialization was a deep dive into the world of Health Informatics, offering a comprehensive understanding of both the social and technical dimensions of the field. It comprised five meticulously curated courses, each providing unique insights and knowledge essential for anyone interested in the intersection of healthcare and information technology.\nThe Social and Technical Context of Health Informatics: This course laid the foundation, exploring the broader contexts in which health informatics operates. It helped me appreciate the complexities involved in integrating technology into healthcare systems, especially in low-resource settings.\nLeading Change in Health Informatics: This was my favorite course, expertly taught by Dr. Ashwini Davison. It resonated deeply with me as it discussed the challenges and strategies for implementing change in healthcare systems—a critical issue for countries like Nigeria, where the adoption of electronic health records (EHRs) is still in its infancy. The course provided practical frameworks and tools for driving change, which I found incredibly valuable.\nThe Outcomes and Interventions of Health Informatics: This course offered a deep understanding of the types of interventions that can be made using health informatics and how to measure their outcomes. It provided insights into how data can be leveraged to drive decision-making and improve patient care.\nThe Data Science of Health Informatics: As someone who has been delving into data science for a while, this course was a perfect blend of my interests. It covered critical data science concepts and techniques relevant to health informatics, enhancing my ability to analyze and interpret complex health data.\nCulminating Project in Health Informatics: The final course brought everything together with a project-based approach. We were tasked with analyzing a real-world scenario involving ImprovingHealth, a non-profit healthcare organization facing challenges in implementing systematic Health-Related Social Needs (HRSN) screenings across its clinics. The project highlighted the inefficiencies in paper-based data collection methods and the lack of adequate staff training and buy-in, which were significant barriers to identifying and addressing social determinants of health for Medicare beneficiaries.\nReflecting on this journey, I realize how far-reaching the impact of health informatics can be, particularly in enhancing healthcare delivery and improving patient outcomes. The specialization has equipped me with the knowledge and skills to better understand and tackle the challenges in this field, especially in regions where healthcare systems face significant technological and infrastructural barriers.\nI look forward to applying these insights and continuing to contribute to the evolution of digital health and health informatics in meaningful ways. I am especially excited about the potential for collaboration and innovation that lies ahead.\nThank you to all the instructors and fellow learners who made this journey enriching and rewarding!\nStay tuned for more updates on my journey into health informatics and how we can leverage data for better health outcomes. 🚀"
  },
  {
    "objectID": "posts/geom-stats/index.html",
    "href": "posts/geom-stats/index.html",
    "title": "An Exploration of Geom-Stat Pairs in R Programming’s Tidyverse",
    "section": "",
    "text": "R’s tidyverse has a rich visualization ecosystem that is exemplified by the popular ggplot2. In this article, I attempt to look at the different geoms and the stats linked to them.\nBelow is a list of common geom-stat pairs in ggplot2. Each pair consists of a geometric object (geom) and a statistical transformation (stat). They are closely associated because the geom specifies how the data is visually represented, while the stat determines how the data is processed before being passed to the geom."
  },
  {
    "objectID": "posts/geom-stats/index.html#introduction",
    "href": "posts/geom-stats/index.html#introduction",
    "title": "An Exploration of Geom-Stat Pairs in R Programming’s Tidyverse",
    "section": "",
    "text": "R’s tidyverse has a rich visualization ecosystem that is exemplified by the popular ggplot2. In this article, I attempt to look at the different geoms and the stats linked to them.\nBelow is a list of common geom-stat pairs in ggplot2. Each pair consists of a geometric object (geom) and a statistical transformation (stat). They are closely associated because the geom specifies how the data is visually represented, while the stat determines how the data is processed before being passed to the geom."
  },
  {
    "objectID": "posts/geom-stats/index.html#list-of-common-geom-stat-pairs",
    "href": "posts/geom-stats/index.html#list-of-common-geom-stat-pairs",
    "title": "An Exploration of Geom-Stat Pairs in R Programming’s Tidyverse",
    "section": "List of Common Geom-Stat Pairs",
    "text": "List of Common Geom-Stat Pairs\n\n\n\n\n\n\n\n\nGeom Function\nDefault Stat\nDescription\n\n\n\n\ngeom_bar()\nstat_count\nCreates bar charts by counting occurrences in raw data.\n\n\ngeom_col()\nstat_identity\nCreates bar charts directly from precomputed data.\n\n\ngeom_histogram()\nstat_bin\nCreates histograms by binning data into ranges.\n\n\ngeom_boxplot()\nstat_boxplot\nCreates box plots summarizing data with median and quartiles.\n\n\ngeom_density()\nstat_density\nCreates density plots by estimating the distribution curve.\n\n\ngeom_smooth()\nstat_smooth\nAdds a smoothed line (e.g., LOESS, linear) to the data.\n\n\ngeom_point()\nstat_identity\nPlots points directly as provided in the data.\n\n\ngeom_line()\nstat_identity\nCreates line plots directly from the data.\n\n\ngeom_violin()\nstat_ydensity\nCreates violin plots showing the density distribution.\n\n\ngeom_pointrange()\nstat_identity\nDisplays points with ranges using precomputed data.\n\n\ngeom_ribbon()\nstat_identity\nDisplays shaded areas between ranges (e.g., confidence bands).\n\n\ngeom_path()\nstat_identity\nConnects data points in order provided.\n\n\ngeom_tile()\nstat_identity\nCreates tiles (e.g., heatmaps) based on precomputed values.\n\n\ngeom_text()\nstat_identity\nAdds text annotations to the plot.\n\n\ngeom_sf()\nstat_sf\nHandles spatial data in simple features (SF) format.\n\n\ngeom_contour()\nstat_contour\nPlots contour lines based on 2D data.\n\n\ngeom_map()\nstat_identity\nDisplays map polygons or paths from preprocessed spatial data.\n\n\ngeom_polygon()\nstat_identity\nDisplays polygons (e.g., regions).\n\n\ngeom_freqpoly()\nstat_bin\nSimilar to a histogram, but displays frequencies as lines.\n\n\ngeom_bin2d()\nstat_bin_2d\nBins 2D data into squares for heatmaps.\n\n\ngeom_hex()\nstat_binhex\nBins 2D data into hexagons for hexbin plots.\n\n\ngeom_quantile()\nstat_quantile\nAdds quantile regression lines to the plot.\n\n\ngeom_errorbar()\nstat_identity\nDisplays error bars from precomputed summary data.\n\n\ngeom_segment()\nstat_identity\nDraws line segments using precomputed start and end points."
  },
  {
    "objectID": "posts/geom-stats/index.html#what-do-these-pairs-have-in-common",
    "href": "posts/geom-stats/index.html#what-do-these-pairs-have-in-common",
    "title": "An Exploration of Geom-Stat Pairs in R Programming’s Tidyverse",
    "section": "What Do These Pairs Have in Common?",
    "text": "What Do These Pairs Have in Common?\n\nDivision of Responsibility: The stat processes or summarizes the raw data (e.g., counting, binning, calculating densities, or smoothing). The geom visualizes the processed data (e.g., as bars, lines, points, or polygons).\nInterchangeability: Most geoms can work with different stats and vice versa, though the defaults are designed for common use cases. For example, geom_bar() uses stat_count by default, but you can explicitly specify stat_identity for precomputed data.\nDefaults:Each geom has a default stat, and vice versa. These defaults are chosen to match typical use cases (e.g., geom_histogram() defaults to stat_bin).\nInput Data: Both geoms and stats rely on mappings defined in aes() to determine how variables in the dataset relate to the plot’s structure.\nCustomization:You can override the default behavior of either the geom or the stat to fit specific needs (e.g., applying a custom statistical transformation or using a different visualization)."
  },
  {
    "objectID": "posts/geom-stats/index.html#conclusion",
    "href": "posts/geom-stats/index.html#conclusion",
    "title": "An Exploration of Geom-Stat Pairs in R Programming’s Tidyverse",
    "section": "Conclusion",
    "text": "Conclusion\nGeom-stat pairs in ggplot2 represent a powerful abstraction where the stat transforms data, and the geom visualizes it. While defaults streamline common tasks, their flexibility allows users to mix and match geoms and stats for a wide variety of visualizations."
  },
  {
    "objectID": "posts/gans/index.html",
    "href": "posts/gans/index.html",
    "title": "My Adventure with Generative Adversarial Networks (GANs)",
    "section": "",
    "text": "Have you ever watched two kids play a game of “make-believe”? One creates a story, and the other tries to catch any flaws in it. If this game was a computer program, it would be called a Generative Adversarial Network or GAN for short. I’ve recently taken a deep dive into GANs with DeepLearning.ai, and here’s my take on it.\nIn a GAN, there are two parts: the Generator and the Discriminator. Imagine the Generator as a kid trying to make up stories, and the Discriminator as the friend who points out when something doesn’t make sense. The Generator then takes this feedback and tries to come up with better stories.\nThis back-and-forth game helps in creating very real-looking images, music, and more. It’s like teaching a computer to dream and imagine.\nNow, for those who’ve gone a bit deeper into GANs, you’d know that making these two parts play nicely isn’t always simple. This is where things like dice loss and Wasserstein loss (W-loss) come in.\n\nDice Loss: Without diving too deep, dice loss is a way to measure how similar two samples are. In the world of GANs, it helps in understanding how close the Generator’s “made-up” data is to real data. It’s like comparing two drawings to see how alike they are.\nWasserstein Loss (W-loss): This is another tool to make our GAN game more effective. Think of W-loss as a fancy ruler that measures the difference between the real deal and the computer’s creations. It’s super useful because it provides clearer feedback to the Generator, helping it improve faster.\n\nWhat’s cool about GANs is how they can be used in so many ways. They can make pictures clearer, create fun video game scenes, or even invent new music. But, like all powerful tools, there are challenges. GANs can be used to create “fake” videos that look real, which can be misleading.\nAfter finishing my course, I feel like I’ve unlocked a new level in the world of technology. GANs are powerful and exciting, and as they keep getting better, I can’t wait to see where we’ll go next. Remember, it’s not just about tech; it’s about how we use it to make our world more interesting and fun!"
  },
  {
    "objectID": "posts/fizzbuzz_r/index.html",
    "href": "posts/fizzbuzz_r/index.html",
    "title": "Solving the FizzBuzz Problem: Eight Creative Solutions in R Programming",
    "section": "",
    "text": "The FizzBuzz problem is a classic programming exercise that tests a developer’s ability to think logically and solve a simple but often misunderstood challenge. I have written previously about the FizzBuzz challenge here and here. In this article, we will explore ten creative solutions to the FizzBuzz problem using the R programming language. These solutions will demonstrate different approaches, from basic conditional statements to more advanced techniques, showcasing the versatility and power of R.\n\n\nThis solution follows the traditional FizzBuzz approach, utilizing conditional statements to check divisibility and printing the appropriate output.\n\n# Solution 1: Traditional Approach with Conditional Statements\nfor (i in 1:100) {\n  if (i %% 3 == 0 & i %% 5 == 0)\n    print(\"FizzBuzz\")\n  else if (i %% 3 == 0)\n    print(\"Fizz\")\n  else if (i %% 5 == 0)\n    print(\"Buzz\")\n  else\n    print(i)\n}\n\n[1] 1\n[1] 2\n[1] \"Fizz\"\n[1] 4\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 7\n[1] 8\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 11\n[1] \"Fizz\"\n[1] 13\n[1] 14\n[1] \"FizzBuzz\"\n[1] 16\n[1] 17\n[1] \"Fizz\"\n[1] 19\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 22\n[1] 23\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 26\n[1] \"Fizz\"\n[1] 28\n[1] 29\n[1] \"FizzBuzz\"\n[1] 31\n[1] 32\n[1] \"Fizz\"\n[1] 34\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 37\n[1] 38\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 41\n[1] \"Fizz\"\n[1] 43\n[1] 44\n[1] \"FizzBuzz\"\n[1] 46\n[1] 47\n[1] \"Fizz\"\n[1] 49\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 52\n[1] 53\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 56\n[1] \"Fizz\"\n[1] 58\n[1] 59\n[1] \"FizzBuzz\"\n[1] 61\n[1] 62\n[1] \"Fizz\"\n[1] 64\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 67\n[1] 68\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 71\n[1] \"Fizz\"\n[1] 73\n[1] 74\n[1] \"FizzBuzz\"\n[1] 76\n[1] 77\n[1] \"Fizz\"\n[1] 79\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 82\n[1] 83\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 86\n[1] \"Fizz\"\n[1] 88\n[1] 89\n[1] \"FizzBuzz\"\n[1] 91\n[1] 92\n[1] \"Fizz\"\n[1] 94\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 97\n[1] 98\n[1] \"Fizz\"\n[1] \"Buzz\"\n\n\n\n\n\nLeveraging the vectorized capabilities of R, this solution employs the modulo operator to check divisibility for multiple inputs simultaneously, resulting in efficient code.\n\n# Solution 2: Vectorized Solution using Modulo Operator\nnumbers &lt;- 1:100\nresult &lt;- ifelse(numbers %% 15 == 0, \"FizzBuzz\",\n         ifelse(numbers %% 3 == 0, \"Fizz\",\n           ifelse(numbers %% 5 == 0, \"Buzz\", numbers)))\nprint(result)\n\n  [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n  [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n [13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n [19] \"19\"       \"Buzz\"     \"Fizz\"     \"22\"       \"23\"       \"Fizz\"    \n [25] \"Buzz\"     \"26\"       \"Fizz\"     \"28\"       \"29\"       \"FizzBuzz\"\n [31] \"31\"       \"32\"       \"Fizz\"     \"34\"       \"Buzz\"     \"Fizz\"    \n [37] \"37\"       \"38\"       \"Fizz\"     \"Buzz\"     \"41\"       \"Fizz\"    \n [43] \"43\"       \"44\"       \"FizzBuzz\" \"46\"       \"47\"       \"Fizz\"    \n [49] \"49\"       \"Buzz\"     \"Fizz\"     \"52\"       \"53\"       \"Fizz\"    \n [55] \"Buzz\"     \"56\"       \"Fizz\"     \"58\"       \"59\"       \"FizzBuzz\"\n [61] \"61\"       \"62\"       \"Fizz\"     \"64\"       \"Buzz\"     \"Fizz\"    \n [67] \"67\"       \"68\"       \"Fizz\"     \"Buzz\"     \"71\"       \"Fizz\"    \n [73] \"73\"       \"74\"       \"FizzBuzz\" \"76\"       \"77\"       \"Fizz\"    \n [79] \"79\"       \"Buzz\"     \"Fizz\"     \"82\"       \"83\"       \"Fizz\"    \n [85] \"Buzz\"     \"86\"       \"Fizz\"     \"88\"       \"89\"       \"FizzBuzz\"\n [91] \"91\"       \"92\"       \"Fizz\"     \"94\"       \"Buzz\"     \"Fizz\"    \n [97] \"97\"       \"98\"       \"Fizz\"     \"Buzz\"    \n\n\n\n\n\nThis solution showcases the elegance of recursion by defining a recursive function that outputs the FizzBuzz sequence.\n\n# Solution 3: Recursive Function Approach\nfizzbuzz &lt;- function(n) {\n  if (n == 0)\n    return()\n  fizzbuzz(n - 1)\n  if (n %% 3 == 0 & n %% 5 == 0)\n    print(\"FizzBuzz\")\n  else if (n %% 3 == 0)\n    print(\"Fizz\")\n  else if (n %% 5 == 0)\n    print(\"Buzz\")\n  else\n    print(n)\n}\nfizzbuzz(100)\n\n[1] 1\n[1] 2\n[1] \"Fizz\"\n[1] 4\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 7\n[1] 8\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 11\n[1] \"Fizz\"\n[1] 13\n[1] 14\n[1] \"FizzBuzz\"\n[1] 16\n[1] 17\n[1] \"Fizz\"\n[1] 19\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 22\n[1] 23\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 26\n[1] \"Fizz\"\n[1] 28\n[1] 29\n[1] \"FizzBuzz\"\n[1] 31\n[1] 32\n[1] \"Fizz\"\n[1] 34\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 37\n[1] 38\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 41\n[1] \"Fizz\"\n[1] 43\n[1] 44\n[1] \"FizzBuzz\"\n[1] 46\n[1] 47\n[1] \"Fizz\"\n[1] 49\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 52\n[1] 53\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 56\n[1] \"Fizz\"\n[1] 58\n[1] 59\n[1] \"FizzBuzz\"\n[1] 61\n[1] 62\n[1] \"Fizz\"\n[1] 64\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 67\n[1] 68\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 71\n[1] \"Fizz\"\n[1] 73\n[1] 74\n[1] \"FizzBuzz\"\n[1] 76\n[1] 77\n[1] \"Fizz\"\n[1] 79\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 82\n[1] 83\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 86\n[1] \"Fizz\"\n[1] 88\n[1] 89\n[1] \"FizzBuzz\"\n[1] 91\n[1] 92\n[1] \"Fizz\"\n[1] 94\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 97\n[1] 98\n[1] \"Fizz\"\n[1] \"Buzz\"\n\n\n\n\n\nBy employing the ternary operator, this solution achieves concise code by condensing conditional statements into a single line.\n\n# Solution 4: Ternary Operators for Concise Code\nfor (i in 1:100)\n  print(ifelse(i %% 3 == 0 & i %% 5 == 0, \"FizzBuzz\",\n         ifelse(i %% 3 == 0, \"Fizz\",\n           ifelse(i %% 5 == 0, \"Buzz\", i))))\n\n[1] 1\n[1] 2\n[1] \"Fizz\"\n[1] 4\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 7\n[1] 8\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 11\n[1] \"Fizz\"\n[1] 13\n[1] 14\n[1] \"FizzBuzz\"\n[1] 16\n[1] 17\n[1] \"Fizz\"\n[1] 19\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 22\n[1] 23\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 26\n[1] \"Fizz\"\n[1] 28\n[1] 29\n[1] \"FizzBuzz\"\n[1] 31\n[1] 32\n[1] \"Fizz\"\n[1] 34\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 37\n[1] 38\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 41\n[1] \"Fizz\"\n[1] 43\n[1] 44\n[1] \"FizzBuzz\"\n[1] 46\n[1] 47\n[1] \"Fizz\"\n[1] 49\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 52\n[1] 53\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 56\n[1] \"Fizz\"\n[1] 58\n[1] 59\n[1] \"FizzBuzz\"\n[1] 61\n[1] 62\n[1] \"Fizz\"\n[1] 64\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 67\n[1] 68\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 71\n[1] \"Fizz\"\n[1] 73\n[1] 74\n[1] \"FizzBuzz\"\n[1] 76\n[1] 77\n[1] \"Fizz\"\n[1] 79\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 82\n[1] 83\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 86\n[1] \"Fizz\"\n[1] 88\n[1] 89\n[1] \"FizzBuzz\"\n[1] 91\n[1] 92\n[1] \"Fizz\"\n[1] 94\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 97\n[1] 98\n[1] \"Fizz\"\n[1] \"Buzz\"\n\n\n\n\n\nThe ifelse() function in R allows for concise and efficient conditional statements. This solution demonstrates its use to solve the FizzBuzz problem.\n\n# Solution 5: Utilizing the `ifelse()` Function\nfor (i in 1:100) {\n  result &lt;- ifelse(i %% 15 == 0, \"FizzBuzz\",\n              ifelse(i %% 3 == 0, \"Fizz\",\n                ifelse(i %% 5 == 0, \"Buzz\", i)))\n  print(result)\n}\n\n[1] 1\n[1] 2\n[1] \"Fizz\"\n[1] 4\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 7\n[1] 8\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 11\n[1] \"Fizz\"\n[1] 13\n[1] 14\n[1] \"FizzBuzz\"\n[1] 16\n[1] 17\n[1] \"Fizz\"\n[1] 19\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 22\n[1] 23\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 26\n[1] \"Fizz\"\n[1] 28\n[1] 29\n[1] \"FizzBuzz\"\n[1] 31\n[1] 32\n[1] \"Fizz\"\n[1] 34\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 37\n[1] 38\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 41\n[1] \"Fizz\"\n[1] 43\n[1] 44\n[1] \"FizzBuzz\"\n[1] 46\n[1] 47\n[1] \"Fizz\"\n[1] 49\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 52\n[1] 53\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 56\n[1] \"Fizz\"\n[1] 58\n[1] 59\n[1] \"FizzBuzz\"\n[1] 61\n[1] 62\n[1] \"Fizz\"\n[1] 64\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 67\n[1] 68\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 71\n[1] \"Fizz\"\n[1] 73\n[1] 74\n[1] \"FizzBuzz\"\n[1] 76\n[1] 77\n[1] \"Fizz\"\n[1] 79\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 82\n[1] 83\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 86\n[1] \"Fizz\"\n[1] 88\n[1] 89\n[1] \"FizzBuzz\"\n[1] 91\n[1] 92\n[1] \"Fizz\"\n[1] 94\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 97\n[1] 98\n[1] \"Fizz\"\n[1] \"Buzz\"\n\n\n\n\n\nLeveraging the power of functional programming, this solution utilizes the map() function from the purrr package to solve the FizzBuzz problem.\n\n# Solution 6: Functional Programming Approach with `purrr` Package\nlibrary(purrr)\nmap(1:100, function(i) {\n  if (i %% 3 == 0 & i %% 5 == 0)\n    return(\"FizzBuzz\")\n  else if (i %% 3 == 0)\n    return(\"Fizz\")\n  else if (i %% 5 == 0)\n    return(\"Buzz\")\n  else\n    return(i)\n})\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] \"Fizz\"\n\n[[4]]\n[1] 4\n\n[[5]]\n[1] \"Buzz\"\n\n[[6]]\n[1] \"Fizz\"\n\n[[7]]\n[1] 7\n\n[[8]]\n[1] 8\n\n[[9]]\n[1] \"Fizz\"\n\n[[10]]\n[1] \"Buzz\"\n\n[[11]]\n[1] 11\n\n[[12]]\n[1] \"Fizz\"\n\n[[13]]\n[1] 13\n\n[[14]]\n[1] 14\n\n[[15]]\n[1] \"FizzBuzz\"\n\n[[16]]\n[1] 16\n\n[[17]]\n[1] 17\n\n[[18]]\n[1] \"Fizz\"\n\n[[19]]\n[1] 19\n\n[[20]]\n[1] \"Buzz\"\n\n[[21]]\n[1] \"Fizz\"\n\n[[22]]\n[1] 22\n\n[[23]]\n[1] 23\n\n[[24]]\n[1] \"Fizz\"\n\n[[25]]\n[1] \"Buzz\"\n\n[[26]]\n[1] 26\n\n[[27]]\n[1] \"Fizz\"\n\n[[28]]\n[1] 28\n\n[[29]]\n[1] 29\n\n[[30]]\n[1] \"FizzBuzz\"\n\n[[31]]\n[1] 31\n\n[[32]]\n[1] 32\n\n[[33]]\n[1] \"Fizz\"\n\n[[34]]\n[1] 34\n\n[[35]]\n[1] \"Buzz\"\n\n[[36]]\n[1] \"Fizz\"\n\n[[37]]\n[1] 37\n\n[[38]]\n[1] 38\n\n[[39]]\n[1] \"Fizz\"\n\n[[40]]\n[1] \"Buzz\"\n\n[[41]]\n[1] 41\n\n[[42]]\n[1] \"Fizz\"\n\n[[43]]\n[1] 43\n\n[[44]]\n[1] 44\n\n[[45]]\n[1] \"FizzBuzz\"\n\n[[46]]\n[1] 46\n\n[[47]]\n[1] 47\n\n[[48]]\n[1] \"Fizz\"\n\n[[49]]\n[1] 49\n\n[[50]]\n[1] \"Buzz\"\n\n[[51]]\n[1] \"Fizz\"\n\n[[52]]\n[1] 52\n\n[[53]]\n[1] 53\n\n[[54]]\n[1] \"Fizz\"\n\n[[55]]\n[1] \"Buzz\"\n\n[[56]]\n[1] 56\n\n[[57]]\n[1] \"Fizz\"\n\n[[58]]\n[1] 58\n\n[[59]]\n[1] 59\n\n[[60]]\n[1] \"FizzBuzz\"\n\n[[61]]\n[1] 61\n\n[[62]]\n[1] 62\n\n[[63]]\n[1] \"Fizz\"\n\n[[64]]\n[1] 64\n\n[[65]]\n[1] \"Buzz\"\n\n[[66]]\n[1] \"Fizz\"\n\n[[67]]\n[1] 67\n\n[[68]]\n[1] 68\n\n[[69]]\n[1] \"Fizz\"\n\n[[70]]\n[1] \"Buzz\"\n\n[[71]]\n[1] 71\n\n[[72]]\n[1] \"Fizz\"\n\n[[73]]\n[1] 73\n\n[[74]]\n[1] 74\n\n[[75]]\n[1] \"FizzBuzz\"\n\n[[76]]\n[1] 76\n\n[[77]]\n[1] 77\n\n[[78]]\n[1] \"Fizz\"\n\n[[79]]\n[1] 79\n\n[[80]]\n[1] \"Buzz\"\n\n[[81]]\n[1] \"Fizz\"\n\n[[82]]\n[1] 82\n\n[[83]]\n[1] 83\n\n[[84]]\n[1] \"Fizz\"\n\n[[85]]\n[1] \"Buzz\"\n\n[[86]]\n[1] 86\n\n[[87]]\n[1] \"Fizz\"\n\n[[88]]\n[1] 88\n\n[[89]]\n[1] 89\n\n[[90]]\n[1] \"FizzBuzz\"\n\n[[91]]\n[1] 91\n\n[[92]]\n[1] 92\n\n[[93]]\n[1] \"Fizz\"\n\n[[94]]\n[1] 94\n\n[[95]]\n[1] \"Buzz\"\n\n[[96]]\n[1] \"Fizz\"\n\n[[97]]\n[1] 97\n\n[[98]]\n[1] 98\n\n[[99]]\n[1] \"Fizz\"\n\n[[100]]\n[1] \"Buzz\""
  },
  {
    "objectID": "posts/fizzbuzz_r/index.html#solution-1-traditional-approach-with-conditional-statements",
    "href": "posts/fizzbuzz_r/index.html#solution-1-traditional-approach-with-conditional-statements",
    "title": "Solving the FizzBuzz Problem: Eight Creative Solutions in R Programming",
    "section": "",
    "text": "This solution follows the traditional FizzBuzz approach, utilizing conditional statements to check divisibility and printing the appropriate output.\n\n# Solution 1: Traditional Approach with Conditional Statements\nfor (i in 1:100) {\n  if (i %% 3 == 0 & i %% 5 == 0)\n    print(\"FizzBuzz\")\n  else if (i %% 3 == 0)\n    print(\"Fizz\")\n  else if (i %% 5 == 0)\n    print(\"Buzz\")\n  else\n    print(i)\n}\n\n[1] 1\n[1] 2\n[1] \"Fizz\"\n[1] 4\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 7\n[1] 8\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 11\n[1] \"Fizz\"\n[1] 13\n[1] 14\n[1] \"FizzBuzz\"\n[1] 16\n[1] 17\n[1] \"Fizz\"\n[1] 19\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 22\n[1] 23\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 26\n[1] \"Fizz\"\n[1] 28\n[1] 29\n[1] \"FizzBuzz\"\n[1] 31\n[1] 32\n[1] \"Fizz\"\n[1] 34\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 37\n[1] 38\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 41\n[1] \"Fizz\"\n[1] 43\n[1] 44\n[1] \"FizzBuzz\"\n[1] 46\n[1] 47\n[1] \"Fizz\"\n[1] 49\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 52\n[1] 53\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 56\n[1] \"Fizz\"\n[1] 58\n[1] 59\n[1] \"FizzBuzz\"\n[1] 61\n[1] 62\n[1] \"Fizz\"\n[1] 64\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 67\n[1] 68\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 71\n[1] \"Fizz\"\n[1] 73\n[1] 74\n[1] \"FizzBuzz\"\n[1] 76\n[1] 77\n[1] \"Fizz\"\n[1] 79\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 82\n[1] 83\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 86\n[1] \"Fizz\"\n[1] 88\n[1] 89\n[1] \"FizzBuzz\"\n[1] 91\n[1] 92\n[1] \"Fizz\"\n[1] 94\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 97\n[1] 98\n[1] \"Fizz\"\n[1] \"Buzz\""
  },
  {
    "objectID": "posts/fizzbuzz_r/index.html#solution-2-vectorized-solution-using-modulo-operator",
    "href": "posts/fizzbuzz_r/index.html#solution-2-vectorized-solution-using-modulo-operator",
    "title": "Solving the FizzBuzz Problem: Eight Creative Solutions in R Programming",
    "section": "",
    "text": "Leveraging the vectorized capabilities of R, this solution employs the modulo operator to check divisibility for multiple inputs simultaneously, resulting in efficient code.\n\n# Solution 2: Vectorized Solution using Modulo Operator\nnumbers &lt;- 1:100\nresult &lt;- ifelse(numbers %% 15 == 0, \"FizzBuzz\",\n         ifelse(numbers %% 3 == 0, \"Fizz\",\n           ifelse(numbers %% 5 == 0, \"Buzz\", numbers)))\nprint(result)\n\n  [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n  [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n [13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n [19] \"19\"       \"Buzz\"     \"Fizz\"     \"22\"       \"23\"       \"Fizz\"    \n [25] \"Buzz\"     \"26\"       \"Fizz\"     \"28\"       \"29\"       \"FizzBuzz\"\n [31] \"31\"       \"32\"       \"Fizz\"     \"34\"       \"Buzz\"     \"Fizz\"    \n [37] \"37\"       \"38\"       \"Fizz\"     \"Buzz\"     \"41\"       \"Fizz\"    \n [43] \"43\"       \"44\"       \"FizzBuzz\" \"46\"       \"47\"       \"Fizz\"    \n [49] \"49\"       \"Buzz\"     \"Fizz\"     \"52\"       \"53\"       \"Fizz\"    \n [55] \"Buzz\"     \"56\"       \"Fizz\"     \"58\"       \"59\"       \"FizzBuzz\"\n [61] \"61\"       \"62\"       \"Fizz\"     \"64\"       \"Buzz\"     \"Fizz\"    \n [67] \"67\"       \"68\"       \"Fizz\"     \"Buzz\"     \"71\"       \"Fizz\"    \n [73] \"73\"       \"74\"       \"FizzBuzz\" \"76\"       \"77\"       \"Fizz\"    \n [79] \"79\"       \"Buzz\"     \"Fizz\"     \"82\"       \"83\"       \"Fizz\"    \n [85] \"Buzz\"     \"86\"       \"Fizz\"     \"88\"       \"89\"       \"FizzBuzz\"\n [91] \"91\"       \"92\"       \"Fizz\"     \"94\"       \"Buzz\"     \"Fizz\"    \n [97] \"97\"       \"98\"       \"Fizz\"     \"Buzz\""
  },
  {
    "objectID": "posts/fizzbuzz_r/index.html#solution-3-recursive-function-approach",
    "href": "posts/fizzbuzz_r/index.html#solution-3-recursive-function-approach",
    "title": "Solving the FizzBuzz Problem: Eight Creative Solutions in R Programming",
    "section": "",
    "text": "This solution showcases the elegance of recursion by defining a recursive function that outputs the FizzBuzz sequence.\n\n# Solution 3: Recursive Function Approach\nfizzbuzz &lt;- function(n) {\n  if (n == 0)\n    return()\n  fizzbuzz(n - 1)\n  if (n %% 3 == 0 & n %% 5 == 0)\n    print(\"FizzBuzz\")\n  else if (n %% 3 == 0)\n    print(\"Fizz\")\n  else if (n %% 5 == 0)\n    print(\"Buzz\")\n  else\n    print(n)\n}\nfizzbuzz(100)\n\n[1] 1\n[1] 2\n[1] \"Fizz\"\n[1] 4\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 7\n[1] 8\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 11\n[1] \"Fizz\"\n[1] 13\n[1] 14\n[1] \"FizzBuzz\"\n[1] 16\n[1] 17\n[1] \"Fizz\"\n[1] 19\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 22\n[1] 23\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 26\n[1] \"Fizz\"\n[1] 28\n[1] 29\n[1] \"FizzBuzz\"\n[1] 31\n[1] 32\n[1] \"Fizz\"\n[1] 34\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 37\n[1] 38\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 41\n[1] \"Fizz\"\n[1] 43\n[1] 44\n[1] \"FizzBuzz\"\n[1] 46\n[1] 47\n[1] \"Fizz\"\n[1] 49\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 52\n[1] 53\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 56\n[1] \"Fizz\"\n[1] 58\n[1] 59\n[1] \"FizzBuzz\"\n[1] 61\n[1] 62\n[1] \"Fizz\"\n[1] 64\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 67\n[1] 68\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 71\n[1] \"Fizz\"\n[1] 73\n[1] 74\n[1] \"FizzBuzz\"\n[1] 76\n[1] 77\n[1] \"Fizz\"\n[1] 79\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 82\n[1] 83\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 86\n[1] \"Fizz\"\n[1] 88\n[1] 89\n[1] \"FizzBuzz\"\n[1] 91\n[1] 92\n[1] \"Fizz\"\n[1] 94\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 97\n[1] 98\n[1] \"Fizz\"\n[1] \"Buzz\""
  },
  {
    "objectID": "posts/fizzbuzz_r/index.html#solution-4-ternary-operators-for-concise-code",
    "href": "posts/fizzbuzz_r/index.html#solution-4-ternary-operators-for-concise-code",
    "title": "Solving the FizzBuzz Problem: Eight Creative Solutions in R Programming",
    "section": "",
    "text": "By employing the ternary operator, this solution achieves concise code by condensing conditional statements into a single line.\n\n# Solution 4: Ternary Operators for Concise Code\nfor (i in 1:100)\n  print(ifelse(i %% 3 == 0 & i %% 5 == 0, \"FizzBuzz\",\n         ifelse(i %% 3 == 0, \"Fizz\",\n           ifelse(i %% 5 == 0, \"Buzz\", i))))\n\n[1] 1\n[1] 2\n[1] \"Fizz\"\n[1] 4\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 7\n[1] 8\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 11\n[1] \"Fizz\"\n[1] 13\n[1] 14\n[1] \"FizzBuzz\"\n[1] 16\n[1] 17\n[1] \"Fizz\"\n[1] 19\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 22\n[1] 23\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 26\n[1] \"Fizz\"\n[1] 28\n[1] 29\n[1] \"FizzBuzz\"\n[1] 31\n[1] 32\n[1] \"Fizz\"\n[1] 34\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 37\n[1] 38\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 41\n[1] \"Fizz\"\n[1] 43\n[1] 44\n[1] \"FizzBuzz\"\n[1] 46\n[1] 47\n[1] \"Fizz\"\n[1] 49\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 52\n[1] 53\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 56\n[1] \"Fizz\"\n[1] 58\n[1] 59\n[1] \"FizzBuzz\"\n[1] 61\n[1] 62\n[1] \"Fizz\"\n[1] 64\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 67\n[1] 68\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 71\n[1] \"Fizz\"\n[1] 73\n[1] 74\n[1] \"FizzBuzz\"\n[1] 76\n[1] 77\n[1] \"Fizz\"\n[1] 79\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 82\n[1] 83\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 86\n[1] \"Fizz\"\n[1] 88\n[1] 89\n[1] \"FizzBuzz\"\n[1] 91\n[1] 92\n[1] \"Fizz\"\n[1] 94\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 97\n[1] 98\n[1] \"Fizz\"\n[1] \"Buzz\""
  },
  {
    "objectID": "posts/fizzbuzz_r/index.html#solution-5-utilizing-the-ifelse-function",
    "href": "posts/fizzbuzz_r/index.html#solution-5-utilizing-the-ifelse-function",
    "title": "Solving the FizzBuzz Problem: Eight Creative Solutions in R Programming",
    "section": "",
    "text": "The ifelse() function in R allows for concise and efficient conditional statements. This solution demonstrates its use to solve the FizzBuzz problem.\n\n# Solution 5: Utilizing the `ifelse()` Function\nfor (i in 1:100) {\n  result &lt;- ifelse(i %% 15 == 0, \"FizzBuzz\",\n              ifelse(i %% 3 == 0, \"Fizz\",\n                ifelse(i %% 5 == 0, \"Buzz\", i)))\n  print(result)\n}\n\n[1] 1\n[1] 2\n[1] \"Fizz\"\n[1] 4\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 7\n[1] 8\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 11\n[1] \"Fizz\"\n[1] 13\n[1] 14\n[1] \"FizzBuzz\"\n[1] 16\n[1] 17\n[1] \"Fizz\"\n[1] 19\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 22\n[1] 23\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 26\n[1] \"Fizz\"\n[1] 28\n[1] 29\n[1] \"FizzBuzz\"\n[1] 31\n[1] 32\n[1] \"Fizz\"\n[1] 34\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 37\n[1] 38\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 41\n[1] \"Fizz\"\n[1] 43\n[1] 44\n[1] \"FizzBuzz\"\n[1] 46\n[1] 47\n[1] \"Fizz\"\n[1] 49\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 52\n[1] 53\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 56\n[1] \"Fizz\"\n[1] 58\n[1] 59\n[1] \"FizzBuzz\"\n[1] 61\n[1] 62\n[1] \"Fizz\"\n[1] 64\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 67\n[1] 68\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 71\n[1] \"Fizz\"\n[1] 73\n[1] 74\n[1] \"FizzBuzz\"\n[1] 76\n[1] 77\n[1] \"Fizz\"\n[1] 79\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 82\n[1] 83\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 86\n[1] \"Fizz\"\n[1] 88\n[1] 89\n[1] \"FizzBuzz\"\n[1] 91\n[1] 92\n[1] \"Fizz\"\n[1] 94\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 97\n[1] 98\n[1] \"Fizz\"\n[1] \"Buzz\""
  },
  {
    "objectID": "posts/fizzbuzz_r/index.html#solution-6-functional-programming-approach-with-purrr-package",
    "href": "posts/fizzbuzz_r/index.html#solution-6-functional-programming-approach-with-purrr-package",
    "title": "Solving the FizzBuzz Problem: Eight Creative Solutions in R Programming",
    "section": "",
    "text": "Leveraging the power of functional programming, this solution utilizes the map() function from the purrr package to solve the FizzBuzz problem.\n\n# Solution 6: Functional Programming Approach with `purrr` Package\nlibrary(purrr)\nmap(1:100, function(i) {\n  if (i %% 3 == 0 & i %% 5 == 0)\n    return(\"FizzBuzz\")\n  else if (i %% 3 == 0)\n    return(\"Fizz\")\n  else if (i %% 5 == 0)\n    return(\"Buzz\")\n  else\n    return(i)\n})\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] \"Fizz\"\n\n[[4]]\n[1] 4\n\n[[5]]\n[1] \"Buzz\"\n\n[[6]]\n[1] \"Fizz\"\n\n[[7]]\n[1] 7\n\n[[8]]\n[1] 8\n\n[[9]]\n[1] \"Fizz\"\n\n[[10]]\n[1] \"Buzz\"\n\n[[11]]\n[1] 11\n\n[[12]]\n[1] \"Fizz\"\n\n[[13]]\n[1] 13\n\n[[14]]\n[1] 14\n\n[[15]]\n[1] \"FizzBuzz\"\n\n[[16]]\n[1] 16\n\n[[17]]\n[1] 17\n\n[[18]]\n[1] \"Fizz\"\n\n[[19]]\n[1] 19\n\n[[20]]\n[1] \"Buzz\"\n\n[[21]]\n[1] \"Fizz\"\n\n[[22]]\n[1] 22\n\n[[23]]\n[1] 23\n\n[[24]]\n[1] \"Fizz\"\n\n[[25]]\n[1] \"Buzz\"\n\n[[26]]\n[1] 26\n\n[[27]]\n[1] \"Fizz\"\n\n[[28]]\n[1] 28\n\n[[29]]\n[1] 29\n\n[[30]]\n[1] \"FizzBuzz\"\n\n[[31]]\n[1] 31\n\n[[32]]\n[1] 32\n\n[[33]]\n[1] \"Fizz\"\n\n[[34]]\n[1] 34\n\n[[35]]\n[1] \"Buzz\"\n\n[[36]]\n[1] \"Fizz\"\n\n[[37]]\n[1] 37\n\n[[38]]\n[1] 38\n\n[[39]]\n[1] \"Fizz\"\n\n[[40]]\n[1] \"Buzz\"\n\n[[41]]\n[1] 41\n\n[[42]]\n[1] \"Fizz\"\n\n[[43]]\n[1] 43\n\n[[44]]\n[1] 44\n\n[[45]]\n[1] \"FizzBuzz\"\n\n[[46]]\n[1] 46\n\n[[47]]\n[1] 47\n\n[[48]]\n[1] \"Fizz\"\n\n[[49]]\n[1] 49\n\n[[50]]\n[1] \"Buzz\"\n\n[[51]]\n[1] \"Fizz\"\n\n[[52]]\n[1] 52\n\n[[53]]\n[1] 53\n\n[[54]]\n[1] \"Fizz\"\n\n[[55]]\n[1] \"Buzz\"\n\n[[56]]\n[1] 56\n\n[[57]]\n[1] \"Fizz\"\n\n[[58]]\n[1] 58\n\n[[59]]\n[1] 59\n\n[[60]]\n[1] \"FizzBuzz\"\n\n[[61]]\n[1] 61\n\n[[62]]\n[1] 62\n\n[[63]]\n[1] \"Fizz\"\n\n[[64]]\n[1] 64\n\n[[65]]\n[1] \"Buzz\"\n\n[[66]]\n[1] \"Fizz\"\n\n[[67]]\n[1] 67\n\n[[68]]\n[1] 68\n\n[[69]]\n[1] \"Fizz\"\n\n[[70]]\n[1] \"Buzz\"\n\n[[71]]\n[1] 71\n\n[[72]]\n[1] \"Fizz\"\n\n[[73]]\n[1] 73\n\n[[74]]\n[1] 74\n\n[[75]]\n[1] \"FizzBuzz\"\n\n[[76]]\n[1] 76\n\n[[77]]\n[1] 77\n\n[[78]]\n[1] \"Fizz\"\n\n[[79]]\n[1] 79\n\n[[80]]\n[1] \"Buzz\"\n\n[[81]]\n[1] \"Fizz\"\n\n[[82]]\n[1] 82\n\n[[83]]\n[1] 83\n\n[[84]]\n[1] \"Fizz\"\n\n[[85]]\n[1] \"Buzz\"\n\n[[86]]\n[1] 86\n\n[[87]]\n[1] \"Fizz\"\n\n[[88]]\n[1] 88\n\n[[89]]\n[1] 89\n\n[[90]]\n[1] \"FizzBuzz\"\n\n[[91]]\n[1] 91\n\n[[92]]\n[1] 92\n\n[[93]]\n[1] \"Fizz\"\n\n[[94]]\n[1] 94\n\n[[95]]\n[1] \"Buzz\"\n\n[[96]]\n[1] \"Fizz\"\n\n[[97]]\n[1] 97\n\n[[98]]\n[1] 98\n\n[[99]]\n[1] \"Fizz\"\n\n[[100]]\n[1] \"Buzz\""
  },
  {
    "objectID": "posts/fizzbuzz_r/index.html#solution-8-functional-reactive-programming-with-shiny",
    "href": "posts/fizzbuzz_r/index.html#solution-8-functional-reactive-programming-with-shiny",
    "title": "Solving the FizzBuzz Problem: Eight Creative Solutions in R Programming",
    "section": "Solution 8: Functional Reactive Programming with Shiny",
    "text": "Solution 8: Functional Reactive Programming with Shiny\nThis solution showcases the power of Shiny, an R package for web application development, by building an interactive FizzBuzz generator.\n# Solution 8: Functional Reactive Programmming with Shiny\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  numericInput(\"n\", \"Enter a number:\", min = 1, max = 100, value = 1),\n  verbatimTextOutput(\"result\")\n)\n\nserver &lt;- function(input, output) {\n  output$result &lt;- renderPrint({\n    if (input$n %% 3 == 0 & input$n %% 5 == 0)\n      return(\"FizzBuzz\")\n    else if (input$n %% 3 == 0)\n      return(\"Fizz\")\n    else if (input$n %% 5 == 0)\n      return(\"Buzz\")\n    else\n      return(input$n)\n  })\n}\n\nshinyApp(ui, server)"
  },
  {
    "objectID": "posts/first-shiny/index.html",
    "href": "posts/first-shiny/index.html",
    "title": "Harnessing the Power of R Shiny for Interactive Data Visualization",
    "section": "",
    "text": "In the realm of data science and statistical computing, R has long been a staple for analysts and researchers. However, the R ecosystem extends far beyond static analyses and plots, entering the dynamic world of web applications with R Shiny. This powerful framework allows users to build interactive web applications directly from R, enabling end-users to interact with their data analyses and visualizations in real-time."
  },
  {
    "objectID": "posts/first-shiny/index.html#introduction",
    "href": "posts/first-shiny/index.html#introduction",
    "title": "Harnessing the Power of R Shiny for Interactive Data Visualization",
    "section": "",
    "text": "In the realm of data science and statistical computing, R has long been a staple for analysts and researchers. However, the R ecosystem extends far beyond static analyses and plots, entering the dynamic world of web applications with R Shiny. This powerful framework allows users to build interactive web applications directly from R, enabling end-users to interact with their data analyses and visualizations in real-time."
  },
  {
    "objectID": "posts/first-shiny/index.html#what-is-r-shiny",
    "href": "posts/first-shiny/index.html#what-is-r-shiny",
    "title": "Harnessing the Power of R Shiny for Interactive Data Visualization",
    "section": "What is R Shiny?",
    "text": "What is R Shiny?\nR Shiny is an R package that makes it straightforward to build interactive web apps straight from R. Without needing to know HTML, CSS, or JavaScript, data scientists can create applications that allow users to interact with their data, change parameters, and visualize the results instantly."
  },
  {
    "objectID": "posts/first-shiny/index.html#key-features-of-r-shiny",
    "href": "posts/first-shiny/index.html#key-features-of-r-shiny",
    "title": "Harnessing the Power of R Shiny for Interactive Data Visualization",
    "section": "Key Features of R Shiny:",
    "text": "Key Features of R Shiny:\n\nInteractivity: Users can interact with the application, altering inputs and immediately seeing the output change.\nAccessibility: Shiny apps can be hosted on a webpage, making your R analysis accessible to anyone with internet access.\nCustomization: While Shiny makes it easy to get started with default settings, it also allows for extensive customization for more advanced users."
  },
  {
    "objectID": "posts/first-shiny/index.html#getting-started-with-r-shiny",
    "href": "posts/first-shiny/index.html#getting-started-with-r-shiny",
    "title": "Harnessing the Power of R Shiny for Interactive Data Visualization",
    "section": "Getting Started with R Shiny:",
    "text": "Getting Started with R Shiny:\nTo begin, you need to install the Shiny package from CRAN:\ninstall.packages(\"shiny\")\nThen, you can create a simple Shiny app with just a few lines of code:\nlibrary(shiny)\n\n# Define UI for application\nui &lt;- fluidPage(\n   titlePanel(\"My First Shiny App\"),\n   sidebarLayout(\n      sidebarPanel(\n         sliderInput(\"num\", \n                     \"Choose a number:\", \n                     min = 1, \n                     max = 100, \n                     value = 50)\n      ),\n      mainPanel(\n         textOutput(\"selectedNum\")\n      )\n   )\n)\n\n# Define server logic\nserver &lt;- function(input, output) {\n   output$selectedNum &lt;- renderText({ \n       paste(\"You selected\", input$num)\n   })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\nThis app creates a slider for the user to select a number and immediately displays the selected number on the screen."
  },
  {
    "objectID": "posts/first-shiny/index.html#challenges-and-limitations",
    "href": "posts/first-shiny/index.html#challenges-and-limitations",
    "title": "Harnessing the Power of R Shiny for Interactive Data Visualization",
    "section": "Challenges and Limitations:",
    "text": "Challenges and Limitations:\nWhile R Shiny is an incredibly powerful tool, it does come with its challenges and limitations:\n\nPerformance: Heavy computational tasks can slow down your app, affecting user experience.\nScalability: Shiny apps can require significant resources for multiple users, which can be a challenge for large-scale applications.\nLearning Curve: For those unfamiliar with web development concepts, there might be a learning curve."
  },
  {
    "objectID": "posts/first-shiny/index.html#conclusion",
    "href": "posts/first-shiny/index.html#conclusion",
    "title": "Harnessing the Power of R Shiny for Interactive Data Visualization",
    "section": "Conclusion:",
    "text": "Conclusion:\nR Shiny offers a bridge between data analysis in R and interactive web applications, empowering data scientists to share their insights in a more dynamic and accessible way. Despite its challenges, the benefits of creating interactive and user-friendly applications make R Shiny a valuable tool in the data scientist’s toolkit."
  },
  {
    "objectID": "posts/effective-writing/index.html",
    "href": "posts/effective-writing/index.html",
    "title": "Enhancing Effective Writing: Insights from Larry McEnerney’s Leadership Lab",
    "section": "",
    "text": "Larry McEnerney’s Leadership Lab on effective writing delivered on May 9, 2014 began with a compelling assertion: “Shift your focus from rules to readers.” This succinctly encapsulates his core message, a crucial takeaway for the audience composed mainly of academic researchers. McEnerney aimed to dismantle prevalent misconceptions about writing, challenging notions that view it merely as a tool for conveying complex ideas or a means of thinking. While acknowledging these as valid purposes, he asserted that the primary goal of writing is to transform readers’ perspectives or worldviews, positioning the reader at the heart of the writing process.\nCentral to McEnerney’s discourse was the concept of creating writing that holds inherent value for the reader, ultimately driving change and influencing their ideas. He delved into the notion of knowledge, asserting its boundless nature and the role of field-specific experts in shaping valuable contributions. Convincing this community of experts, he emphasized, is a critical objective in writing.\nMcEnerney underscored the importance of understanding the community’s dynamics and norms, urging writers to “know the code.” This involves skillfully presenting information in a manner acceptable to the community, even incorporating praise before presenting arguments. Identifying key influencers within the community and catering to their preferences emerged as a pivotal strategy.\nThe workshop also deconstructed common motivations for writing, highlighting the nuanced nature of these reasons. McEnerney contended that while additional motives exist, academic writing’s core purpose remains convincing the community of experts.\nAn important principle that McEnerney tackled was the necessity of introducing valuable new knowledge. He transitioned to discussing the writing process itself, challenging the conventional approach of starting with a background and definitions. Instead, he advocated for commencing the introduction by outlining the problem, injecting a sense of instability and tension. Such an approach, he argued, captures readers’ attention and sets the stage for proposing elegant solutions.\nFurthermore, McEnerney advocated employing language that accentuates instability and tension to engage readers. He suggested dramatizing issues before presenting refined solutions, catering to readers’ preference for tension-filled narratives. Additionally, he advised incorporating the language of cost and benefit, underlining the problem’s consequences in terms of monetary or other values, as well as emphasizing the benefits of adopting the proposed solution.\nIn the context of literature reviews, McEnerney positioned them as a tool to enrich and contextualize the problem. He encouraged using terms like “knowledge gap” to convey the notion that knowledge is expandable, rather than bounded. The key, he stressed, lies in demonstrating a solution’s value to the expert community, ensuring its integration into the broader body of knowledge.\nIn summary, McEnerney’s workshop wasn’t a guide to writing formats or grammar rules; its aim was to reshape the audience’s approach to writing, placing readers at the forefront. Paramount considerations included the target journal, the expert community, and the academic field’s constituents. This approach, he asserted, significantly elevates writing quality and enhances the chances of successful journal submissions.\nFor those interested, the full video of the workshop can be accessed here."
  },
  {
    "objectID": "posts/dli-24-day-07/index.html",
    "href": "posts/dli-24-day-07/index.html",
    "title": "Reflecting on the Final Day of the Deep Learning Indaba 2024: A Journey of Insights and Inspiration",
    "section": "",
    "text": "As the saying goes, every beginning has an end, and the final day of the Deep Learning Indaba 2024 marked the conclusion of a week filled with knowledge-sharing, networking, and inspiration. This year’s Indaba was a true testament to the diversity and depth of AI conversations shaping Africa’s future. The final day was no exception, featuring several concurrent half-day workshops that challenged us to choose just one path to follow!\nThe workshop options were diverse, each promising to delve into a unique aspect of AI and its potential impact:\n\nEnvisioning Humans in a Safe, AI Future\nGeoAI for a Sustainable Future in Africa\nEmpowering Africa’s AI Future through High Performance and Cloud Computing\nAI Regulation in Africa: Rationale, Considerations, and Way Forward\n\nI opted for the “Envisioning Humans in a Safe, AI Future” workshop, a choice that proved to be the right one for me. However, I must admit it was a tough call—each workshop offered something valuable, and it was unfortunate that I couldn’t attend them all. Alas, one can only be in one place at a time!\nThe workshop began with an introduction to ML Commons, an open initiative that aims to accelerate machine learning research, and ways to get involved. This set the stage for the discussions that followed, starting with a panel discussion on “Seeking Safer Spaces with AI Tools.” The panelists brought forward intriguing perspectives on how we can create AI tools that foster safer and more inclusive environments. The discussions were not only thought-provoking but also a call to action for all of us in the AI community.\nFollowing the panel, we moved on to a segment titled “Behind the Research: Author Insights.” This session provided a deeper look into the journeys and motivations of researchers working at the intersection of AI and social good. It was inspiring to hear firsthand about the challenges they faced, the breakthroughs they achieved, and the lessons they learned along the way.\nThe workshop wrapped up with some final announcements and reflections on the discussions held throughout the day. As a token of appreciation for our engagement in this packed and event-studded session, all participants received certificates of attendance—a wonderful keepsake to remember this enriching experience.\nAfter the workshops, the Wangari Maathai Impact Award was presented to Grace Muthoni for her groundbreaking research. Her work stood out among many, and it was heartening to see such talent being recognized and celebrated. Congratulations to Grace!\nThe day then transitioned into the final Indaba panel discussion, which served as a comprehensive summary of the entire event. The panel, composed of notable experts and thought leaders, tackled critical themes such as the need for skilling, foundational research, and the concept of human-in-the-loop AI—all while emphasizing the importance of not losing our unique identities as we navigate the evolving AI landscape.\nOne of the most compelling parts of the discussion was the exploration of future research directions, including the relationship between language and machine learning education, and AI’s potential role in tackling climate change. It was an important reminder that while it’s easy to get caught up in trending topics, our research should be driven by genuine curiosity and the unique challenges we wish to solve.\nA major takeaway from the panel was the affirmation that our experiences are uniquely ours and that they are a valuable asset in shaping our paths in AI. No one else can walk our path for us, and leveraging our unique backgrounds and perspectives can lead to innovative and impactful contributions to the field.\nThroughout the week, I’ve been sharing my experiences from Indaba, as encouraged, and it has been truly inspiring to see the passion and commitment of AI practitioners across the continent.\nThe final day concluded with a vibrant closing ceremony, where prizes were awarded to deserving winners, followed by a celebratory closing party that was filled with joy, music, and a sense of accomplishment. With that, Deep Learning Indaba 2024 came to a close.\nBut this isn’t goodbye—it’s simply “see you next year.” The next Indaba will be held in Kigali, Rwanda, and I’m already looking forward to the incredible conversations and experiences that await us there.\nStay tuned, and see you in Kigali next year!"
  },
  {
    "objectID": "posts/dli-24-day-05/index.html",
    "href": "posts/dli-24-day-05/index.html",
    "title": "Deep Learning Indaba 2024:Day 5, A Day of Insights, Innovation, and Inspiration",
    "section": "",
    "text": "Day 5 of the Deep Learning Indaba 2024 was filled with moments that highlighted the vibrancy and depth of AI research and innovation happening across Africa. The day began with a celebration of academic excellence as Fiskani Banda received the Grace Alele-Williams Masters Award for her outstanding master’s thesis. Her dedication to pushing the boundaries of knowledge is a true inspiration for all of us in the AI community.\nFollowing this uplifting moment, we moved into a thought-provoking keynote by Dr. Judy Gichoya, Associate Professor of Radiology and Informatics at Emory University. Dr. Gichoya’s presentation, “Shortcuts Causing Bias in Medical Imaging,” was a deep dive into the often-overlooked issue of shortcut learning in AI. She skillfully unpacked how biases can stem from both dataset limitations and the nature of discriminative learning itself. This session resonated with me profoundly, particularly given my interest in the intersection of AI and health. It served as a powerful reminder of the need for transparency and understanding in AI models, especially when they are applied to critical fields like healthcare where biases can have real-world consequences.\nThe day’s agenda then shifted to a series of research showcases, demonstrating the diverse and impactful AI work being done across the continent. From using pre-trained models for essay scoring to flood prediction models and innovative approaches to predicting and preventing farmer-herder conflicts, the research presentations covered a wide array of domains. Each project underscored the potential of AI to address both local and global challenges, reaffirming the importance of inclusive and context-aware AI development.\nI also had the opportunity to attend the Data Science for Health Ideathon. This was a dynamic and engaging session that brought together bright minds to brainstorm and propose innovative AI solutions for health challenges in Africa. The creativity and passion in the room were palpable, making it one of the most energizing parts of the day.\nThe afternoon featured a panel discussion and presentations from African startups, many of which are harnessing AI to tackle pressing issues on the continent. It’s always inspiring to see how these emerging companies are pushing the envelope and driving impact through AI.\nAnother significant session was the Faculty Engagement discussion, a first at Indaba. Faculty members from universities across Africa and beyond came together to share insights, challenges, and solutions for advancing AI education and research on the continent. It was a meaningful conversation, and I’m excited to hear that this will be a recurring feature at future Indabas.\nBetween sessions, I made sure to network with other attendees, exchanging ideas and learning about their work. I also visited the booths of various Indaba partners, gaining a deeper understanding of the incredible projects and initiatives they are supporting.\nAs the day concluded, I found myself reflecting on how each day at the Indaba seems to surpass the last. The experiences, the learning, and the connections all continue to make this journey incredibly rewarding. I can’t wait to see what the next days have in store!\nStay tuned for more updates from Deep Learning Indaba 2024!"
  },
  {
    "objectID": "posts/dli-24-day-03/index.html",
    "href": "posts/dli-24-day-03/index.html",
    "title": "Deep Learning Indaba 2024:Day 3, A Day of Insights, Innovation, and Inspiration",
    "section": "",
    "text": "Attending the Deep Learning Indaba 2024 in Dakar has been a whirlwind of learning, networking, and inspiration. Day 3 was particularly packed with enriching activities and thought-provoking sessions that left me energized and inspired.\nThe day began with a compelling keynote by Samy Bengio on “Learning to Reason.” Samy delved deep into the challenges that transformer models still face when it comes to reasoning. While transformers have revolutionized natural language processing, their ability to reason like humans remains limited. Samy presented some innovative methods to enhance this reasoning ability, sparking a lively discussion among attendees. It was fascinating to see how research continues to push the boundaries of what AI can achieve.\nAfter the keynote, I attended a hands-on practical session on Federated Learning. Having spent the last two days tutoring, it was refreshing to be on the learning side again. We explored how federated learning can play a crucial role in fostering responsible AI, particularly by ensuring data privacy and security. We also got hands-on with Flower, an open-source framework for building federated learning systems, and discussed the added complexities that arise when working with heterogeneous datasets. Thanks to my prior experience with the DeepLearning.AI Federated Learning short course, I could dive deeper into the challenges and opportunities in this space.\nNext, I joined a career mentorship session with Sekou Remy from IBM. This was one of the highlights of my day, as Sekou shared invaluable advice on navigating one’s career in AI. Some key takeaways included:\n\nMake decisions based on the evidence you can gather while planning your future.\nTake advantage of internships during undergrad and grad school; they are invaluable for gaining experience.\nFind a balance between acquiring new knowledge (exploration) and applying what you know (exploitation).\nTime is your most valuable asset—use it wisely.\nAI offers significant potential in data capture and analysis.\nAlways keep an open mind and be willing to learn.\nNetworking is not just a buzzword; it is key to professional growth.\n\nThis session was a reminder of the importance of being strategic, adaptable, and proactive in our career journeys.\nAfter that, I attended a fascinating session on “Multimodal Perception for Human and Animal Behavior Understanding,” led by Marwa Mahmoud from the University of Glasgow’s Behavioural AI Laboratory. Marwa discussed her work on the early detection of neurodevelopmental conditions in children using multimodal features, highlighting the critical need for explainable and interpretable models. Given that these models are designed to assist healthcare providers, transparency in AI decisions is paramount. She also touched on the use of probabilistic models as a first line of support in decision-making.\nWhat struck me was the complexity and subjectivity involved in applying AI to psychology and behavior analysis, where getting well-labeled datasets can be incredibly challenging. Marwa also provided a glimpse into her work on modeling animal behavior, which can be even more daunting than human behavior. She referenced her paper, “Going Deeper than Tracking: A Survey of Computer Vision-Based Recognition of Animal Pain and Affective States,” which further explores these complexities.\nTo wrap up the day, I joined discussions on community programs, featuring inspiring talks from various AI communities like GhanaAI, Masakhane, and others. They shared their initiatives and the tangible impacts they’ve made in their regions. It was a powerful reminder of the importance of community in driving AI forward, especially in underrepresented regions. The discussions were filled with passion and a shared vision of using AI for good.\nThroughout the day, I also had the opportunity to network with fellow attendees, exchange ideas, and work on our hackathon submission. The energy and camaraderie in the room were palpable, and it was exciting to be part of a community so dedicated to pushing the boundaries of AI.\nDay 3 at Deep Learning Indaba 2024 was truly a day of insights, innovation, and inspiration—a perfect blend of learning, sharing, and growing together. I am looking forward to what the rest of the week will bring!"
  },
  {
    "objectID": "posts/dli-24-day-01/index.html",
    "href": "posts/dli-24-day-01/index.html",
    "title": "Deep Learning Indaba 2024: Day 1 Highlights and Reflections",
    "section": "",
    "text": "I’m thrilled to share my experience from the Deep Learning Indaba 2024, currently happening in Dakar, Senegal, from September 1st to 7th. This event brings together some of the most talented AI minds from across Africa, creating a vibrant and inspiring environment for knowledge exchange, collaboration, and innovation."
  },
  {
    "objectID": "posts/dli-24-day-01/index.html#day-1-a-journey-into-probabilistic-programming",
    "href": "posts/dli-24-day-01/index.html#day-1-a-journey-into-probabilistic-programming",
    "title": "Deep Learning Indaba 2024: Day 1 Highlights and Reflections",
    "section": "Day 1: A Journey into Probabilistic Programming",
    "text": "Day 1: A Journey into Probabilistic Programming\nDay 1 kicked off with an exciting mix of registration, introductions, and a variety of sessions that set the tone for the rest of the week. I was honored to lead the practical session on Probabilistic Programming, where I had the chance to tutor participants on essential concepts such as probability, probability distributions, bayes theoerem, and their applications in machine learning.\nThe session was incredibly rewarding. The participants’ engagement and enthusiasm were evident as we dove deep into understanding how probabilistic thinking forms the backbone of many AI models. It was inspiring to see so many bright minds eager to learn and apply these concepts. If you’d like to explore the notebooks (two parts, both parts in english and french) we used for this session, it’s available on GitHub: Probabilistic Thinking and Programming Notebook."
  },
  {
    "objectID": "posts/dli-24-day-01/index.html#connecting-with-the-community",
    "href": "posts/dli-24-day-01/index.html#connecting-with-the-community",
    "title": "Deep Learning Indaba 2024: Day 1 Highlights and Reflections",
    "section": "Connecting with the Community",
    "text": "Connecting with the Community\nOne of the highlights for me was meeting the dynamic duo behind our practical lab—James and Kira. Their hard work and dedication to organizing these sessions were evident, and it was great to share insights and ideas with James from DeepMind and Kira from UCL. We even managed to capture a memorable moment together, which I’ll cherish as a reminder of this wonderful experience."
  },
  {
    "objectID": "posts/dli-24-day-01/index.html#engaging-talks-on-fairness-and-speech-recognition",
    "href": "posts/dli-24-day-01/index.html#engaging-talks-on-fairness-and-speech-recognition",
    "title": "Deep Learning Indaba 2024: Day 1 Highlights and Reflections",
    "section": "Engaging Talks on Fairness and Speech Recognition",
    "text": "Engaging Talks on Fairness and Speech Recognition\nWith so many sessions happening concurrently, choosing which ones to attend was a challenge! I was fortunate enough to participate in two deeply insightful presentations. The first was on fairness in machine learning algorithms, a critical topic that delved into how we can ensure AI systems are fair, ethical, and inclusive. The second presentation focused on automatic speech recognition for low-resource languages, showcasing groundbreaking work that aims to bridge the digital divide and make AI accessible to all communities, regardless of linguistic diversity."
  },
  {
    "objectID": "posts/dli-24-day-01/index.html#celebrating-the-start-of-an-inspiring-week",
    "href": "posts/dli-24-day-01/index.html#celebrating-the-start-of-an-inspiring-week",
    "title": "Deep Learning Indaba 2024: Day 1 Highlights and Reflections",
    "section": "Celebrating the Start of an Inspiring Week",
    "text": "Celebrating the Start of an Inspiring Week\nWe capped off the day with a vibrant celebration, marking the official start of the Deep Learning Indaba 2024. The energy was infectious, and it was a fantastic way to unwind and connect with fellow AI enthusiasts from different parts of the continent."
  },
  {
    "objectID": "posts/dli-24-day-01/index.html#final-thoughts",
    "href": "posts/dli-24-day-01/index.html#final-thoughts",
    "title": "Deep Learning Indaba 2024: Day 1 Highlights and Reflections",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nReflecting on Day 1, I’m filled with gratitude and excitement. It’s truly an honor to be part of such a passionate and forward-thinking community. The discussions, learning, and networking have already been incredible, and I can’t wait to see what the rest of the week has in store.\nStay tuned for more updates, reflections, and possibly pictures from this inspiring event! 📸"
  },
  {
    "objectID": "posts/datacamp-cert/index.html",
    "href": "posts/datacamp-cert/index.html",
    "title": "My Journey to Becoming a Professional Data Scientist: A DataCamp Certification Odyssey",
    "section": "",
    "text": "Embarking on the path to deepen my data science expertise, I was presented with an invaluable opportunity—a free premium subscription to DataCamp, courtesy of Arewa Data Science Academy. This opportunity was not just a stepping stone but a gateway to a broader horizon in data science that I eagerly embraced.\nThe journey commenced with the Associate Data Scientist with Python track. Given my foundational knowledge and previous encounter with DataCamp’s Introduction to Python course, I was not venturing into uncharted territory. This familiarity served me well, enabling me to swiftly navigate through the coursework and complete the entire track within a week.\nBut the quest for knowledge is insatiable. I ventured further into the realm of data manipulation and retrieval by tackling the revamped SQL Fundamentals track. This was followed by successfully achieving the Associate Data Scientist with SQL certification, marking another milestone in my journey.\nEager to delve deeper, I enrolled in the Data Scientist with Python track. This comprehensive curriculum was not just about Python; it integrated SQL courses and introduced vital skills in package writing and Git fundamentals. This holistic approach equipped me with a robust set of tools and knowledge, preparing me for the pinnacle of my journey—the Professional Data Scientist Certification.\nThe certification process was rigorous, encompassing two timed written exams that tested my theoretical understanding through multiple-choice and fill-in-the-blank questions. However, the real challenge lay in the practical data science exam, which required me to code machine learning models and present my findings via webcam. This phase tested not just my technical skills but also my ability to communicate complex results effectively.\nDespite the pressure and the initial setback in the practical exam—stemming from difficulties in data cleaning—I remained undeterred. After a period of diligent study and review, I reattempted the practical exam. With renewed confidence and enhanced skills, I passed on my second attempt.\nThis journey was demanding, yet immensely rewarding. It culminated in earning a certificate that not only represents a testament to my skills and dedication but also empowers me with credentials recognized for two years.\nReflecting on this experience, I am reminded of the importance of resilience, the value of continuous learning, and the doors that can open when one is willing to push through challenges. My journey to becoming a professional data scientist was not just about acquiring a certificate; it was about growth, perseverance, and the relentless pursuit of excellence in the ever-evolving field of data science."
  },
  {
    "objectID": "posts/cleancodes/index.html",
    "href": "posts/cleancodes/index.html",
    "title": "Writing Clean Codes in JavaScript: A Necessity",
    "section": "",
    "text": "I recently started seriously practicing JavaScript, hence this post. Another impact of being a fellow of the Arewa Data Science Academy. Writing clean and readable code is an essential skill for any JavaScript developer. In fact, for all developers, regardless of favoured language. Clean code is code that is easy to understand, modify, and maintain. It is also code that follows the best practices and standards of the language. In this essay, we will discuss some of the best practices and style guides for writing clean JavaScript code.\n\nUse Meaningful Names for Variables, Functions, and Classes One of the most important aspects of writing clean code is to use meaningful names for your variables, functions, and classes. Your code should be self-explanatory, and the names should reflect the purpose of the code. Avoid abbreviations and acronyms that are not widely known, and use camelCase for variables and functions.\nUse Consistent Formatting Consistent formatting is essential for readability and maintainability. Use consistent indentation, spacing, and line breaks. Many style guides recommend using two spaces for indentation and avoiding tabs. Use a consistent brace style, either the “K&R style” or the “Allman style”.\nUse Comments Comments are an essential part of any codebase. They help explain the code’s purpose, how it works, and why it was written. Use comments to describe your functions and classes, to explain complex code, and to document your code’s edge cases. However, be careful not to overuse comments or to write comments that simply restate the code.\nAvoid Global Variables Global variables can cause a lot of problems in your code. They can create unexpected side effects, make debugging difficult, and make it harder to reason about your code. Instead, use local variables, function parameters, and closures to encapsulate your code and avoid global variables.\nUse Short Functions and Classes Short functions and classes are easier to read, understand, and maintain. Try to keep your functions and classes as short as possible, ideally under 50 lines of code. If a function or class is too long, consider breaking it down into smaller functions or classes.\nUse Error Handling Error handling is an important part of any codebase. It helps prevent bugs, improve user experience, and make your code more reliable. Use try-catch blocks to catch and handle errors, and use descriptive error messages to help users understand what went wrong.\nUse Modern JavaScript Features Modern JavaScript features such as arrow functions, template literals, and destructuring can help make your code more concise and expressive. Use these features whenever possible, but be careful not to overuse them and make your code less readable.\nUse a Style Guide Using a style guide can help ensure consistency and readability in your codebase. Some popular style guides for JavaScript include the Airbnb JavaScript Style Guide, Google JavaScript Style Guide, and StandardJS. Choose a style guide that fits your team’s needs and preferences and follow it consistently.\n\nIn conclusion, writing clean code is an essential skill for any JavaScript developer. Use meaningful names, consistent formatting, comments, avoid global variables, short functions and classes, error handling, modern JavaScript features, and a style guide to ensure your code is clean and readable. With these practices and guidelines, you can write high-quality code that is easy to understand, modify, and maintain."
  },
  {
    "objectID": "posts/arewads/index.html",
    "href": "posts/arewads/index.html",
    "title": "Joining The First Cohort of the Arewa Data Science Academy Fellowship",
    "section": "",
    "text": "The application link for the fellowship was sent to me by my friend, Abbas. Reluctantly, I opened it, only to be surprised by the requirement to complete an introduction to Python course on Udacity. Uploading the course certificate was mandatory. After completing the course, I discovered that obtaining the certificate required payment. To overcome this, I took a screenshot of my course dashboard showing my completion and uploaded that instead. A few days later, I received an email confirming my acceptance. This marked the beginning of my Python journey. Before joining Arewa, my experience with Python was purely theoretical, and I didn’t have an IDE installed on my laptop. I used Colab occasionally for practice.\nAfter joining Arewa, I was encouraged to install VSCode and practice daily. Our mentor, Dr. Shamsuddeen Muhammad, often emphasizes the importance of atomic habits, inspired by James Clear’s famous book. Having an IDE on my laptop and a structured course with daily exercises has significantly improved my understanding and appreciation of Python.\nOur Arewa Data Science Fellowship officially began on January 23, 2023, with Phase One of four phases, which includes:\n\n30 Days of Python Challenge: Designed to enhance our Python skills in preparation for Machine Learning and Deep Learning with PyTorch.\nCoursera Course ‘Learning How to Learn’: Aimed at improving our learning efficiency due to the numerous concepts in data science.\nGit and GitHub: For version control and collaboration.\nBlogging: To document our learning journey, which is why I am here.\n\nThe fellowship is structured for self-paced learning, complemented by twice-weekly office hours with mentors on weekends to clarify concepts and discuss challenges encountered during exercises.\nI will share more about the fellowship as I delve into various subjects in my programming journey with Arewa Data Science Academy.\n\nPractice makes improvement and possible mastery. This is my take home from my first few weeks in this fellowship. Practice is what I have been missing in my learning journey. As such, it is a great pleasure to be part of the first cohort of this fellowship. In sha Allah, greatness awaits."
  },
  {
    "objectID": "posts/arewads-impact/index.html",
    "href": "posts/arewads-impact/index.html",
    "title": "Arewa Data Science Academy Fellowship: A Transformative Journey to Success in Data Science",
    "section": "",
    "text": "The Arewa Data Science Academy fellowship has been an exceptional journey that has profoundly shaped my life this year. In just six months, I have acquired a wealth of knowledge, leaving an indelible mark on my personal and professional growth.\nThe fellowship’s comprehensive curriculum not only covered programming and data science skills but also delved into vital areas like version control using Git. Engaging and interactive workshops provided me with hands-on experiences that deepened my understanding of this critical aspect of modern software development. Additionally, I honed my expertise in bash scripting and command-line usage, enhancing my technical proficiency.\nAn extraordinary aspect of the fellowship was its focus on sharing our learning experiences through blogging. With training in using Quarto, I mastered the art of effective scientific writing, perfecting my ability to communicate complex ideas concisely. Blogging not only allowed me to reflect on my progress but also reinforced my understanding of the concepts I was learning, solidifying my grasp on data science principles.\nThroughout this transformative journey, our mentors played an indispensable role. Their immense talent and dedication created a supportive and encouraging environment for us to thrive. Regular updates and organization through platforms like Notion, slack and telegram ensured we never missed any valuable insights, keeping us on track and motivated.\nBeyond technical skills, the fellowship also emphasized essential aspects of professional development, including crafting an impressive CV. With practical exercises using LaTeX, we learned to create captivating CVs that effectively showcased our capabilities. The mentors’ commitment to providing personalized feedback and constructive criticism proved invaluable in refining our CVs to perfection, preparing us for future career opportunities.\nThe Arewa Data Science Academy has been an absolute blessing, propelling me beyond the boundaries of my imagination. As a part of this prestigious fellowship, I have gained newfound confidence and a fervor for continuous learning. My technical repertoire has expanded, and I have developed a deeper sense of self-awareness and determination to succeed in the data science field.\nReflecting on my journey as a fellow, I am amazed by how much I have achieved in such a short time. The skills and knowledge I have acquired will undoubtedly pave the way for a bright future in data science, and I am eager to explore the limitless possibilities that lie ahead. The academy has laid a strong foundation for my ongoing pursuit of excellence, inspiring me to keep pushing my boundaries and seeking new challenges.\nIn conclusion, joining the Arewa Data Science Academy fellowship has been a pivotal moment in my life. It has equipped me with a diverse set of skills, provided a supportive network, and instilled an unwavering passion for continuous growth. I am profoundly grateful for this transformative experience and eagerly anticipate the boundless opportunities that await me on this remarkable path of learning and discovery."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ailearningloop",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nYet Another Quarter of Dedicated Mentorship\n\n\n\n\n\n\ndeeplearning-ai\n\n\nmentorship\n\n\n\n\n\n\n\n\n\nMay 8, 2025\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nBehind the Scenes: Testing the DeepLearning.AI Data Analytics Professional Certificate\n\n\n\n\n\n\ndeeplearning-ai\n\n\nmentorship\n\n\n\n\n\n\n\n\n\nApr 29, 2025\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nCelebrating Recognition from DeepLearning.AI\n\n\n\n\n\n\ndeeplearning-ai\n\n\nmentorship\n\n\n\n\n\n\n\n\n\nJan 8, 2025\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nAn Exploration of Geom-Stat Pairs in R Programming’s Tidyverse\n\n\n\n\n\n\nggplot2\n\n\ntidyverse\n\n\nr-programming\n\n\n\n\n\n\n\n\n\nNov 11, 2024\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nOn Being Featured by DeepLearning.AI: A Milestone in My AI Journey\n\n\n\n\n\n\ndeeplearning-ai\n\n\ncareer-milestone\n\n\nai-education\n\n\n\n\n\n\n\n\n\nNov 6, 2024\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nCompleting the Analyzing Genomic Data in R track on DataCamp\n\n\n\n\n\n\ngenomics\n\n\n\n\n\n\n\n\n\nOct 12, 2024\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nCompleting the Health Informatics Specialization on Coursera\n\n\n\n\n\n\ninformatics\n\n\n\n\n\n\n\n\n\nSep 16, 2024\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning Indaba 2024 Day 6: A Journey Through Inspiring Workshops and Innovation in Health AI\n\n\n\n\n\n\ndeeplearning indaba\n\n\n\n\n\n\n\n\n\nSep 6, 2024\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nReflecting on the Final Day of the Deep Learning Indaba 2024: A Journey of Insights and Inspiration\n\n\n\n\n\n\ndeeplearning indaba\n\n\n\n\n\n\n\n\n\nSep 6, 2024\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning Indaba 2024:Day 5, A Day of Insights, Innovation, and Inspiration\n\n\n\n\n\n\ndeeplearning indaba\n\n\n\n\n\n\n\n\n\nSep 5, 2024\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning Indaba 2024:Day 4, Celebrating African Research, Learning, and Community Building\n\n\n\n\n\n\ndeeplearning indaba\n\n\n\n\n\n\n\n\n\nSep 4, 2024\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning Indaba 2024:Day 3, A Day of Insights, Innovation, and Inspiration\n\n\n\n\n\n\ndeeplearning indaba\n\n\n\n\n\n\n\n\n\nSep 3, 2024\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nDay 2 at Deep Learning Indaba 2024: Bridging AI Innovation and Ethics in Africa\n\n\n\n\n\n\ndeeplearning indaba\n\n\n\n\n\n\n\n\n\nSep 2, 2024\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning Indaba 2024: Day 1 Highlights and Reflections\n\n\n\n\n\n\ndeeplearning indaba\n\n\n\n\n\n\n\n\n\nSep 1, 2024\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Responsible AI: A Deep Dive into Ethical AI Development\n\n\n\n\n\n\ndeeplearning indaba\n\n\nresponsible ai\n\n\ndatacamp\n\n\n\n\n\n\n\n\n\nAug 25, 2024\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nThe Power of Persistence: How 365 Days of Consistency Transformed My Learning Journey\n\n\n\n\n\n\ndeeplearning-ai\n\n\n\n\n\n\n\n\n\nAug 19, 2024\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nA Journey from Learner to Leader: My Deep Learning.AI and Coursera Adventure\n\n\n\n\n\n\ntensorflow\n\n\narewads\n\n\ndeeplearning-ai\n\n\n\n\n\n\n\n\n\nAug 18, 2024\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nMy Contribution to Quarto-web: A Small Edit with a Big Impact\n\n\n\n\n\n\nquarto\n\n\nquarto-web\n\n\n\n\n\n\n\n\n\nJul 27, 2024\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nMetrics for Medical Diagnosis Evaluation\n\n\n\n\n\n\nevaluation metrics\n\n\nmedical diagnosis\n\n\n\n\n\n\n\n\n\nJul 6, 2024\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Data Structures and Algorithms on Coursera\n\n\n\n\n\n\ndata structures\n\n\nalgorithms\n\n\ndata science\n\n\nprogramming\n\n\n\n\n\n\n\n\n\nJun 10, 2024\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nMy Journey in Deep Learning: From TensorFlow to PyTorch and Back\n\n\n\n\n\n\ntensorflow\n\n\nai\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\nMay 27, 2024\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nPyTorch for Natural Language Processing\n\n\n\n\n\n\narewads\n\n\npytorch\n\n\nnlp\n\n\n\n\n\n\n\n\n\nMay 12, 2024\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nMy Journey to Becoming a Professional Data Scientist: A DataCamp Certification Odyssey\n\n\n\n\n\n\narewads\n\n\ndatacamp\n\n\ndcdonates\n\n\n\n\n\n\n\n\n\nMar 17, 2024\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nHarnessing the Power of R Shiny for Interactive Data Visualization\n\n\n\n\n\n\narewads\n\n\nshiny\n\n\nrprogramming\n\n\n\n\n\n\n\n\n\nMar 10, 2024\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nObject Oriented Programming: Python vs R\n\n\n\n\n\n\noop\n\n\npython\n\n\nrprogramming\n\n\n\n\n\n\n\n\n\nMar 6, 2024\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nCelebrating a Milestone in Statistics: A Journey through DataCamp’s ‘Statistics Fundamentals with R’ Track\n\n\n\n\n\n\narewads\n\n\ndatacamp\n\n\nrstats\n\n\n\n\n\n\n\n\n\nMar 3, 2024\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nBecoming a Mentor at Arewa Data Science Academy: A Journey of Growth and Contribution\n\n\n\n\n\n\narewads\n\n\nmentorship\n\n\n\n\n\n\n\n\n\nJan 28, 2024\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced Optimization Techniques in Deep Learning: Mastering PyTorch\n\n\n\n\n\n\ndeep learning\n\n\npytorch\n\n\noptimization\n\n\narewads\n\n\nfellowship\n\n\n\n\n\n\n\n\n\nJan 17, 2024\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nA Beginner’s Comprehensive Guide to PyTorch\n\n\n\n\n\n\ndeep learning\n\n\npytorch\n\n\narewads\n\n\nfellowship\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nEmbarking on a Deep Learning Journey with Arewa Data Science Academy’s Deep Learning with PyTorch Fellowship\n\n\n\n\n\n\ndeep learning\n\n\npytorch\n\n\narewads\n\n\nfellowship\n\n\n\n\n\n\n\n\n\nJan 15, 2024\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nSpeed and Elegance: How Python’s List Comprehensions Outshine Traditional Loops\n\n\n\n\n\n\nprogramming\n\n\npython\n\n\n\n\n\n\n\n\n\nDec 13, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nJuggling Code and Career: My Arewa Data Science 30 Days Of Python Learning Journey Amidst a Full Schedule\n\n\n\n\n\n\nlearning\n\n\nmotivation\n\n\nprogramming\n\n\npython\n\n\n\n\n\n\n\n\n\nDec 8, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Deep Learning Frameworks: FizzBuzz with PyTorch, TensorFlow, and Keras\n\n\n\n\n\n\nai\n\n\npytorch\n\n\ntensorflow\n\n\nprogramming\n\n\npython\n\n\n\n\n\n\n\n\n\nNov 6, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nComputation Graphs, Eager Execution and Flow Control in TensorFlow\n\n\n\n\n\n\nai\n\n\ntensorflow\n\n\nprogramming\n\n\npython\n\n\n\n\n\n\n\n\n\nNov 5, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nPreparing for the Future of AI: My Journey with IBM’s AI Engineering Professional Certificate\n\n\n\n\n\n\nai\n\n\npytorch\n\n\nkeras\n\n\ntensorflow\n\n\ncomputer vision\n\n\n\n\n\n\n\n\n\nNov 4, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nData Visualization with R using ggplot2\n\n\n\n\n\n\ndata science\n\n\nr programming\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\nOct 7, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nData Science: Foundations using R Specialization\n\n\n\n\n\n\ndata science\n\n\nr programming\n\n\ncoursera\n\n\n\n\n\n\n\n\n\nOct 3, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nReproducible Research Final Project\n\n\n\n\n\n\nr programming\n\n\nproject\n\n\nrpubs\n\n\n\n\n\n\n\n\n\nOct 2, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nMy Adventure with Generative Adversarial Networks (GANs)\n\n\n\n\n\n\nGANs\n\n\n\n\n\n\n\n\n\nSep 22, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nU-Net Architecture: Revolutionizing Computer Vision Through Innovative Image Segmentation\n\n\n\n\n\n\ncnn\n\n\ndeep learning\n\n\ncomputer vision\n\n\nimage segmentation\n\n\n\n\n\n\n\n\n\nAug 28, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nA Study on the Impact of Data Augmentation for Training Convolutional Neural Networks in the Presence of Noisy Labels - Paper Summary\n\n\n\n\n\n\ncnn\n\n\nresearch\n\n\npaper summary\n\n\n\n\n\n\n\n\n\nAug 19, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nMatrix Multiplication: Deep Learning Insights, Implementation, and Performance in Python and R\n\n\n\n\n\n\npython\n\n\nr\n\n\nmatrices\n\n\nmathematics\n\n\n\n\n\n\n\n\n\nAug 16, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Symbolic Computation with SymPy\n\n\n\n\n\n\npython\n\n\nsympy\n\n\nmathematics\n\n\nsymbolic computing\n\n\n\n\n\n\n\n\n\nAug 15, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nMy Journey into the Mathematics in Machine Learning\n\n\n\n\n\n\nmathematics\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\nAug 14, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nRevving Up: MPG Regression Unleashed through Deep Learning in PyTorch\n\n\n\n\n\n\ndeep learning\n\n\npytorch\n\n\nregression\n\n\npython\n\n\n\n\n\n\n\n\n\nAug 9, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nThe Crucial Role of the Sigmoid Function and its Derivative in Machine Learning\n\n\n\n\n\n\nmachine learning\n\n\nsigmoid\n\n\nactivation\n\n\nmathematics\n\n\n\n\n\n\n\n\n\nAug 7, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nEnhancing Effective Writing: Insights from Larry McEnerney’s Leadership Lab\n\n\n\n\n\n\nwriting\n\n\nresearch\n\n\neffective writing\n\n\n\n\n\n\n\n\n\nAug 4, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nArewa Data Science Academy Fellowship: A Transformative Journey to Success in Data Science\n\n\n\n\n\n\nskills\n\n\narewads\n\n\ngit\n\n\nquarto\n\n\nwriting\n\n\n\n\n\n\n\n\n\nJul 31, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nEndless Paths in Data Science: Insights from Claudiu Musat’s Talk on Handwriting Synthesis and Career Prospects\n\n\n\n\n\n\nresearch\n\n\nadvice\n\n\n\n\n\n\n\n\n\nJul 30, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Vectorized Code: Boosting Efficiency in R\n\n\n\n\n\n\nr programming\n\n\nvectorization\n\n\n\n\n\n\n\n\n\nJul 29, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nWriting a Scientific Paper\n\n\n\n\n\n\nscientific writing\n\n\njournal article\n\n\nresearch\n\n\n\n\n\n\n\n\n\nJul 25, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nCompletion of Arewa Data Science Academy Fellowship\n\n\n\n\n\n\narewads\n\n\n\n\n\n\n\n\n\nJul 24, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nSolving the FizzBuzz Problem: Eight Creative Solutions in R Programming\n\n\n\n\n\n\nr programming\n\n\nfizzbuzz\n\n\nloops\n\n\n\n\n\n\n\n\n\nJun 19, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nHarnessing the Power of Natural Language Processing: A Guide to Python and R Packages and Practical Use-Cases\n\n\n\n\n\n\nnatural language processing\n\n\npython\n\n\nr\n\n\npackages\n\n\n\n\n\n\n\n\n\nJun 13, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nJavaScript FizzBuzz Solutions\n\n\n\n\n\n\njavascript\n\n\nprogramming\n\n\nloops\n\n\nfizzbuzz\n\n\n\n\n\n\n\n\n\nMay 14, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Data to Insight: Visualizing Quantities, Proportions, Relationships, and Distributions with Python’s Matplotlib and Seaborn\n\n\n\n\n\n\npython\n\n\ndata visualization\n\n\nmatplotlib\n\n\nseaborn\n\n\n\n\n\n\n\n\n\nMay 13, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Scientific Writing: Key Takeaways from Coursera’s Writing in the Sciences MOOC by Kristin-Sainani\n\n\n\n\n\n\nwriting\n\n\nsciences\n\n\ncoursera\n\n\nmooc\n\n\n\n\n\n\n\n\n\nApr 26, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nWriting Clean Codes in JavaScript: A Necessity\n\n\n\n\n\n\njavascript\n\n\ncode\n\n\nwriting\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nOverview of Data Processing Using Pandas\n\n\n\n\n\n\npython\n\n\npandas\n\n\ndata analysis\n\n\n\n\n\n\n\n\n\nApr 7, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nMachine-Learning: Relationship With Deep Learning, AI ,and Data Science\n\n\n\n\n\n\nmachine learning\n\n\ndeep learning\n\n\nai\n\n\ndata science\n\n\n\n\n\n\n\n\n\nMar 29, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Pandas: A Powerful Pythonic Analytic Tool\n\n\n\n\n\n\npandas\n\n\npython\n\n\ndata analysis\n\n\n\n\n\n\n\n\n\nMar 14, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Beautiful Visualizations with Python: A Guide to Pandas Matplotlib and Seaborn\n\n\n\n\n\n\npython\n\n\npandas\n\n\nmatplotlib\n\n\nseaborn\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\nMar 14, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nNumPy ndarrays: Introduction to a Powerful Tool for Scientific Computing in Python\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nMar 11, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping a Daily Trust Article using Requests and Beautiful Soup Libraries\n\n\n\n\n\n\npython\n\n\nrequests\n\n\nweb scraping\n\n\n\n\n\n\n\n\n\nMar 4, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nPractising Concepts Learnt So far in ArewaDS\n\n\n\n\n\n\npython\n\n\nhow to learn\n\n\n\n\n\n\n\n\n\nFeb 21, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nNecessity of Developing a Learning Method\n\n\n\n\n\n\nhow to learn\n\n\n\n\n\n\n\n\n\nFeb 19, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nHow I Started Learning Python\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nJoining The First Cohort of the Arewa Data Science Academy Fellowship\n\n\n\n\n\n\narewads\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nHow I Started Learning Python\n\n\n\n\n\n\norigins\n\n\n\n\n\n\n\n\n\nFeb 12, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nwelcome\n\n\n\n\n\n\n\n\n\nFeb 11, 2023\n\n\nLukman Aliyu Jibril\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a passionate and dedicated professional with a background in Pharmacy and a strong focus on Artificial Intelligence and Machine Learning in healthcare. I have proven experience in research, data science, and machine learning, with a track record of high-performing model development and publication. I currently work at Zipline, ensuring the delivery of medical supplies using innovative solutions. I am highly skilled in data analysis, programming, and interprofessional collaboration to improve patient outcomes and advance healthcare delivery.\nIn addition to my professional role, I am an active participant at the Arewa Data Science Academy, where as a fellow and now a mentor, I have built high-performing machine learning models, taught data science, and achieved recognition by securing second place in the Cohere AI Hack. I am also an active moderator,mentor (for the DeepLearning.AI TensorFlow Developer Professional Certificate) and course tester at the DeepLearning.AI Forum.\nMy journey into the world of machine learning began with the Arewa Data Science Academy fellowship in January 2023. Since then, I have been exploring the intersection of healthcare and artificial intelligence, focusing on biostatistics in public health. I am particularly interested in using machine learning to improve health outcomes and address challenges in low-resource settings.\nI blog about my learning journey, sharing insights and experiences as I navigate the realms of healthcare and machine learning. My goal is to contribute to the growing body of knowledge in these fields and inspire others to explore the potential of AI in healthcare. For a view of my earned certifications, kindly check here\nFeel free to connect with me on LinkedIn or GitHub."
  },
  {
    "objectID": "posts/30-days-python/index.html",
    "href": "posts/30-days-python/index.html",
    "title": "How I Started Learning Python",
    "section": "",
    "text": "As part of the first phase of the Arewa Data science fellowship, the mentors organised a 30 days of Python challenge that consists of exercises tailored to a particular topic per day. Each day offers a challenge on a particular topic in Python. For example, day 5 covers list and exercises of an appropriate hardness level are provided for practice. This is to support understanding of the concept of lists in Python. In addition, the challenge employs a 5 days a week format, since the weekends serve as virtual office hours and can be used to refactor our code and get ready for the next week.\nParticipating in this 30 Days of Python challenge has given me an avenue to practice the Python concepts I have learnt in the past. Before we began the challenge proper, we were taught how to set up VSCode as an IDE for our Python practice. VSCode is an awesome code editor/IDE I must confess. Afterwards, we were instructed to push our daily exercises to the github repository we all made specifically for the challenge. This challenge has significantly improved my Python experience and I am grateful to Arewa Data Science for the opportunity.\nIn subsequent outings, I will try to explain my experience going through some of the exercises and also discuss what I learnt in some of the virtual office hours and of course, the MOOC, ‘Learning How to Learn’ on coursera. Arewa Data Science Fellowship is an interesting addition to my routine with a potential to be life changing."
  },
  {
    "objectID": "posts/arewads-mentorship/index.html",
    "href": "posts/arewads-mentorship/index.html",
    "title": "Becoming a Mentor at Arewa Data Science Academy: A Journey of Growth and Contribution",
    "section": "",
    "text": "I am thrilled to announce my recent role as a mentor for the second cohort of machine learning fellows at the Arewa Data Science Academy, guiding them through the Python programming phase of their fellowship. Reflecting on this experience brings a mix of emotions; it’s hard to believe that just a year ago, I was in their shoes, diligently learning Python myself. This journey has been a testament to how far I’ve come.\nOne of the most rewarding aspects of this role is the reciprocal nature of learning and teaching. As I share my knowledge and experience with the fellows, I find my own skills sharpening. The process of mentoring not only benefits the learners but also enhances my understanding and proficiency in data science.\nCurrently, I’m engaged in mentoring these bright individuals in the data science phase of their fellowship. The academy has graciously recognized my contributions and efforts by awarding me a certificate, which I proudly share with immense gratitude.\nThis past year has been a remarkable period of personal and professional growth for me. The journey with Arewa Data Science Academy has been incredibly fulfilling, offering me the chance to both impart knowledge and continuously learn. As I navigate this path of ongoing exploration and adaptation, I deeply appreciate the opportunity given to me by the Academy. It’s a privilege to contribute to the growth of others while simultaneously enriching my own experience.\nI am grateful for this journey and look forward to what the future holds. May Allah guide us all on our path to success."
  },
  {
    "objectID": "posts/claudiu-talk/index.html",
    "href": "posts/claudiu-talk/index.html",
    "title": "Endless Paths in Data Science: Insights from Claudiu Musat’s Talk on Handwriting Synthesis and Career Prospects",
    "section": "",
    "text": "After recently concluding our fellowship at Arewa Data Science Academy, we had the privilege of attending a captivating talk by Claudiu Musat, a distinguished researcher from Google. The session revolved around illuminating career prospects in data science and guiding us on our next steps in this dynamic field. In the beginning, he warmly congratulated us on reaching this significant milestone, emphasizing that embarking on our journey was in itself an achievement worth celebrating, and each step we take marks progress towards something greater.\nThe highlight of the talk was his comprehensive presentation on handwriting synthesis, tracing its origins and evolution to the cutting-edge use of transformers in the field today. He also delved into the fascinating realm of generative AI, illustrating its potential to revolutionize the world by already disrupting various aspects of life. As Claudiu skillfully articulated the technical aspects of Deep Learning and its application to problem-solving, I gained a profound understanding of how recurrent neural networks can perform awe-inspiring feats and the critical interplay between research in academia and industry.\nAn invaluable piece of advice he shared with us was the importance of not just reading research papers but actively striving to publish our own work. Moreover, he offered practical tips for aspiring data scientists like us, encouraging us to fearlessly explore, experiment, and break barriers. One idea that resonated deeply was his suggestion to “look at that TensorFlow code and try implementing it in PyTorch.” This philosophy, reminiscent of Yann LeCun’s advice to beginners, can significantly enhance one’s chances of getting noticed in the field.\nI must express my profound gratitude for this enlightening session; I meticulously took notes and left with a wealth of actionable insights for my future endeavors. Thank you, Claudiu Musat, for inspiring and equipping us with the tools to carve out a successful path in the captivating realm of data science."
  },
  {
    "objectID": "posts/contributing-to-quarto/index.html",
    "href": "posts/contributing-to-quarto/index.html",
    "title": "My Contribution to Quarto-web: A Small Edit with a Big Impact",
    "section": "",
    "text": "I’m thrilled to share my recent experience contributing to the Quarto-web open-source project. Although it might seem like a small change, my contribution aimed to improve the clarity and readability of the documentation, ensuring users have a smoother experience when learning about Quarto. I’ve always appreciated Quarto since I first learned about it from Dr. Shamsuddeen Muhammad during the first cohort of the Arewa Data Science Fellowship. I continued practicing, and now I am proudly a contributor. I look forward to making more impactful contributions in the future."
  },
  {
    "objectID": "posts/contributing-to-quarto/index.html#introduction",
    "href": "posts/contributing-to-quarto/index.html#introduction",
    "title": "My Contribution to Quarto-web: A Small Edit with a Big Impact",
    "section": "",
    "text": "I’m thrilled to share my recent experience contributing to the Quarto-web open-source project. Although it might seem like a small change, my contribution aimed to improve the clarity and readability of the documentation, ensuring users have a smoother experience when learning about Quarto. I’ve always appreciated Quarto since I first learned about it from Dr. Shamsuddeen Muhammad during the first cohort of the Arewa Data Science Fellowship. I continued practicing, and now I am proudly a contributor. I look forward to making more impactful contributions in the future."
  },
  {
    "objectID": "posts/contributing-to-quarto/index.html#spotting-the-issue-a-simple-typo-and-punctuation-fix",
    "href": "posts/contributing-to-quarto/index.html#spotting-the-issue-a-simple-typo-and-punctuation-fix",
    "title": "My Contribution to Quarto-web: A Small Edit with a Big Impact",
    "section": "Spotting the Issue: A Simple Typo and Punctuation Fix",
    "text": "Spotting the Issue: A Simple Typo and Punctuation Fix\nWhile exploring the Quarto Dashboards documentation, specifically the R Graphics section tip on this page, I noticed a minor typo—‘than’ was mistakenly written as ‘that’. Additionally, there were a few missing commas, which could potentially confuse readers.\nThese might seem like minor issues, but clear and accurate documentation is crucial in helping users understand and utilize software tools effectively. I decided to take action and correct these mistakes to enhance the user experience."
  },
  {
    "objectID": "posts/contributing-to-quarto/index.html#the-pull-request-a-quick-fix",
    "href": "posts/contributing-to-quarto/index.html#the-pull-request-a-quick-fix",
    "title": "My Contribution to Quarto-web: A Small Edit with a Big Impact",
    "section": "The Pull Request: A Quick Fix",
    "text": "The Pull Request: A Quick Fix\nI submitted a pull request (#1214) to update the _plots-interactive.md file, which contained the typo and punctuation errors. The changes were straightforward:\n\nCorrected the typo from ‘that’ to ‘than’.\nAdded a few missing commas for better clarity and readability.\n\nThese edits were minor but important, as they helped ensure the documentation was accurate and easy to follow."
  },
  {
    "objectID": "posts/contributing-to-quarto/index.html#merged-and-celebrated-the-joy-of-contributing",
    "href": "posts/contributing-to-quarto/index.html#merged-and-celebrated-the-joy-of-contributing",
    "title": "My Contribution to Quarto-web: A Small Edit with a Big Impact",
    "section": "Merged and Celebrated: The Joy of Contributing",
    "text": "Merged and Celebrated: The Joy of Contributing\nTo my delight, my pull request was reviewed and merged by the maintainers. You can view the merged changes in the pull request here. It was a small contribution, but seeing my changes go live was incredibly rewarding. It was a reminder that even small contributions can make a big difference in the open-source community."
  },
  {
    "objectID": "posts/contributing-to-quarto/index.html#reflections-on-my-contribution",
    "href": "posts/contributing-to-quarto/index.html#reflections-on-my-contribution",
    "title": "My Contribution to Quarto-web: A Small Edit with a Big Impact",
    "section": "Reflections on My Contribution",
    "text": "Reflections on My Contribution\nThis experience has taught me a few valuable lessons:\n\nAttention to Detail Matters: Even small errors in documentation can lead to misunderstandings. Taking the time to correct them helps improve the overall quality of the project.\nEvery Contribution Counts: No matter the size, every contribution to an open-source project helps make it better. It’s about being part of a community effort.\nLearning and Growing: This was my first contribution to Quarto-web, and it has encouraged me to continue participating in open-source projects. There’s always something new to learn and areas to improve."
  },
  {
    "objectID": "posts/contributing-to-quarto/index.html#looking-forward",
    "href": "posts/contributing-to-quarto/index.html#looking-forward",
    "title": "My Contribution to Quarto-web: A Small Edit with a Big Impact",
    "section": "Looking Forward",
    "text": "Looking Forward\nI’m excited to continue exploring opportunities in the open-source world. Whether it’s fixing typos, adding features, or writing documentation, every bit helps the community grow and thrive. If you’re interested in getting involved, I encourage you to look for ways to contribute, no matter how small they may seem.\nThank you for reading, and feel free to follow me on the social media links in my about page. Happy coding!"
  },
  {
    "objectID": "posts/dlai-feature/index.html",
    "href": "posts/dlai-feature/index.html",
    "title": "On Being Featured by DeepLearning.AI: A Milestone in My AI Journey",
    "section": "",
    "text": "A significant milestone in my artificial intelligence journey recently materialized when DeepLearning.AI featured my story across their global platforms. The recognition came from a forum post where I shared my experiences as a course tester and mentor, reflecting on a journey that began in January 2023 with Andrew Ng’s Machine Learning Specialization."
  },
  {
    "objectID": "posts/dlai-feature/index.html#the-journey-to-recognition",
    "href": "posts/dlai-feature/index.html#the-journey-to-recognition",
    "title": "On Being Featured by DeepLearning.AI: A Milestone in My AI Journey",
    "section": "The Journey to Recognition",
    "text": "The Journey to Recognition\nMy involvement with DeepLearning.AI has been transformative. What started as enrolling in a single course evolved into a deep engagement with the AI community. As a course tester, I’ve had the privilege of providing feedback that helps shape the learning experience for future students. In my role as a mentor, I’ve guided numerous learners through the complexities of machine learning, witnessing their growth from novices to confident practitioners."
  },
  {
    "objectID": "posts/dlai-feature/index.html#global-reach-and-impact",
    "href": "posts/dlai-feature/index.html#global-reach-and-impact",
    "title": "On Being Featured by DeepLearning.AI: A Milestone in My AI Journey",
    "section": "Global Reach and Impact",
    "text": "Global Reach and Impact\nThe feature was shared across DeepLearning.AI’s extensive social media network:\n\nLinkedIn showcased my journey to their professional network of AI enthusiasts\nX (formerly Twitter) broadcast my story to the global tech community\nFacebook reached aspiring AI practitioners worldwide\nInstagram captured the visual narrative of my path in AI\n\nA particularly touching moment came when my friend Abbas shared the Instagram feature with me—a reminder of how supportive the tech community can be."
  },
  {
    "objectID": "posts/dlai-feature/index.html#professional-growth-and-future-aspirations",
    "href": "posts/dlai-feature/index.html#professional-growth-and-future-aspirations",
    "title": "On Being Featured by DeepLearning.AI: A Milestone in My AI Journey",
    "section": "Professional Growth and Future Aspirations",
    "text": "Professional Growth and Future Aspirations\nThis recognition arrives at a pivotal moment in my career. After three enriching years at Zipline, where I applied AI to revolutionize medical delivery systems in Africa, I’m preparing for new challenges. My experience has reinforced my belief in AI’s potential to transform healthcare delivery in developing regions. The intersection of my pharmaceutical background with artificial intelligence has positioned me uniquely in the healthcare-tech space. At Zipline, I’ve witnessed firsthand how AI can optimize medical supply chains and improve access to healthcare in remote areas."
  },
  {
    "objectID": "posts/dlai-feature/index.html#community-impact-and-mentorship",
    "href": "posts/dlai-feature/index.html#community-impact-and-mentorship",
    "title": "On Being Featured by DeepLearning.AI: A Milestone in My AI Journey",
    "section": "Community Impact and Mentorship",
    "text": "Community Impact and Mentorship\nOne of my proudest achievements has been mentoring in the AI community. Through DeepLearning.AI’s platform, I’ve had the opportunity to: - Guide aspiring data scientists through complex concepts - Share practical insights from real-world AI applications - Build a supportive network of learners and practitioners - Contribute to making AI education more accessible"
  },
  {
    "objectID": "posts/dlai-feature/index.html#looking-forward",
    "href": "posts/dlai-feature/index.html#looking-forward",
    "title": "On Being Featured by DeepLearning.AI: A Milestone in My AI Journey",
    "section": "Looking Forward",
    "text": "Looking Forward\nThis recognition from DeepLearning.AI has reinforced my commitment to advancing AI education and application in healthcare. As I prepare for the next chapter in my career, I remain dedicated to: - Continuing to mentor and support aspiring AI practitioners - Contributing to the development of AI solutions in healthcare - Building bridges between technology and healthcare delivery - Advancing AI education in Africa"
  },
  {
    "objectID": "posts/dlai-feature/index.html#social-media-links",
    "href": "posts/dlai-feature/index.html#social-media-links",
    "title": "On Being Featured by DeepLearning.AI: A Milestone in My AI Journey",
    "section": "Social Media Links",
    "text": "Social Media Links\n\nView on X (Twitter)\nRead on LinkedIn\nFacebook Post\nInstagram Feature\n\nIn sha Allah, this milestone marks just the beginning of greater achievements to come. I’m deeply grateful for the support of my community, mentors, and colleagues who have been part of this journey. As we continue to push the boundaries of what’s possible with AI, I look forward to contributing to more innovations that make a real difference in people’s lives."
  },
  {
    "objectID": "posts/dli-24-day-02/index.html",
    "href": "posts/dli-24-day-02/index.html",
    "title": "Day 2 at Deep Learning Indaba 2024: Bridging AI Innovation and Ethics in Africa",
    "section": "",
    "text": "As the sun rose over Dakar, Senegal, it marked the beginning of another exhilarating day at the Deep Learning Indaba 2024. Hosted by the prestigious Université Amadou Mahtar Mbow, this gathering of brilliant minds from across Africa continues to push the boundaries of AI innovation on the continent. Today’s experiences were nothing short of transformative, blending cutting-edge technology with ethical considerations and a spirit of collaboration that defines the African AI community."
  },
  {
    "objectID": "posts/dli-24-day-02/index.html#morning-keynote-causethical-ml---from-theory-to-practice",
    "href": "posts/dli-24-day-02/index.html#morning-keynote-causethical-ml---from-theory-to-practice",
    "title": "Day 2 at Deep Learning Indaba 2024: Bridging AI Innovation and Ethics in Africa",
    "section": "Morning Keynote: Causethical ML - From Theory to Practice",
    "text": "Morning Keynote: Causethical ML - From Theory to Practice\nThe day kicked off with an enlightening keynote on Causethical Machine Learning. This session delved into the critical intersection of causal inference and ethical AI, emphasizing a fundamental principle: algorithms should not only be efficient but also fair and grounded in ethically acceptable reasoning.\nThe speaker articulated a compelling vision where fairness in AI isn’t just a moral imperative but a practical one. By ensuring that our algorithms make decisions based on ethically sound principles, we create a win-win scenario. Not only do we uphold justice and equality, but we also create systems that benefit all stakeholders in the long run. This approach to AI development resonates deeply with the African context, where technology must be developed with a keen awareness of diverse societal needs and historical inequities."
  },
  {
    "objectID": "posts/dli-24-day-02/index.html#ai-for-good-microsofts-vision-for-positive-impact",
    "href": "posts/dli-24-day-02/index.html#ai-for-good-microsofts-vision-for-positive-impact",
    "title": "Day 2 at Deep Learning Indaba 2024: Bridging AI Innovation and Ethics in Africa",
    "section": "AI for Good: Microsoft’s Vision for Positive Impact",
    "text": "AI for Good: Microsoft’s Vision for Positive Impact\nFollowing the keynote, we had the privilege of hearing from representatives of the Microsoft AI for Good Research Lab. Their presentation was a testament to the transformative power of AI when applied to pressing global challenges."
  },
  {
    "objectID": "posts/dli-24-day-02/index.html#pushing-boundaries-snake-venom-analysis-using-large-language-models",
    "href": "posts/dli-24-day-02/index.html#pushing-boundaries-snake-venom-analysis-using-large-language-models",
    "title": "Day 2 at Deep Learning Indaba 2024: Bridging AI Innovation and Ethics in Africa",
    "section": "Pushing Boundaries: Snake Venom Analysis Using Large Language Models",
    "text": "Pushing Boundaries: Snake Venom Analysis Using Large Language Models\nOne of the most fascinating sessions of the day focused on an unexpected application of AI: analyzing and sequencing snake venom using Large Language Models (LLMs). This presentation was a perfect example of how AI can revolutionize even the most specialized scientific fields. The researchers demonstrated how they’ve adapted LLMs, typically used for processing human languages, to “read” the complex molecular structures of various snake venoms. This application has enormous potential for:\n\nAccelerating the development of antivenoms\nIdentifying new compounds for pharmaceutical research\nEnhancing our understanding of snake biology and evolution\n\nAs someone deeply interested in both AI and biodiversity, I found this session to be a brilliant demonstration of interdisciplinary research. It showcased how AI tools developed for one domain can find groundbreaking applications in entirely different fields."
  },
  {
    "objectID": "posts/dli-24-day-02/index.html#hands-on-learning-tutoring-responsible-ai-practicals",
    "href": "posts/dli-24-day-02/index.html#hands-on-learning-tutoring-responsible-ai-practicals",
    "title": "Day 2 at Deep Learning Indaba 2024: Bridging AI Innovation and Ethics in Africa",
    "section": "Hands-on Learning: Tutoring Responsible AI Practicals",
    "text": "Hands-on Learning: Tutoring Responsible AI Practicals\nA personal highlight of my day was the opportunity to serve as a tutor for the Responsible AI practicals. Having reviewed the notebooks ahead of the Indaba, I was excited to guide participants through the intricacies of developing AI systems with ethical considerations at their core. The notebooks (English and French) are available on github\nWhat struck me most was the eagerness of the participants to engage with these critical issues. Their questions and insights reflected a deep understanding of the potential impacts of AI on African societies and a commitment to developing technologies that uplift rather than marginalize.\nThese sessions embodied the Indaba’s theme of “Xam Xamlé” – to gain knowledge and share it. The collaborative spirit in the room was palpable, with participants from various countries sharing their unique perspectives on responsible AI development."
  },
  {
    "objectID": "posts/dli-24-day-02/index.html#inspiring-vision-building-up-the-future-of-ai-in-africa",
    "href": "posts/dli-24-day-02/index.html#inspiring-vision-building-up-the-future-of-ai-in-africa",
    "title": "Day 2 at Deep Learning Indaba 2024: Bridging AI Innovation and Ethics in Africa",
    "section": "Inspiring Vision: Building Up the Future of AI in Africa",
    "text": "Inspiring Vision: Building Up the Future of AI in Africa\nThe day concluded with an electrifying presentation by Karim Beguir titled “Building Up the Future of AI in Africa.” Beguir’s talk was a rallying cry for African AI researchers and developers, emphasizing that we are living in an unprecedented era of opportunity.\nKey points from his presentation included:\n\nThe democratization of AI tools, with resources like Llama 3.1 now accessible to researchers across the continent\nThe groundbreaking work being done at InstaDeep, showcasing African-led innovation in AI\nAn overview of Project Tatooine, a collaboration with GoMyCode aimed at nurturing the next generation of AI talent in Africa, particularly in less developed locations.\n\nBeguir’s central message – that “agents are the future” – sparked intense discussions among attendees. The vision of AI agents autonomously solving complex problems, tailored to African contexts, opened up exciting possibilities for innovation across various sectors."
  },
  {
    "objectID": "posts/dli-24-day-02/index.html#networking-and-collaboration-the-heart-of-indaba",
    "href": "posts/dli-24-day-02/index.html#networking-and-collaboration-the-heart-of-indaba",
    "title": "Day 2 at Deep Learning Indaba 2024: Bridging AI Innovation and Ethics in Africa",
    "section": "Networking and Collaboration: The Heart of Indaba",
    "text": "Networking and Collaboration: The Heart of Indaba\nWhile the formal sessions were incredibly informative, some of the most valuable moments came from the connections made between sessions. The Indaba’s atmosphere encourages spontaneous discussions and collaborations, leading to some exciting developments:\nI joined two hackathons – one hosted by Meta and another by InstaDeep. These events are not just competitions but incubators for ideas that could evolve into impactful projects.\nThe coffee breaks buzzed with energy as participants exchanged ideas, formed impromptu working groups, and planned future collaborations."
  },
  {
    "objectID": "posts/dli-24-day-02/index.html#a-moment-of-personal-connection",
    "href": "posts/dli-24-day-02/index.html#a-moment-of-personal-connection",
    "title": "Day 2 at Deep Learning Indaba 2024: Bridging AI Innovation and Ethics in Africa",
    "section": "A Moment of Personal Connection",
    "text": "A Moment of Personal Connection\nAmidst the whirlwind of activities, I experienced a deeply meaningful personal moment. After nearly two years of virtual collaboration, I finally met Dr. Shamsuddeen Hassan Muhammad in person. Our work together has been a source of immense learning and growth for me, and meeting face-to-face was an emotional and inspiring experience.\nI also had the pleasure of meeting Dr. Idris Abdulmumin, another mentor who has played a crucial role in my journey. Having collaborated on a paper and benefited from his guidance during our first hackathon win, meeting him in person was a moment of profound gratitude.\nThese interactions underscored the importance of mentorship and personal connections in the world of AI research. They reminded me that behind every algorithm and research paper are human relationships that drive innovation and personal growth."
  },
  {
    "objectID": "posts/dli-24-day-02/index.html#reflections-and-looking-ahead",
    "href": "posts/dli-24-day-02/index.html#reflections-and-looking-ahead",
    "title": "Day 2 at Deep Learning Indaba 2024: Bridging AI Innovation and Ethics in Africa",
    "section": "Reflections and Looking Ahead",
    "text": "Reflections and Looking Ahead\nAs I reflect on this second day of the Deep Learning Indaba, I’m filled with a sense of optimism and purpose. The convergence of brilliant minds, cutting-edge research, and a shared commitment to ethical AI development creates an environment where true innovation can flourish.\nThe discussions and presentations today highlighted several key themes that I believe will shape the future of AI in Africa:\n\nThe importance of developing AI solutions that address uniquely African challenges and opportunities\nThe critical need for responsible AI practices that consider the diverse ethical and societal implications of these technologies\nThe power of collaboration and knowledge sharing in accelerating AI innovation across the continent\n\nAs we move into the latter half of the Indaba, I’m eager to see how these themes will evolve and what new insights will emerge. The energy and intellect gathered here in Dakar are a testament to the bright future of AI in Africa.\nAlhamdulillah for another day filled with learning, connection, and inspiration. The Deep Learning Indaba continues to be a catalyst for AI innovation in Africa, and I’m honored to be part of this transformative experience.\nStay tuned for more updates as we continue this exciting journey at the forefront of AI in Africa!"
  },
  {
    "objectID": "posts/dli-24-day-04/index.html",
    "href": "posts/dli-24-day-04/index.html",
    "title": "Deep Learning Indaba 2024:Day 4, Celebrating African Research, Learning, and Community Building",
    "section": "",
    "text": "Today at Deep Learning Indaba 2024 was nothing short of inspiring! As the event unfolds, I’m continually amazed by the depth and diversity of research being conducted across Africa. Here’s a recap of my fourth day:\nCelebrating African Research Excellence: The day began with the Indaba Kambule Doctoral Thesis Award ceremony, which recognizes outstanding doctoral research by African scholars. This award is a testament to the originality and impact of research emerging from the continent, highlighting the innovative spirit driving Africa forward in AI and data science.\nDiving into Oral Presentations and Poster Sessions: The day continued with IndabaX Oral Presentations and two sessions of Poster Presentations. The posters showcased a range of exciting research:\n\nDeep Learning-Based Lesion Segmentation for Early Liver Tumor Detection in Rwanda: Using a hybrid ResNet-UNet model, researchers presented significant results in early tumor detection from liver CT scans—a major step forward for healthcare in Rwanda.\nsiSwatiSent: Sentiment Analysis Dataset for siSwati: A unique sentiment analysis dataset for the siSwati language, extracted from YouTube comments. This study emphasizes the need for more African language resources in NLP.\nCHOWNET-V1: An Image Dataset of Nigerian Food: A fascinating dataset that addresses the underrepresentation of African cuisine in machine learning research, enhancing diversity in food classification tasks.\n\nMentorship and Knowledge Sharing: I was fortunate to join a mentorship session with Georgina Curto Rex. Our discussion revolved around the importance of selecting meaningful problems to work on and the role of having a guiding purpose as we explore various research avenues. This session was a reminder that our work should always aim to make a real-world impact.\nUnderstanding the Limitations of LLMs: The keynote lecture by Caglar Gulcehre focused on the limitations of large language models (LLMs). It was an eye-opening discussion that explored both the capabilities and the constraints of these models, sparking ideas on how we might improve and innovate further in the field.\nLearning to Publish: The day wrapped up with a Research Writing event, where we were taught the nuances of writing and publishing impactful research papers. This session was invaluable for anyone looking to contribute meaningfully to the academic community.\nA Memorable Arewa Data Science Connect: The day ended on a high note with our Arewa Data Science connect session. It was heartwarming to take a picture with my fellow Arewa Data Science folks at Indaba—strengthening our sense of community and shared purpose in advancing data science across Africa.\nA New Discovery: On a lighter note, I discovered a new country today—Eswatini! It was a humbling moment, reminding me of the endless learning opportunities that events like this bring.\nReflecting on the Day: Today was a blend of knowledge, mentorship, and community. I feel incredibly fortunate to be part of this transformative event that is shaping the future of AI and data science in Africa. Here’s to more learning, connecting, and growing together!\nStay tuned for more updates from Deep Learning Indaba 2024!"
  },
  {
    "objectID": "posts/dli-24-day-06/index.html",
    "href": "posts/dli-24-day-06/index.html",
    "title": "Deep Learning Indaba 2024 Day 6: A Journey Through Inspiring Workshops and Innovation in Health AI",
    "section": "",
    "text": "It’s hard to believe that we’re already at Day 6 of the Deep Learning Indaba 2024! Time seems to have flown by, and it’s incredible to think that there’s only one more day left. The Indaba has been a whirlwind of knowledge, networking, and inspiration. Today was a day dedicated to workshops, and the challenge was real: so many fascinating topics were on the table, but they all ran simultaneously!\nThe workshops available today were diverse and compelling, each catering to different facets of AI and machine learning in Africa. Here’s a glimpse of the workshops I had to choose from:\nWith so many great choices, it was difficult to decide, but ultimately, my interest in the intersection of AI and health led me to attend the Data Science for Health in Africa workshop."
  },
  {
    "objectID": "posts/dli-24-day-06/index.html#diving-deep-into-ai-for-health-in-africa",
    "href": "posts/dli-24-day-06/index.html#diving-deep-into-ai-for-health-in-africa",
    "title": "Deep Learning Indaba 2024 Day 6: A Journey Through Inspiring Workshops and Innovation in Health AI",
    "section": "Diving Deep into AI for Health in Africa",
    "text": "Diving Deep into AI for Health in Africa\nThe workshop, jointly organized by Sisonke Biotik and Ro’ya, was a hub of activity and collaboration. Both organizations are grassroots, participatory communities focused on advancing Machine Learning and Global Health in Africa. The sessions were highly interactive, and the atmosphere was one of shared learning and innovation.\nWe started with a practical workshop on using R for health data algorithms, which was particularly valuable for practitioners and researchers interested in leveraging open-source tools for health analytics. It was a hands-on session that allowed participants to get acquainted with different techniques and tools for analyzing complex health datasets."
  },
  {
    "objectID": "posts/dli-24-day-06/index.html#insights-from-ai-pioneers-in-health",
    "href": "posts/dli-24-day-06/index.html#insights-from-ai-pioneers-in-health",
    "title": "Deep Learning Indaba 2024 Day 6: A Journey Through Inspiring Workshops and Innovation in Health AI",
    "section": "Insights from AI Pioneers in Health",
    "text": "Insights from AI Pioneers in Health\nThe workshop featured a range of insightful presentations. Among the highlights were:\n\nMuhammad Al-Fatih’s Presentation on SKAI: SKAI (Satellite Knowledge Augmented Intelligence) is an innovative platform that utilizes AI and satellite imagery to provide real-time insights and actionable intelligence. This is particularly crucial for effective decision-making during disaster response, where timely information can save lives. Muhammad walked us through how SKAI integrates different data sources and uses AI to analyze satellite imagery, making it a game-changer for disaster management and emergency response in Africa.\nMercy Asiedu’s Talk on Multifaceted Approaches to AI for Health in Africa: Mercy’s presentation was a deep dive into the multiple ways AI can be leveraged for health in Africa. She touched on the importance of benchmark datasets for health on the continent, noting that these datasets must reflect the diverse population to be truly impactful. The talk also covered innovative AI solutions that address specific health challenges in Africa, emphasizing the need for culturally and contextually relevant technologies."
  },
  {
    "objectID": "posts/dli-24-day-06/index.html#collaborative-innovation-pitching-ideas-to-revolutionize-healthcare",
    "href": "posts/dli-24-day-06/index.html#collaborative-innovation-pitching-ideas-to-revolutionize-healthcare",
    "title": "Deep Learning Indaba 2024 Day 6: A Journey Through Inspiring Workshops and Innovation in Health AI",
    "section": "Collaborative Innovation: Pitching Ideas to Revolutionize Healthcare",
    "text": "Collaborative Innovation: Pitching Ideas to Revolutionize Healthcare\nOne of the most exciting parts of the day was a collaborative session where participants were grouped together to pitch an idea that could potentially revolutionize healthcare in Africa. This was a fantastic opportunity to brainstorm and exchange ideas with people from diverse backgrounds, ranging from data scientists to healthcare professionals. The energy in the room was palpable, and the ideas that came out of these discussions were both creative and practical."
  },
  {
    "objectID": "posts/dli-24-day-06/index.html#speed-presentations-and-ideathon-results",
    "href": "posts/dli-24-day-06/index.html#speed-presentations-and-ideathon-results",
    "title": "Deep Learning Indaba 2024 Day 6: A Journey Through Inspiring Workshops and Innovation in Health AI",
    "section": "Speed Presentations and Ideathon Results",
    "text": "Speed Presentations and Ideathon Results\nTo add to the excitement, there was a speed presentation competition where participants had a few minutes to present their innovative ideas or research findings. This fast-paced format was engaging and showcased the breadth of talent and creativity present at the Indaba.\nAdditionally, the results of the Ideathon, which was held a day earlier, were announced today. My team, NextGen Health Innovators, was thrilled to be awarded third place! We were recognized for our innovative solution aimed at improving vaccine delivery through AI and data science. Receiving our certificate and prize was a moment of pride and joy, capping off what has been an incredible day of learning and collaboration."
  },
  {
    "objectID": "posts/dli-24-day-06/index.html#reflections-and-looking-ahead",
    "href": "posts/dli-24-day-06/index.html#reflections-and-looking-ahead",
    "title": "Deep Learning Indaba 2024 Day 6: A Journey Through Inspiring Workshops and Innovation in Health AI",
    "section": "Reflections and Looking Ahead",
    "text": "Reflections and Looking Ahead\nReflecting on today, I can confidently say it has been my best day at the Indaba so far. The workshops were about learning and connecting with like-minded individuals, exchanging ideas, and envisioning a future where AI can truly make a difference in healthcare across Africa. It was a day filled with inspiration, and it reinforced my belief in the power of collaboration and innovation.\nIt’s a bit bittersweet to realize that the Indaba is almost over. Tomorrow marks the final day, and while I’m looking forward to what it has in store, I can’t help but feel a little nostalgic already. The Deep Learning Indaba has been more than just a conference; it’s been a journey of growth, learning, and connection."
  },
  {
    "objectID": "posts/dsa-algorithms-cert/index.html",
    "href": "posts/dsa-algorithms-cert/index.html",
    "title": "Mastering Data Structures and Algorithms on Coursera",
    "section": "",
    "text": "I am thrilled to announce that I have successfully completed the Foundations of Algorithms and Data Structures Specialization offered by the University of Colorado Boulder on Coursera. The specialization was excellently taught by Professor Sriram Sankaranarayanan and spanned five comprehensive courses. I embarked on this journey last year, and it has been an enriching experience."
  },
  {
    "objectID": "posts/dsa-algorithms-cert/index.html#course-overview-and-learnings",
    "href": "posts/dsa-algorithms-cert/index.html#course-overview-and-learnings",
    "title": "Mastering Data Structures and Algorithms on Coursera",
    "section": "Course Overview and Learnings",
    "text": "Course Overview and Learnings\n\n1. Algorithms for Searching, Sorting, and Indexing\nThis course laid the foundation for algorithm design and analysis. It covered:\n\nBasics of algorithm design and analysis\nSorting arrays\nData structures such as priority queues and hash functions\nApplications like Bloom filters\n\n\n\n2. Trees and Graphs: Basics\nThis course delved into tree and graph data structures. Key topics included:\n\nBasic algorithms on tree data structures\nBinary search trees and self-balancing trees\nGraph data structures and traversal algorithms\nAdvanced topics like kd-trees for spatial data and algorithms for spatial data\n\n\n\n3. Dynamic Programming and Greedy Algorithms\nIn this course, I learned essential algorithm design techniques, including:\n\nDivide and conquer\nDynamic programming\nGreedy algorithms\nIntroduction to intractability (NP-completeness)\nUsing linear/integer programming solvers for optimization problems\nAdvanced topics in data structures\n\n\n\n4. Approximate Algorithms and Linear Programming\nThis course focused on:\n\nLinear and integer programming formulations for solving algorithmic problems\nApplications in resource allocation, scheduling, task assignment, and the traveling salesperson problem\nAlgorithms for NP-hard problems with guaranteed approximation factors\nEfficient algorithms providing useful bounds on optimal solutions\n\n\n\n5. Advanced Data Structures, RSA, and Quantum Algorithms\nThe final course introduced:\n\nNumber-theory based cryptography\nBasics of quantum algorithms\nAdvanced data structures like B-Trees and Suffix Tries"
  },
  {
    "objectID": "posts/dsa-algorithms-cert/index.html#conclusion",
    "href": "posts/dsa-algorithms-cert/index.html#conclusion",
    "title": "Mastering Data Structures and Algorithms on Coursera",
    "section": "Conclusion",
    "text": "Conclusion\nCompleting this specialization has been a significant milestone in my Data Science and Machine Learning/AI journey. I am excited to apply the knowledge and skills I have gained to future projects and challenges.\nI highly recommend this specialization to anyone looking to deepen their understanding of algorithms and data structures. The courses are well-structured, and Professor Sriram Sankaranarayanan’s teaching is both clear and engaging.\nI look forward to what the future holds and I am eager to continue my learning journey in the ever-evolving field of Data Science and AI."
  },
  {
    "objectID": "posts/fellowship-conclusion/index.html",
    "href": "posts/fellowship-conclusion/index.html",
    "title": "Completion of Arewa Data Science Academy Fellowship",
    "section": "",
    "text": "It is with immense pleasure and gratitude that I announce my successful completion of the Arewa Data Science Academy fellowship. After months of unwavering dedication, deliberate practice, and continuous learning, I have achieved this milestone in my data science journey.\nThis journey would not have been possible without the help and blessings of Allah, for with His guidance, nothing is insurmountable. I would like to emphasize the significance of consistency and surrounding oneself with good companions throughout this pursuit. Additionally, I extend my heartfelt appreciation to the supportive mentors who made this entire experience more accessible and enjoyable for all of us.\nThe fellowship commenced with a comprehensive introduction to the Python programming language, laying a strong foundation for the subsequent stages of learning. We delved into the realm of machine learning, where we mastered the implementation of algorithms for both supervised and unsupervised learning, as well as reinforcement learning.\nIn addition to the core data science curriculum, we had the opportunity to enhance our overall skills by taking courses on Coursera, such as “Learning How to Learn” and “Writing in the Sciences,” which undoubtedly contributed to our growth as aspiring data scientists.\nFurthermore, we were equipped with essential tools and technologies used in the industry, including sessions on git and GitHub for version control, LaTeX for document preparation, and blogging using Quarto and similar platforms.\nI must confess that this fellowship has been a truly transformative experience, one that has enriched my understanding and ignited my passion for data science even more. I am thrilled to invite you all to explore my blog and GitHub repository, where I have documented my learning journey and projects.\nI extend my deepest gratitude to the mentors and organizers of this fellowship for their unwavering support and dedication. Their guidance has been instrumental in shaping our learning and instilling the confidence in us to pursue great things in the near future.\nWith Allah’s blessings and our collective potential, I am super confident that we will achieve remarkable feats in the field of data science. Alhamdulillah!\nThank you all for being a part of this incredible journey."
  },
  {
    "objectID": "posts/fizzbuzz-javascript/index.html",
    "href": "posts/fizzbuzz-javascript/index.html",
    "title": "JavaScript FizzBuzz Solutions",
    "section": "",
    "text": "FizzBuzz is a common programming problem that is often used as an exercise in basic programming logic and algorithm development. The problem is typically presented as follows:\n\nWrite a program that prints the numbers from 1 to 100. For multiples of three, print “Fizz” instead of the number, and for multiples of five, print “Buzz” instead of the number. For numbers that are multiples of both three and five, print “FizzBuzz”.\n\nIn essence, the FizzBuzz problem requires the programmer to loop through a sequence of numbers, identify which ones are multiples of 3, 5, or both, and print the appropriate text instead of the number. The problem is often used as a simple test of a programmer’s ability to use loops, conditionals, and basic programming concepts like variables and functions. It can be solved using a variety of programming languages, and is often used as an interview question for entry-level software development positions.\nIn this article, I present, as a follow-up to my Python article, 15 different solutions to the FizzBuzz problem in the JavaScript programming language. This is to highlight the flair of JavaScript.\nHere are 15 cool FizzBuzz solutions in JavaScript:\n\nUsing a for loop and if/else statements:\n\nfor (let i = 1; i &lt;= 100; i++) {\n  if (i % 3 === 0 && i % 5 === 0) {\n    console.log(\"FizzBuzz\");\n  } else if (i % 3 === 0) {\n    console.log(\"Fizz\");\n  } else if (i % 5 === 0) {\n    console.log(\"Buzz\");\n  } else {\n    console.log(i);\n  }\n}\n\nUsing a for loop and a switch statement:\n\nfor (let i = 1; i &lt;= 100; i++) {\n  switch (true) {\n    case i % 3 === 0 && i % 5 === 0:\n      console.log(\"FizzBuzz\");\n      break;\n    case i % 3 === 0:\n      console.log(\"Fizz\");\n      break;\n    case i % 5 === 0:\n      console.log(\"Buzz\");\n      break;\n    default:\n      console.log(i);\n  }\n}\n\nUsing a while loop and a ternary operator:\n\nlet i = 1;\nwhile (i &lt;= 100) {\n  console.log(\n    i % 3 === 0 && i % 5 === 0 ? \"FizzBuzz\" :\n    i % 3 === 0 ? \"Fizz\" :\n    i % 5 === 0 ? \"Buzz\" :\n    i\n  );\n  i++;\n}\n\nUsing recursion:\n\nfunction fizzBuzzRecursive(num) {\n  if (num === 0) {\n    return;\n  }\n  fizzBuzzRecursive(num - 1);\n  console.log(\n    num % 3 === 0 && num % 5 === 0 ? \"FizzBuzz\" :\n    num % 3 === 0 ? \"Fizz\" :\n    num % 5 === 0 ? \"Buzz\" :\n    num\n  );\n}\nfizzBuzzRecursive(100);\n\nUsing Array.from() and map():\n\nArray.from({ length: 100 }, (_, i) =&gt; {\n  const num = i + 1;\n  return (\n    num % 3 === 0 && num % 5 === 0 ? \"FizzBuzz\" :\n    num % 3 === 0 ? \"Fizz\" :\n    num % 5 === 0 ? \"Buzz\" :\n    num\n  );\n}).forEach(console.log);\n\nUsing a do-while loop and a conditional operator:\n\nlet i = 0;\ndo {\n  i++;\n  console.log(i % 3 === 0 && i % 5 === 0 ? \"FizzBuzz\" : i % 3 === 0 ? \"Fizz\" : i % 5 === 0 ? \"Buzz\" : i);\n} while (i &lt; 100);\n\nUsing a for loop and an object to store the strings:\n\nconst strings = { 3: \"Fizz\", 5: \"Buzz\" };\nfor (let i = 1; i &lt;= 100; i++) {\n  let output = \"\";\n  for (const key in strings) {\n    if (i % key === 0) {\n      output += strings[key];\n    }\n  }\n  console.log(output || i);\n}\n\nUsing recursion and an array to store the strings:\n\nconst fizzBuzzStrings = [\"\", \"\", \"Fizz\", \"\", \"Buzz\", \"Fizz\", \"\", \"\", \"Fizz\", \"Buzz\", \"\", \"Fizz\", \"\", \"\", \"FizzBuzz\"];\nfunction fizzBuzzRecursive(num) {\n  if (num === 0) {\n    return;\n  }\n  fizzBuzzRecursive(num - 1);\n  console.log(fizzBuzzStrings[num % 15] || num);\n}\nfizzBuzzRecursive(100);\n\nUsing a for loop and a template literal:\n\nfor (let i = 1; i &lt;= 100; i++) {\n  console.log(`${i % 3 ? \"\" : \"Fizz\"}${i % 5 ? \"\" : \"Buzz\"}` || i);\n}\n\nUsing a for loop and a function to generate the output:\n\nfunction generateOutput(num) {\n  let output = \"\";\n  if (num % 3 === 0) {\n    output += \"Fizz\";\n  }\n  if (num % 5 === 0) {\n    output += \"Buzz\";\n  }\n  return output || num;\n}\nfor (let i = 1; i &lt;= 100; i++) {\n  console.log(generateOutput(i));\n}\n\nUsing a for loop and a nested ternary operator:\n\nfor (let i = 1; i &lt;= 100; i++) {\n  console.log(i % 3 === 0 ? (i % 5 === 0 ? \"FizzBuzz\" : \"Fizz\") : (i % 5 === 0 ? \"Buzz\" : i));\n}\n\nUsing a for loop and an array to store the numbers and strings:\n\nconst arr = Array.from({ length: 100 }, (_, i) =&gt; i + 1);\narr.forEach((num) =&gt; {\n  const output = [\"Fizz\"][num % 3] || \"\";\n  const output2 = [\"Buzz\"][num % 5] || \"\";\n  console.log(`${output}${output2}` || num);\n});\n\nUsing a for loop and a self-invoking function to generate the output:\n\nfor (let i = 1; i &lt;= 100; i++) {\n  console.log((function () {\n    let output = \"\";\n    if (i % 3 === 0) {\n      output += \"Fizz\";\n    }\n    if (i % 5 === 0) {\n      output += \"Buzz\";\n    }\n    return output || i;\n  })());\n}\n\nUsing a for loop and two switch statements:\n\nfor (let i = 1; i &lt;= 100; i++) {\n  switch (true) {\n    case i % 3 === 0:\n      switch (true) {\n        case i % 5 === 0:\n          console.log(\"FizzBuzz\");\n          break;\n        default:\n          console.log(\"Fizz\");\n          break;\n      }\n      break;\n    case i % 5 === 0:\n      console.log(\"Buzz\");\n      break;\n    default:\n      console.log(i);\n      break;\n  }\n}\n\nUsing a for loop and a map() method to generate the output:\n\nfor (let i = 1; i &lt;= 100; i++) {\n  const output = [i, \"\", \"\", \"Fizz\", \"\", \"Buzz\", \"Fizz\", \"\", \"\", \"Fizz\", \"Buzz\", \"\", \"Fizz\", \"\", \"\", \"FizzBuzz\"][i % 15];\n  console.log(output);\n}\nIn conclusion, there are several ways to approach the FizzBuzz problem in JavaScript and I present just 15 above. As you learn new programming languages, try solving problems in different creative ways as this can boost your understanding of the new programming language and your grounding in programming in general."
  },
  {
    "objectID": "posts/forum-devotee/index.html",
    "href": "posts/forum-devotee/index.html",
    "title": "The Power of Persistence: How 365 Days of Consistency Transformed My Learning Journey",
    "section": "",
    "text": "After 365 consecutive days of active participation, I’m thrilled to announce that I’ve achieved Devotee status on the DeepLearning.AI forum. This milestone is more than just a badge—it shows the power of unwavering commitment and the compound effect of daily habits.\n\nThe journey to Devotee status is as unforgiving as it is rewarding. Missing even a single day means starting over from scratch. Imagine the dedication required to maintain this streak for an entire year, knowing that a momentary lapse on day 364 would reset your progress to zero. It’s this level of consistency that separates casual participants from true devotees.\nMy path to this achievement began with smaller milestones: the 10-day Enthusiast badge, followed by the 100-day Aficionado badge. Each step reinforced my commitment and built momentum towards the ultimate goal. Now, as only the fifth person in over four years to reach Devotee status, I find myself in the company of the forum’s most dedicated members.\nBut this streak is just one facet of a larger commitment to personal growth through consistent effort. Parallel to my DeepLearning.AI journey, I’m maintaining a 196-day streak on DataCamp and a 261 day streak on GitHub. These aren’t just numbers; they represent a fundamental shift in how I approach learning and self-improvement.\nThe concept of “atomic habits,” popularized by James Clear, has been instrumental in my approach. The idea is simple yet powerful: tiny, consistent changes accumulate over time, leading to significant personal transformation. By showing up every day, even when motivation wanes, I’m not just learning new skills—I’m reshaping my identity into someone who pursues growth relentlessly.\nThis journey has taught me several valuable lessons:\n\nConsistency trumps intensity: Small, daily efforts are more impactful than sporadic bursts of activity.\nProgress compounds: What seems like slow progress early on accelerates as habits become ingrained.\nIdentity-based habits stick: By viewing myself as a “lifelong learner,” maintaining these streaks becomes natural.\nCommunity matters: The DeepLearning.AI forum and other platforms provide accountability and support.\n\nMoving forward, I’m committed to channeling my energy into endeavors that contribute to my professional and personal growth. This means setting ambitious targets, minimizing distractions like excessive social media use, and continuously reassessing my habits to ensure they align with my long-term goals.\nAs I reflect on this achievement, I’m filled with gratitude for the guidance and opportunities that have brought me here. I’m also excited about the future, knowing that the habits I’ve built will serve as a foundation for even greater accomplishments.\nTo those embarking on their own journey of consistent self-improvement, remember: every day is an opportunity to take a step forward. The path may be challenging, but the rewards—in knowledge gained, skills developed, and personal growth achieved—are immeasurable.\nAlhamdulillah for this milestone, and may Allah continue to guide and bless our efforts towards meaningful growth and contribution."
  },
  {
    "objectID": "posts/genomic-data-analysis/index.html",
    "href": "posts/genomic-data-analysis/index.html",
    "title": "Completing the Analyzing Genomic Data in R track on DataCamp",
    "section": "",
    "text": "I just completed DataCamp’s “Analyzing Genomic Data in R” track, which was an incredible dive into using R and Bioconductor for genomic analysis. This track had four in-depth courses that helped me build skills for analyzing genomic datasets:\nIn this post, I’ll briefly walk you through each course and share what I learned and why it was so exciting."
  },
  {
    "objectID": "posts/genomic-data-analysis/index.html#introduction-to-bioconductor-in-r",
    "href": "posts/genomic-data-analysis/index.html#introduction-to-bioconductor-in-r",
    "title": "Completing the Analyzing Genomic Data in R track on DataCamp",
    "section": "Introduction to Bioconductor in R",
    "text": "Introduction to Bioconductor in R\nThis course was my gateway to Bioconductor—an essential resource for genomic analysis in R. I learned how to install packages with the BiocInstaller, work with different types of genomic data, and explore real-life datasets like those for fungi, viruses, and even plants. Tools like BioStrings, GenomicRanges, and ShortRead made manipulating and filtering raw data surprisingly accessible.\nOne highlight was learning to use built-in datasets (BSgenome and TxDb) and apply functions to search for patterns in genomic sequences. Quality control was another important aspect, using ShortRead and Rqc to ensure that my data was clean and usable—a crucial step in genomic research."
  },
  {
    "objectID": "posts/genomic-data-analysis/index.html#rna-seq-with-bioconductor-in-r",
    "href": "posts/genomic-data-analysis/index.html#rna-seq-with-bioconductor-in-r",
    "title": "Completing the Analyzing Genomic Data in R track on DataCamp",
    "section": "RNA-Seq with Bioconductor in R",
    "text": "RNA-Seq with Bioconductor in R\nIn this course, I got hands-on with RNA sequencing (RNA-Seq) data, focusing on differential expression analysis—identifying genes that express differently between experimental conditions. I used DESeq2 to analyze gene expression between fibrosis and normal samples, which was a powerful introduction to understanding biological significance in genomics.\nAn interesting takeaway was the reminder that data interpretation doesn’t stop at identifying significant genes. I learned that experimental validation is key—around 5% of “significant” results are false positives, and ensuring that our findings are accurate requires lab-based verification. Additionally, I explored how functional analysis can help us see the biological meaning behind the data by identifying enriched processes or pathways."
  },
  {
    "objectID": "posts/genomic-data-analysis/index.html#differential-expression-analysis-with-limma-in-r",
    "href": "posts/genomic-data-analysis/index.html#differential-expression-analysis-with-limma-in-r",
    "title": "Completing the Analyzing Genomic Data in R track on DataCamp",
    "section": "Differential Expression Analysis with limma in R",
    "text": "Differential Expression Analysis with limma in R\nThis course added another layer to my understanding of differential expression—specifically using limma, a versatile and widely-used Bioconductor package. I visualized gene expression levels with plotDensities and normalized raw data to ensure reliable analysis. One key aspect was principal component analysis (plotMDS), which helped me see whether data variation matched my experimental variables or if there were other technical factors at play.\nI also learned to identify important genes using volcano plots and assess results using p-value distributions. The course highlighted how essential quality control is, emphasizing the need for careful examination to avoid misleading conclusions. Wrapping up, I conducted gene set enrichment analysis to explore biological pathways using KEGG and Gene Ontology databases—this really helped broaden my perspective from individual genes to broader biological systems."
  },
  {
    "objectID": "posts/genomic-data-analysis/index.html#chip-seq-with-bioconductor-in-r",
    "href": "posts/genomic-data-analysis/index.html#chip-seq-with-bioconductor-in-r",
    "title": "Completing the Analyzing Genomic Data in R track on DataCamp",
    "section": "ChIP-seq with Bioconductor in R",
    "text": "ChIP-seq with Bioconductor in R\nChIP-seq (Chromatin Immunoprecipitation Sequencing) was the final frontier in this track, and it was challenging but incredibly rewarding. I learned to import ChIP-seq data using rtracklayer and GenomicAlignments, visualize it with Gviz, and ensure its quality by dealing with repetitive regions through blacklisting and QC reports with ChIPQC.\nThe analysis culminated in using DiffBind to identify differentially bound peaks and create heatmaps and PCA plots to understand sample clustering. Finally, I used the chipenrich package to link peaks to nearby genes and performed enrichment analysis to see which biological processes were associated with these binding events. This part of the course really helped tie everything together and made me confident about analyzing real-world ChIP-seq data."
  },
  {
    "objectID": "posts/genomic-data-analysis/index.html#reflections-on-the-journey",
    "href": "posts/genomic-data-analysis/index.html#reflections-on-the-journey",
    "title": "Completing the Analyzing Genomic Data in R track on DataCamp",
    "section": "Reflections on the Journey",
    "text": "Reflections on the Journey\nCompleting this track gave me practical skills to handle various genomic datasets and analyze them meaningfully. Each course built on the previous one, reinforcing core concepts while adding new techniques, ultimately providing a well-rounded introduction to genomic analysis in R.\nMy next step is to apply what I’ve learned to real datasets—perhaps even contribute to open research initiatives. Bioconductor’s community and support forums will be great allies as I dive deeper into genomics. As I am seriously considering grad school, this is a skill that will help me immensely in the pursuit of my goals."
  },
  {
    "objectID": "posts/ggplot/index.html",
    "href": "posts/ggplot/index.html",
    "title": "Data Visualization with R using ggplot2",
    "section": "",
    "text": "In this article, I attempt to briefly display the ease of data visualization using the ggplot2 library in R. As always, the first thing is to import the library using the library syntax. Here, I import tidyverse which includes ggplot2 as a core member of the tidyverse. I also import palmerpenguins as it contains the penguins dataset I will like to use.\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(palmerpenguins)\nLooking closely at the mpg dataset to see what variables it contains.\nglimpse(mpg)\n\nRows: 234\nColumns: 11\n$ manufacturer &lt;chr&gt; \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"…\n$ model        &lt;chr&gt; \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4 quattro\", \"…\n$ displ        &lt;dbl&gt; 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.0, 2.0, 2.…\n$ year         &lt;int&gt; 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999, 1999, 200…\n$ cyl          &lt;int&gt; 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 8, 8, …\n$ trans        &lt;chr&gt; \"auto(l5)\", \"manual(m5)\", \"manual(m6)\", \"auto(av)\", \"auto…\n$ drv          &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"4\", \"4\", \"4\", \"4\", \"4…\n$ cty          &lt;int&gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 17, 17, 1…\n$ hwy          &lt;int&gt; 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25, 25, 25, 2…\n$ fl           &lt;chr&gt; \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p…\n$ class        &lt;chr&gt; \"compact\", \"compact\", \"compact\", \"compact\", \"compact\", \"c…\nChecking out the palmer penguins dataset too.\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\nNow, I use the easy to follow ggplot syntax to plot mpg dataset, displ against hwy. I also introduce a smooth line to accurately see the way the variables relate.\nggplot(data=mpg,mapping=aes(x= hwy, y= displ)) + geom_point() + geom_smooth(method= \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\nSecond plot is on the penguins data. I compare the bill length and bill depth and see how species affect the relationship.\nggplot(\n  data = penguins,\n  mapping = aes(\n    x = bill_length_mm, y = bill_depth_mm, \n    color = species, shape = species\n  )\n) +\n  geom_point() +\n  labs(color = \"Species\") +\n  guides(\n    shape = guide_legend(title = \"Species\", override.aes = list(color = NULL))\n  )\n\nWarning: Removed 2 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "posts/ggplot/index.html#conclusion",
    "href": "posts/ggplot/index.html#conclusion",
    "title": "Data Visualization with R using ggplot2",
    "section": "Conclusion",
    "text": "Conclusion\nThe ggplot2 library offers an easy way to do data visualization in R. However, it is important to note that mastering visualization requires a lot of practice and a single article is not enough. Let’s all keep learning friends."
  },
  {
    "objectID": "posts/how-to-learn/index.html",
    "href": "posts/how-to-learn/index.html",
    "title": "Necessity of Developing a Learning Method",
    "section": "",
    "text": "The Arewa Data Science fellowship that I am currently enrolled in required that we complete Barbara Oakley’s and Terrence Sejnowski’s MOOC (massive open online course) on coursera, ‘Learning How to Learn’, as part of the curriculum in the first stage of the fellowship. The fellowship intends to make us efficient learners as we prepare to learn increasingly complex concepts in machine learning and deep learning.I recently completed the course and I found it necessary to share some of the techniques that I have implemented to boost my learning efficiency.\nThe most important and impactful thing is making a good plan. Journaling my learning journey and taking note of techniques that worked and those that did not work out so well for me has helped me hone in on the most efficient techniques I can use. A part of my planning includes making sure that I have a good environment for learning, free of distractions. And after that, adopt the pomodoro technique to ensure that I am completely focussed and make the best use of my learning time. All these are documented as I have found that documentation is an effective way of tracking progress.\nHere are some advices I have for improving learning efficiency: 1. Chunking: Always breakdown the material and ensure that you have dialled down to the main gist. This will improve recall even after a long time 2. Interleaving: I find interleaving better than blocked practice. In interleaving, one tries to mix up different subject materials to prevent boredom whereas blocked practice focuses on completely understanding a subject material in one sitting preferably. Here, I mean allocating sufficient time for different subject materials in a sitting. Interleaving works best for me and I recommend it. 3. Use those flashcards: Memory aids are important in retaining information and I advise that you try some techniques out and settle on the ones that work best for your learning style. 4. Make handwritten notes: Writing helps in consolidating the information in the brain and it is a good way of improving recall and understanding of study material. 5. Spaced repetition: This is far better than trying to ‘conclude’ the learning in one sitting. One can be tempted to read a chapter or topic multiple times in one sitting with no plans of ever revisiting the chapter. This does not work. It is far more effective to plan repeat readings ahead of time. Reading the chapter in different sittings is a more effective way to learn according to the course.\nIn conclusion, the most important step is to have a plan and the next thing is to implement while documenting and iterating. Clever hardwork leads to success."
  },
  {
    "objectID": "posts/learning-motivation/index.html",
    "href": "posts/learning-motivation/index.html",
    "title": "Juggling Code and Career: My Arewa Data Science 30 Days Of Python Learning Journey Amidst a Full Schedule",
    "section": "",
    "text": "As part of the first cohort of the Arewa Data Science Academy Data Science and Machine Learning fellowship, embarking on the thirty days of Python challenge, I found myself navigating through the demanding waters of 12-hour work shifts. Despite these professional commitments, I remained steadfast in my participation in the weekend sessions, often balancing work and learning simultaneously. My thirst for knowledge wasn’t confined to my job; it extended to personal interests like improving my French and Arabic, a testament to my multifaceted curiosity. This same drive propelled me towards learning Python, a field I ventured into without any formal background in computer science.\nIt’s enlightening to realize that, despite our hectic schedules, we often find pockets of time for activities like browsing social media. This observation led me to a pivotal conclusion: if we can dedicate time to scrolling through feeds, we can surely allocate moments for self-improvement and learning. My approach was methodical yet flexible - committing to consistent, daily progress, and embracing a journey of gradual improvement rather than seeking instant perfection. I wasn’t fixated on getting everything right on my first attempt. Rather, I aimed to complete each day’s exercises, occasionally revisiting previous tasks, to solidify my understanding. This consistent effort was a clear signal of my commitment and eagerness to learn, something that I hoped would be apparent to my mentors. I have since completed the python challenge and eventually the data science fellowship, but the lessons and experiences remain.\nIn sharing this experience, my aim is to offer a piece of advice to fellow learners, especially those balancing their professional lives with personal development goals. It’s crucial to assess the role of learning in your life and to find ways to integrate it into your daily routine. The journey of learning programming, particularly a versatile language like Python, is not only about acquiring technical skills. It significantly enhances cognitive abilities, problem-solving skills, and overall mental agility. The impact of this commitment is profound, stretching beyond immediate learning outcomes to influence your professional trajectory and personal growth.\nWhile we are fortunate to have mentors to guide us, the crux of learning lies in our own hands. It’s about taking responsibility for our growth, proactively seeking help when needed, and not shying away from the challenges that come with stepping out of our comfort zones. This journey has been a rewarding one, filled with lessons that transcend the realm of programming, offering insights into persistence, time management, and the power of small, consistent steps towards a larger goal.\nIn conclusion, as you embark on your own learning adventures, remember that the path may not always be easy, but the rewards, both tangible and intangible, are truly worth the effort. Embrace the challenge, cherish the progress, and always keep the flame of curiosity burning bright."
  },
  {
    "objectID": "posts/math-for-ml/index.html",
    "href": "posts/math-for-ml/index.html",
    "title": "My Journey into the Mathematics in Machine Learning",
    "section": "",
    "text": "Have you ever faced the challenge of converting TensorFlow code to PyTorch, wrestling with tensor dimensions, or trying to understand how transpose matrices work in machine learning? If that sounds familiar, my journey will resonate with you. Recently, I took on these challenges, and it completely changed how I view mathematics in the context of machine learning.\nIt all started when I needed to implement a convolutional neural network, specifically the U-Net architecture, in PyTorch. The difference between PyTorch’s channels-first format and TensorFlow’s channels-last format forced me to understand the details of how tensors work in both frameworks. My goal was to get a solid grasp of tensor geometry and understand the role of transpose matrices in these implementations.\nTo tackle this, I decided to strengthen my foundation in mathematics. I began by taking the Mathematics for Machine Learning course by DeepLearning.ai on Coursera. This was a game-changer. The instructor, Luis Serrano, explained complex topics like linear algebra, calculus, and statistics in a way that was clear and engaging. For the first time, I felt confident applying math to solve real problems in machine learning.\nEncouraged by this progress, I went further and enrolled in the Mathematics for Machine Learning specialization by Imperial College London. The professors – David Dye, Samuel J Cooper, and Marc Deisenroth – walked me through the finer details of probability, statistics, and advanced concepts like principal component analysis. Although some parts were challenging, I learned to persevere, and the knowledge I gained significantly improved my understanding of machine learning.\nLooking back, I’m amazed at how much these courses have deepened my intuition for machine learning. They’ve given me the tools to confidently tackle complex problems involving the mathematical structures that underpin algorithms. I only wish I had started this journey earlier!\nHaving already completed the Machine Learning and Deep Learning specializations, I’m now drawn to courses like AI for Medicine and Generative Adversarial Networks. These topics feel like the next exciting steps in my learning journey.\nMoving forward, I’m determined to apply what I’ve learned. With skills in Python and R, I plan to strengthen my understanding by working on real-world problems in different frameworks. I know this is just the beginning, and I’m excited to continue learning and growing.\nThis experience has shown me how critical mathematics is in machine learning. It’s the foundation for understanding the elegant, complex algorithms shaping our world today. I look forward to the next phase of this journey – one filled with new challenges, opportunities, and growth."
  },
  {
    "objectID": "posts/medical-eval/index.html",
    "href": "posts/medical-eval/index.html",
    "title": "Metrics for Medical Diagnosis Evaluation",
    "section": "",
    "text": "Medical diagnosis is a multifaceted endeavour that relies on a comprehensive assessment of various factors, including symptoms, tests, and medical history. Conditional probability, which calculates the likelihood of an event occurring given another event, plays a pivotal role in evaluating the effectiveness of medical diagnoses. In the context of healthcare, this involves estimating the probability of a patient having a disease, given that they have tested positive for it."
  },
  {
    "objectID": "posts/medical-eval/index.html#introduction",
    "href": "posts/medical-eval/index.html#introduction",
    "title": "Metrics for Medical Diagnosis Evaluation",
    "section": "",
    "text": "Medical diagnosis is a multifaceted endeavour that relies on a comprehensive assessment of various factors, including symptoms, tests, and medical history. Conditional probability, which calculates the likelihood of an event occurring given another event, plays a pivotal role in evaluating the effectiveness of medical diagnoses. In the context of healthcare, this involves estimating the probability of a patient having a disease, given that they have tested positive for it."
  },
  {
    "objectID": "posts/medical-eval/index.html#understanding-accuracy",
    "href": "posts/medical-eval/index.html#understanding-accuracy",
    "title": "Metrics for Medical Diagnosis Evaluation",
    "section": "Understanding Accuracy",
    "text": "Understanding Accuracy\nOne of the most commonly used metrics in medical diagnosis is accuracy. It gauges the overall proportion of accurate diagnoses, derived from correctly identifying patients with or without a specific ailment. Accuracy can be mathematically expressed as a combination of sensitivity and specificity, both of which are vital metrics.\n\\(Accuracy = Sensitivity \\times Prevalence + Specificity \\times (1 - Prevalence)\\)"
  },
  {
    "objectID": "posts/medical-eval/index.html#sensitivity-and-specificity",
    "href": "posts/medical-eval/index.html#sensitivity-and-specificity",
    "title": "Metrics for Medical Diagnosis Evaluation",
    "section": "Sensitivity and Specificity",
    "text": "Sensitivity and Specificity\nSensitivity measures the ability of a diagnostic method or model to correctly identify patients who have the disease, while specificity measures its ability to accurately identify those without the disease. These metrics can also be understood in terms of conditional probabilities, simplifying them to the probability of a positive or negative test result given a patient’s disease status.\n\\(Sensitivity = \\dfrac{\\,number\\,of\\,positive\\,and\\,disease}{\\,number\\,of\\,disease}\\)\n\\(Specificity = \\dfrac{\\,number\\,of\\,negative\\,and\\,normal}{\\,number\\,of\\,normal}\\)\nIn terms of conditional probabilities:\n\\(Sensitivity = P(positive|disease)\\) \\(Specificity = P(negative|normal)\\)"
  },
  {
    "objectID": "posts/medical-eval/index.html#positive-predictive-value-and-negative-predictive-value",
    "href": "posts/medical-eval/index.html#positive-predictive-value-and-negative-predictive-value",
    "title": "Metrics for Medical Diagnosis Evaluation",
    "section": "Positive Predictive Value and Negative Predictive Value",
    "text": "Positive Predictive Value and Negative Predictive Value\nTo assess the likelihood of a patient having or not having the disease based on their test results, we employ positive predictive value (PPV) and negative predictive value (NPV). These metrics provide insight into the probabilities of disease presence or absence given a positive or negative test result, respectively. This is more useful and practical than just sensitivity and specificity\n\\(PPV = \\dfrac{\\,number\\,of\\,positive\\,and\\,disease}{\\,number\\,of\\,positive}\\)\n\\(NPV = \\dfrac{\\,number\\,of\\,negative\\,and\\,normal}{\\,number\\,of\\,negative}\\)\n\nLeveraging Bayes’ Rule:\nTo calculate PPV and NPV, Bayes’ rule, a formula relating conditional probabilities, is employed. This rule offers a nuanced perspective by taking into account sensitivity, specificity, and prevalence—the proportion of patients with the disease in the population.\nUsing conditional probabilities, we write:\n\\(PPV = P(disease|positive)\\)\n\\(NPV = P(normal|negative)\\)"
  },
  {
    "objectID": "posts/medical-eval/index.html#confusion-matrix",
    "href": "posts/medical-eval/index.html#confusion-matrix",
    "title": "Metrics for Medical Diagnosis Evaluation",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\nA confusion matrix serves as a valuable tool to visualize the performance of a diagnostic method or model. It displays true positives, true negatives, false positives, and false negatives for different thresholds or cut-off points, which can be adjusted to modify the diagnostic performance metrics.\nA confusion matrix can be represented as:\n\n\n\n\nPositive\nNegative\n\n\n\n\nDisease\nTP\nFN\n\n\nNormal\nFP\nTN\n\n\n\nwhere: TP = True Positives (number of)\nFP = False Positives\nFN = False Negatives\nTN = True Negatives"
  },
  {
    "objectID": "posts/medical-eval/index.html#precision-and-recall",
    "href": "posts/medical-eval/index.html#precision-and-recall",
    "title": "Metrics for Medical Diagnosis Evaluation",
    "section": "Precision and Recall",
    "text": "Precision and Recall\nPrecision measures the proportion of correctly identified positive test results, while recall assesses the proportion of patients with the disease that are accurately identified. These metrics are also synonymous with positive predictive value (PPV) and sensitivity, respectively.\n\\(Precision = \\dfrac{TP}{TP+FP}\\)\n\\(Recall = \\dfrac{TP}{TP+FN}\\)\nPrecision and recall are also known as positive predictive value (PPV) and sensitivity, respectively. Therefore, they are equivalent to:\n\\(Precision = PPR\\)\n\\(Recall = Sensitivity\\)"
  },
  {
    "objectID": "posts/medical-eval/index.html#f1-score",
    "href": "posts/medical-eval/index.html#f1-score",
    "title": "Metrics for Medical Diagnosis Evaluation",
    "section": "F1-Score",
    "text": "F1-Score\nThe F1-score combines precision and recall into a single metric, which is the harmonic mean of both values. This metric is also known as the Dice coefficient score, reflecting the similarity between two sets.\n\\(F1\\,Score  = 2 \\times \\dfrac{PR}{P+R}\\)"
  },
  {
    "objectID": "posts/medical-eval/index.html#evaluating-diagnostic-methods-with-roc-curves",
    "href": "posts/medical-eval/index.html#evaluating-diagnostic-methods-with-roc-curves",
    "title": "Metrics for Medical Diagnosis Evaluation",
    "section": "Evaluating Diagnostic Methods with ROC Curves",
    "text": "Evaluating Diagnostic Methods with ROC Curves\nTo compare different diagnostic methods or models, the receiver operating characteristic (ROC) curve is a valuable tool. It illustrates the trade-off between sensitivity and specificity at varying thresholds. A high-performing diagnostic method or model should yield a ROC curve closely resembling the top-left corner of the plot, with the area under the ROC curve (AUC) indicating its overall performance.\n\n\n\nROC Curve"
  },
  {
    "objectID": "posts/medical-eval/index.html#conclusion",
    "href": "posts/medical-eval/index.html#conclusion",
    "title": "Metrics for Medical Diagnosis Evaluation",
    "section": "Conclusion",
    "text": "Conclusion\nThe multifaceted landscape of medical diagnosis evaluation encompasses a spectrum of metrics and tools. Understanding these metrics and their mathematical underpinnings is crucial for accurately assessing the performance of diagnostic methods and models in healthcare."
  },
  {
    "objectID": "posts/nlp/index.html",
    "href": "posts/nlp/index.html",
    "title": "Harnessing the Power of Natural Language Processing: A Guide to Python and R Packages and Practical Use-Cases",
    "section": "",
    "text": "In today’s digital age, the sheer volume of text-based data being generated has grown exponentially. Extracting meaningful insights and knowledge from this vast amount of information is a daunting task, however, Natural Language Processing (NLP) offers a solution. NLP combines artificial intelligence, computational linguistics, and computer science to enable computers to understand, interpret, and generate human language. In this article, we will explore popular Python and R packages (as both languages are dear to my heart) for NLP and delve into some practical use-cases that demonstrate the power of this technology."
  },
  {
    "objectID": "posts/nlp/index.html#introduction",
    "href": "posts/nlp/index.html#introduction",
    "title": "Harnessing the Power of Natural Language Processing: A Guide to Python and R Packages and Practical Use-Cases",
    "section": "",
    "text": "In today’s digital age, the sheer volume of text-based data being generated has grown exponentially. Extracting meaningful insights and knowledge from this vast amount of information is a daunting task, however, Natural Language Processing (NLP) offers a solution. NLP combines artificial intelligence, computational linguistics, and computer science to enable computers to understand, interpret, and generate human language. In this article, we will explore popular Python and R packages (as both languages are dear to my heart) for NLP and delve into some practical use-cases that demonstrate the power of this technology."
  },
  {
    "objectID": "posts/nlp/index.html#python-packages-for-nlp",
    "href": "posts/nlp/index.html#python-packages-for-nlp",
    "title": "Harnessing the Power of Natural Language Processing: A Guide to Python and R Packages and Practical Use-Cases",
    "section": "Python Packages for NLP",
    "text": "Python Packages for NLP\n\nNLTK (Natural Language Toolkit): NLTK is a comprehensive library that provides a wide range of tools and algorithms for NLP tasks. It offers functionalities for tokenization, stemming, lemmatization, part-of-speech tagging, named entity recognition, sentiment analysis, and much more. NLTK is an excellent choice for beginners due to its extensive documentation and user-friendly interfaces.\nspaCy:\n\nspaCy is a powerful and efficient library for NLP tasks. It is designed to be fast and scalable, making it suitable for processing large volumes of text. spaCy provides pre-trained models for various languages, allowing users to perform tasks such as tokenization, entity recognition, dependency parsing, and text classification with ease.\n\nTextBlob :\n\nTextBlob is a simple and intuitive library built on top of NLTK. It provides an easy-to-use API for common NLP tasks, including sentiment analysis, part-of-speech tagging, noun phrase extraction, translation, and more. TextBlob also includes a sentiment lexicon and can be extended with custom models."
  },
  {
    "objectID": "posts/nlp/index.html#r-packages-for-nlp",
    "href": "posts/nlp/index.html#r-packages-for-nlp",
    "title": "Harnessing the Power of Natural Language Processing: A Guide to Python and R Packages and Practical Use-Cases",
    "section": "R Packages for NLP",
    "text": "R Packages for NLP\n\ntm (Text Mining Package): The tm package is a popular choice for text mining and NLP in R. It offers functions for data preprocessing, such as text cleaning, tokenization, and stemming. The package also provides convenient utilities for creating document-term matrices and performing various text mining operations like topic modeling and sentiment analysis.\ntidytext:\n\ntidytext is an R package that leverages the principles of tidy data and the tidyverse ecosystem for NLP tasks. It provides a consistent and straightforward interface for working with text data, allowing users to perform operations like tokenization, n-gram creation, sentiment analysis, and term frequency-inverse document frequency (TF-IDF) calculations.\n\nNLP:\n\nThe NLP package in R provides a broad set of functionalities for natural language processing. It includes tools for part-of-speech tagging, named entity recognition, tokenization, stemming, and sentiment analysis. The package also offers utilities for creating word clouds, visualizing term frequencies, and exploring co-occurrence networks."
  },
  {
    "objectID": "posts/nlp/index.html#practical-use-cases-of-nlp",
    "href": "posts/nlp/index.html#practical-use-cases-of-nlp",
    "title": "Harnessing the Power of Natural Language Processing: A Guide to Python and R Packages and Practical Use-Cases",
    "section": "Practical Use-Cases of NLP",
    "text": "Practical Use-Cases of NLP\n\nSentiment Analysis: NLP allows us to analyze sentiment from text data, enabling businesses to understand customer opinions, evaluate product reviews, and gauge public sentiment towards brands. By using sentiment analysis, companies can make data-driven decisions to improve their products and services, enhance customer satisfaction, and manage their online reputation effectively.\nText Classification:\n\nNLP techniques can be employed for automatic text classification, where documents are categorized into predefined classes. This has various applications, such as spam email detection, sentiment-based classification, topic categorization, and content filtering. Text classification helps in organizing and retrieving information efficiently, saving time and effort.\n\nNamed Entity Recognition (NER) cont’d:\n\nNER allows for the extraction of key information from large volumes of text, aiding in tasks like information retrieval, knowledge graph construction, and data integration. For example, in the healthcare industry, NER can be used to identify medical entities from clinical records, enabling better patient care and medical research.\n\nMachine Translation:\n\nNLP plays a pivotal role in machine translation, making it possible to automatically translate text from one language to another. This has immense practical value in global communication, cross-border collaborations, and content localization. Machine translation systems like Google Translate leverage NLP techniques to deliver accurate and fluent translations.\n\nChatbots and Virtual Assistants:\n\nNatural Language Processing is at the core of chatbot and virtual assistant technology. These AI-powered systems can understand and respond to human queries, provide customer support, and automate routine tasks. NLP enables chatbots to process and interpret natural language inputs, generating relevant and context-aware responses, enhancing user experiences."
  },
  {
    "objectID": "posts/nlp/index.html#conclusion",
    "href": "posts/nlp/index.html#conclusion",
    "title": "Harnessing the Power of Natural Language Processing: A Guide to Python and R Packages and Practical Use-Cases",
    "section": "Conclusion",
    "text": "Conclusion\nNatural Language Processing has revolutionized the way we interact with text-based data. With the availability of powerful Python and R packages, NLP tasks have become more accessible for people with limited programming skills and the tasks have become more efficient too. Packages like NLTK, spaCy, TextBlob, tm, tidytext, and NLP offer a rich set of tools and functionalities for various NLP tasks. From sentiment analysis and text classification to named entity recognition and machine translation, NLP has practical applications across industries, including customer feedback analysis, information extraction, and automated language processing systems.\nAs NLP continues to advance, we can expect even more sophisticated algorithms and techniques to emerge, further enhancing our ability to extract valuable insights from text data. Whether it’s analyzing customer sentiments, automating language processing tasks, or building intelligent chatbots, NLP opens up a world of possibilities for businesses and researchers alike. By harnessing the power of NLP and leveraging the diverse range of Python and R packages available, we can unlock the full potential of natural language understanding and pave the way for a more connected and intelligent future."
  },
  {
    "objectID": "posts/oop/index.html",
    "href": "posts/oop/index.html",
    "title": "Object Oriented Programming: Python vs R",
    "section": "",
    "text": "Object-Oriented Programming (OOP) is a programming paradigm that uses “objects” to design applications and computer programs. It utilizes several techniques from previously established paradigms, including modularity, polymorphism, and encapsulation. Today, we’ll explore how OOP concepts manifest in two popular programming languages: Python and R, particularly focusing on inheritance, using the context of a microwave oven as an example."
  },
  {
    "objectID": "posts/oop/index.html#introduction",
    "href": "posts/oop/index.html#introduction",
    "title": "Object Oriented Programming: Python vs R",
    "section": "",
    "text": "Object-Oriented Programming (OOP) is a programming paradigm that uses “objects” to design applications and computer programs. It utilizes several techniques from previously established paradigms, including modularity, polymorphism, and encapsulation. Today, we’ll explore how OOP concepts manifest in two popular programming languages: Python and R, particularly focusing on inheritance, using the context of a microwave oven as an example."
  },
  {
    "objectID": "posts/oop/index.html#oop-in-python",
    "href": "posts/oop/index.html#oop-in-python",
    "title": "Object Oriented Programming: Python vs R",
    "section": "OOP in Python",
    "text": "OOP in Python\nIn Python, OOP is central to the language. This can be seen in the way classes are defined and used. Python supports inheritance, allowing new classes to inherit attributes and methods from existing classes. This feature facilitates code reusability and the hierarchical organization of classes. For example, if you were modeling microwave ovens, you could start with a basic microwave class and then create a subclass for a fancier microwave with additional features:\n\nclass MicrowaveOven:\n    def __init__(self, power_rating):\n        self.power_rating = power_rating\n\n    def cook(self, time_seconds):\n        print(\"Your food is cooked!\")\n\nclass FancyMicrowaveOven(MicrowaveOven):\n    def cook_baked_potato(self):\n        self.cook(5)\n        print(\"Enjoy your baked potato!\")\n\na_fancy_microwave = FancyMicrowaveOven(5)\na_fancy_microwave.cook_baked_potato()\n\nYour food is cooked!\nEnjoy your baked potato!\n\n\nIn this Python example, FancyMicrowaveOven inherits from MicrowaveOven, meaning it can use the cook method defined in the parent class and add its methods like cook_baked_potato."
  },
  {
    "objectID": "posts/oop/index.html#oop-in-r",
    "href": "posts/oop/index.html#oop-in-r",
    "title": "Object Oriented Programming: Python vs R",
    "section": "OOP in R",
    "text": "OOP in R\nIn contrast, R, traditionally seen as a statistical programming language, has incorporated OOP features more gradually. The R6 package in R allows for a more classical approach to OOP, supporting encapsulation and inheritance but in a somewhat different manner than Python. The R6 framework allows R developers to create classes with reference semantics, which can be somewhat akin to how Python’s classes operate.\nHere’s how you might define a similar set of microwave classes in R using the R6 package:\n\nlibrary(R6)\nmicrowave_oven_factory &lt;- R6Class(\n    \"MicrowaveOven\",\n    private = list(power_rating_watts = 800),\n    public = list(\n        cook = function(time_seconds) {\n            Sys.sleep(time_seconds)\n            print(\"Your food is cooked!\")\n        }\n    )\n)\n\nfancy_microwave_oven_factory &lt;- R6Class(\n    \"FancyMicrowaveOven\",\n    inherit = microwave_oven_factory,\n    public = list(\n        cook_baked_potato = function() {\n            self$cook(5)\n            print(\"Enjoy your baked potato!\")\n        }\n    )\n)\n\na_fancy_microwave &lt;- fancy_microwave_oven_factory$new()\na_fancy_microwave$cook_baked_potato()\n\n[1] \"Your food is cooked!\"\n[1] \"Enjoy your baked potato!\"\n\n\nIn the R example, FancyMicrowaveOven is defined with inherit = microwave_oven_factory, which establishes an inheritance relationship with MicrowaveOven. This setup allows the fancy microwave to use the cook method from its parent class while adding a new method cook_baked_potato."
  },
  {
    "objectID": "posts/oop/index.html#differences",
    "href": "posts/oop/index.html#differences",
    "title": "Object Oriented Programming: Python vs R",
    "section": "Differences",
    "text": "Differences\nA key difference in the OOP implementation between Python and R is the syntax and the explicit use of self and super. In Python, self refers to the instance itself and is used to access class attributes and methods from within. super(), on the other hand, is used to call methods from a superclass in the context of inheritance.\nIn R’s R6, self serves a similar purpose as in Python, referring to the current object. However, R6 does not have a direct counterpart to Python’s super; instead, method overriding involves calling the superclass method directly through super$method_name(). Inheritance in R6 is established through the inherit parameter in the class definition, allowing the new class to access the public methods and properties of the parent class."
  },
  {
    "objectID": "posts/oop/index.html#conclusion",
    "href": "posts/oop/index.html#conclusion",
    "title": "Object Oriented Programming: Python vs R",
    "section": "Conclusion",
    "text": "Conclusion\nIn summary, while Python’s OOP features have been integral to the language from its conception, R has adopted OOP paradigms over time, with packages like R6 introducing class-based programming that includes inheritance. Both languages offer robust capabilities for OOP, facilitating complex and modular program design. In practice, choosing between Python and R for OOP depends on the specific requirements of your project and your personal or team’s familiarity with each language."
  },
  {
    "objectID": "posts/pandas/index.html",
    "href": "posts/pandas/index.html",
    "title": "Exploring Pandas: A Powerful Pythonic Analytic Tool",
    "section": "",
    "text": "Data analysis is a much sought after skill for data science. In my search for efficient tools for analyzing data, after outgrowing spreadsheets, I came across other tools like R’s tidyverse but discovering pandas was a game changer.\npandas is hardly used alone and mostly use alongside numerical computing tools like NumPy and Scipy and visualization tools like matplotlib or seaborn. pandas is closely associated with NumPy as it adopts certain aspects of NumPy’s array-based computing like array-based functions and vectorization (basically eliminating for loops in Python).\nComparing NumPy and pandas, I’ll say the biggest difference is that while pandas work with tabular data in rows and columns (which can be heterogeneous), NumPy is most suited for homogeneous numerical array data that is usually typed.\nPandas is a super popular library for fast and convenient data analysis built on top of the Python programming language, that provides two data structures for data manipulation: 1. Series 2. DataFrames\nA series is a one dimensional array of indexed data that consists of an array of actual data and an associated array of indices or labels.\nA DataFrame on the other hand is a two dimensional data structure. The data is usually tabular and arranged in rows and columns. A DataFrame can be created from lists, dictionaries or lists of dictionaries. It can also be created by loading data from storage like csv files, SQL database, excel files etc.\nThe index (of pandas Series or DataFrame) is used to access individual data values. A column of a dataframe can also be accessed as a Series. A pandas series is for all intents and purposes a 1-D dataframe.\nTo illustrate some of the concepts already mentioned, it is necessary to import pandas. In this article, it is assumed that the individual already has Python and pandas installed. Importing pandas is usually done usng the syntax below:\nimport pandas as pd\nSince pandas is hardly used alone, the following is a more realistic import:\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nIn addition, since the Series and DataFrame functions will most likely be frequently used, they can be imported as below:\n\nfrom pandas import Series, DataFrame\n\n\nCreating pandas series and DataFrames\nHere I will briefly provide the syntax for creating pandas series and DataFrame objects. To create a pandas series object, the following can suffice:\n\nobj = Series([4,7,-5,3])\nobj\n\n0    4\n1    7\n2   -5\n3    3\ndtype: int64\n\n\nTo create a DataFrame from a dictionary:\n\n#Define the dictionary 'x'\n\nx = {'Name': ['Lukman','Aminu', 'Amina', 'Fatima'], 'ID': [1, 2, 3, 4], 'Department': ['Architect Group', 'Software Group', 'Design Team', 'Infrastructure'], \n      'Salary':[100000, 80000, 50000, 60000]}\n\n#casting the dictionary to a DataFrame\ndf = pd.DataFrame(x)\n\n#display the result df\ndf\n\n\n\n\n\n\n\n\nName\nID\nDepartment\nSalary\n\n\n\n\n0\nLukman\n1\nArchitect Group\n100000\n\n\n1\nAminu\n2\nSoftware Group\n80000\n\n\n2\nAmina\n3\nDesign Team\n50000\n\n\n3\nFatima\n4\nInfrastructure\n60000\n\n\n\n\n\n\n\nThere’s a direct correspondence between the table and the dictionary. The keys correspond to the column labels and the values or lists corresponding to the rows. DataFrames can also be created from lists, lists of lists or loaded from storage:\ndf = pd.read_csv('example.csv')\nThe advantage of loading from storage is that there wont be a need to type in the data and it supports working on large datasets.\nI will concentrate more on manipulating DataFrames.\n\n\nColumn selection in pandas dataframes\nColumns can be selected using two different notations: 1. df[‘columnname.]: returns a series: or df[[’columnname’]]: returns a DataFrame 2. df.columnname The second notation only works if the columnname is a valid Python variable.\nMultiple columns can be accessed by passing the list of columns as follows:\n\ndf[['Name','ID']]\n\n\n\n\n\n\n\n\nName\nID\n\n\n\n\n0\nLukman\n1\n\n\n1\nAminu\n2\n\n\n2\nAmina\n3\n\n\n3\nFatima\n4\n\n\n\n\n\n\n\n\n\nUsing loc and iloc functions for selecting ranges in a DataFrame\nloc() is a label-based data selecting method which means that we have to pass the name of the row or column that we want to select. This method includes the last element of the range passed in it.\nThe syntax can be simplified below:\nloc[row_label, column_label]\niloc() is an indexed-based selecting method which means that we have to pass integer index in the method to select a specific row/column. This method does not include the last element of the range passed in it.\nThe syntax can be simplified below:\niloc[row_index, column_index]\n\n\nPerforming arithmetic on pandas DataFrames\npandas allows for easy arithmetic on DataFrames with different labels by providing the union of the dataframes. For example:\n\ndf1 = pd.DataFrame(np.arange(9.).reshape((3, 3)), columns=list(\"bcd\"),\n                      index=[\"Kaduna\", \"Kano\", \"Katsina\"])\n\ndf2 = pd.DataFrame(np.arange(12.).reshape((4, 3)), columns=list(\"bde\"),\n                      index=[\"Sokoto\", \"Kano\", \"Kaduna\", \"Kebbi\"])\nprint(df1,'\\n')\nprint(df2)\n\n           b    c    d\nKaduna   0.0  1.0  2.0\nKano     3.0  4.0  5.0\nKatsina  6.0  7.0  8.0 \n\n          b     d     e\nSokoto  0.0   1.0   2.0\nKano    3.0   4.0   5.0\nKaduna  6.0   7.0   8.0\nKebbi   9.0  10.0  11.0\n\n\nThe beauty of pandas is that it aligns on both the rows and columns and provide the sum for where the rows or columns appear in both DataFrames and returns NA for those that are not in both as shown below:\n\ndf1 + df2\n\n\n\n\n\n\n\n\nb\nc\nd\ne\n\n\n\n\nKaduna\n6.0\nNaN\n9.0\nNaN\n\n\nKano\n6.0\nNaN\n9.0\nNaN\n\n\nKatsina\nNaN\nNaN\nNaN\nNaN\n\n\nKebbi\nNaN\nNaN\nNaN\nNaN\n\n\nSokoto\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nIn addition, pandas has some flexible arithmetic methods for calculations in DataFrames and series. These include:\n\nadd, radd\nsub, rsub\ndiv, rdiv etc\n\nOperations between DataFrames and series are also supported, just like broadcasting in NumPy.\n\n\nComputing descriptive statistics\npandas objects have built-in methods for computing descriptive statistics on DataFrames and series. For instance: - Calling DataFrame’s sum method returns a Series containing column sums, passing axis=“columns” or axis=1 sums across the columns instead:\n\ndf[['Salary',\"ID\"]].sum(axis=1)\n\n0    100001\n1     80002\n2     50003\n3     60004\ndtype: int64\n\n\n\nSame can be done to get the column means or row means (axis = 1):\n\n\ndf[['Salary',\"ID\"]].mean()\n\nSalary    72500.0\nID            2.5\ndtype: float64\n\n\n\nDescribe method produces multiple summary statistics all at once for all the columns\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nID\nSalary\n\n\n\n\ncount\n4.000000\n4.000000\n\n\nmean\n2.500000\n72500.000000\n\n\nstd\n1.290994\n22173.557826\n\n\nmin\n1.000000\n50000.000000\n\n\n25%\n1.750000\n57500.000000\n\n\n50%\n2.500000\n70000.000000\n\n\n75%\n3.250000\n85000.000000\n\n\nmax\n4.000000\n100000.000000\n\n\n\n\n\n\n\nThere are various other methods for obtaining specific summary statistics in a DataFrame. The aim here is just to provide a taste of the possibilities, as with my NumPy article.\nIn conclusion, pandas is a powerful library with useful tools for data analysis and in this article, I attempted to provide a glimpse of what can be achieved with pandas. The pandas documentation is a useful resource alongside an individual’s inquisitiveness and willingness to explore different datasets using pandas. In addition, I found Python for Data Analysis by Wes McKinney quite useful as it gave an intuition into data analysis with Python."
  },
  {
    "objectID": "posts/paper-summary/index.html",
    "href": "posts/paper-summary/index.html",
    "title": "A Study on the Impact of Data Augmentation for Training Convolutional Neural Networks in the Presence of Noisy Labels - Paper Summary",
    "section": "",
    "text": "Emeson Santana, Gustavo Carneiro, and Filipe R. Cordeiro. A Study on the Impact of Data Augmentation for Training Convolutional Neural Networks in the Presence of Noisy Labels.SIBGRAPI - Conference on Graphics, Patterns and Images (2022)\nThe paper can be accessed here.\nThis paper focuses on the impact of data augmentation on the training of deep convolutional neural networks in the presence of label noise, which is common in real-world datasets. The authors analyze the robustness of different data augmentation methods and evaluate their effectiveness in improving model performance with noisy labels. They conduct experiments on various datasets - MNIST, CIFAR-10, CIFAR-100, and Clothing1M, using both classical and state-of-the-art data augmentation strategies.\nThe main contributions and findings of the paper are as follows:\n\nProblem Definition: The paper defines the label noise problem in the context of image classification, where noisy labels are common due to factors like human error or data quality issues. They consider symmetric, asymmetric, and semantic noise scenarios.\nData Augmentations: The authors evaluate 13 classical and 6 state-of-the-art data augmentation methods. The basic augmentations include random crop, horizontal flip, rotation, translation, and others, while the SOTA methods include Mixup, CutMix, AutoAug, RandAug, and more.\nExperimental Results: The experiments show that the appropriate selection of data augmentation significantly improves model robustness to label noise. The combination of classical and SOTA augmentations outperforms individual augmentations. For example, the use of random crop along with certain SOTA methods showed the best results in some scenarios.\nDataset Impact: The authors highlight that the choice of data augmentation is dataset-dependent, meaning that the best augmentation strategy may vary depending on the specific dataset.\n\n\n\nUpon careful reflection upon the content of the paper and its associated implications, one is prompted to engage in contemplation concerning the metric utilized for the quantification of impact. While accuracy remains a commonly adopted metric, its capacity to comprehensively encapsulate the genuine effects, particularly in the presence of extraneous variables, may be somewhat limited. As a suggestive alternative, it is worth considering metrics such as precision and recall, which offer a more encompassing representation. By doing so, one can gain valuable insights into both positive and negative instances, thereby affording a more nuanced and comprehensive assessment of impact."
  },
  {
    "objectID": "posts/paper-summary/index.html#a-study-on-the-impact-of-data-augmentation-for-training-convolutional-neural-networks-in-the-presence-of-noisy-labels---paper-summary",
    "href": "posts/paper-summary/index.html#a-study-on-the-impact-of-data-augmentation-for-training-convolutional-neural-networks-in-the-presence-of-noisy-labels---paper-summary",
    "title": "A Study on the Impact of Data Augmentation for Training Convolutional Neural Networks in the Presence of Noisy Labels - Paper Summary",
    "section": "",
    "text": "Emeson Santana, Gustavo Carneiro, and Filipe R. Cordeiro. A Study on the Impact of Data Augmentation for Training Convolutional Neural Networks in the Presence of Noisy Labels.SIBGRAPI - Conference on Graphics, Patterns and Images (2022)\nThe paper can be accessed here.\nThis paper focuses on the impact of data augmentation on the training of deep convolutional neural networks in the presence of label noise, which is common in real-world datasets. The authors analyze the robustness of different data augmentation methods and evaluate their effectiveness in improving model performance with noisy labels. They conduct experiments on various datasets - MNIST, CIFAR-10, CIFAR-100, and Clothing1M, using both classical and state-of-the-art data augmentation strategies.\nThe main contributions and findings of the paper are as follows:\n\nProblem Definition: The paper defines the label noise problem in the context of image classification, where noisy labels are common due to factors like human error or data quality issues. They consider symmetric, asymmetric, and semantic noise scenarios.\nData Augmentations: The authors evaluate 13 classical and 6 state-of-the-art data augmentation methods. The basic augmentations include random crop, horizontal flip, rotation, translation, and others, while the SOTA methods include Mixup, CutMix, AutoAug, RandAug, and more.\nExperimental Results: The experiments show that the appropriate selection of data augmentation significantly improves model robustness to label noise. The combination of classical and SOTA augmentations outperforms individual augmentations. For example, the use of random crop along with certain SOTA methods showed the best results in some scenarios.\nDataset Impact: The authors highlight that the choice of data augmentation is dataset-dependent, meaning that the best augmentation strategy may vary depending on the specific dataset.\n\n\n\nUpon careful reflection upon the content of the paper and its associated implications, one is prompted to engage in contemplation concerning the metric utilized for the quantification of impact. While accuracy remains a commonly adopted metric, its capacity to comprehensively encapsulate the genuine effects, particularly in the presence of extraneous variables, may be somewhat limited. As a suggestive alternative, it is worth considering metrics such as precision and recall, which offer a more encompassing representation. By doing so, one can gain valuable insights into both positive and negative instances, thereby affording a more nuanced and comprehensive assessment of impact."
  },
  {
    "objectID": "posts/paper-summary/index.html#conclusion",
    "href": "posts/paper-summary/index.html#conclusion",
    "title": "A Study on the Impact of Data Augmentation for Training Convolutional Neural Networks in the Presence of Noisy Labels - Paper Summary",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, the paper emphasizes the importance of data augmentation as a design choice for training deep convolutional neural networks with noisy labels. The experiments demonstrate that selecting appropriate data augmentation methods can lead to significant improvements in model performance when dealing with label noise. The authors suggest that further research could explore the benefits of using weak and strong augmentations at different stages of training and investigate new data augmentation strategies."
  },
  {
    "objectID": "posts/project/index.html",
    "href": "posts/project/index.html",
    "title": "Reproducible Research Final Project",
    "section": "",
    "text": "This article is the final project of the Reproducible Research course on Coursera offered by John Hopkins University. I make an attempt to look at the U.S. National Oceanic and Atmospheric Administration’s (NOAA) storm database and make deductions about the impact of these events on health and economy."
  },
  {
    "objectID": "posts/project/index.html#data-loading",
    "href": "posts/project/index.html#data-loading",
    "title": "Reproducible Research Final Project",
    "section": "Data Loading",
    "text": "Data Loading\nFirst step is load the data. I used the read_csv from readr (tidyverse) and it was able to read the file even in the compressed state.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nstorm_data &lt;- read_csv(\"repdata_data_StormData.csv.bz2\")\n\nRows: 902297 Columns: 37\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (18): BGN_DATE, BGN_TIME, TIME_ZONE, COUNTYNAME, STATE, EVTYPE, BGN_AZI,...\ndbl (18): STATE__, COUNTY, BGN_RANGE, COUNTY_END, END_RANGE, LENGTH, WIDTH, ...\nlgl  (1): COUNTYENDN\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "posts/project/index.html#data-cleaning-and-transformation",
    "href": "posts/project/index.html#data-cleaning-and-transformation",
    "title": "Reproducible Research Final Project",
    "section": "Data Cleaning and Transformation",
    "text": "Data Cleaning and Transformation\nTo understand how the data look and the variables contained, use was made of the glimpse function.\n\n# Explore the data\nglimpse(storm_data)\n\nRows: 902,297\nColumns: 37\n$ STATE__    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ BGN_DATE   &lt;chr&gt; \"4/18/1950 0:00:00\", \"4/18/1950 0:00:00\", \"2/20/1951 0:00:0…\n$ BGN_TIME   &lt;chr&gt; \"0130\", \"0145\", \"1600\", \"0900\", \"1500\", \"2000\", \"0100\", \"09…\n$ TIME_ZONE  &lt;chr&gt; \"CST\", \"CST\", \"CST\", \"CST\", \"CST\", \"CST\", \"CST\", \"CST\", \"CS…\n$ COUNTY     &lt;dbl&gt; 97, 3, 57, 89, 43, 77, 9, 123, 125, 57, 43, 9, 73, 49, 107,…\n$ COUNTYNAME &lt;chr&gt; \"MOBILE\", \"BALDWIN\", \"FAYETTE\", \"MADISON\", \"CULLMAN\", \"LAUD…\n$ STATE      &lt;chr&gt; \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\",…\n$ EVTYPE     &lt;chr&gt; \"TORNADO\", \"TORNADO\", \"TORNADO\", \"TORNADO\", \"TORNADO\", \"TOR…\n$ BGN_RANGE  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ BGN_AZI    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ BGN_LOCATI &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ END_DATE   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ END_TIME   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ COUNTY_END &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ COUNTYENDN &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ END_RANGE  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ END_AZI    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ END_LOCATI &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ LENGTH     &lt;dbl&gt; 14.0, 2.0, 0.1, 0.0, 0.0, 1.5, 1.5, 0.0, 3.3, 2.3, 1.3, 4.7…\n$ WIDTH      &lt;dbl&gt; 100, 150, 123, 100, 150, 177, 33, 33, 100, 100, 400, 400, 2…\n$ F          &lt;dbl&gt; 3, 2, 2, 2, 2, 2, 2, 1, 3, 3, 1, 1, 3, 3, 3, 4, 1, 1, 1, 1,…\n$ MAG        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ FATALITIES &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 4, 0, 0, 0, 0,…\n$ INJURIES   &lt;dbl&gt; 15, 0, 2, 2, 2, 6, 1, 0, 14, 0, 3, 3, 26, 12, 6, 50, 2, 0, …\n$ PROPDMG    &lt;dbl&gt; 25.0, 2.5, 25.0, 2.5, 2.5, 2.5, 2.5, 2.5, 25.0, 25.0, 2.5, …\n$ PROPDMGEXP &lt;chr&gt; \"K\", \"K\", \"K\", \"K\", \"K\", \"K\", \"K\", \"K\", \"K\", \"K\", \"M\", \"M\",…\n$ CROPDMG    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ CROPDMGEXP &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ WFO        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ STATEOFFIC &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ZONENAMES  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ LATITUDE   &lt;dbl&gt; 3040, 3042, 3340, 3458, 3412, 3450, 3405, 3255, 3334, 3336,…\n$ LONGITUDE  &lt;dbl&gt; 8812, 8755, 8742, 8626, 8642, 8748, 8631, 8558, 8740, 8738,…\n$ LATITUDE_E &lt;dbl&gt; 3051, 0, 0, 0, 0, 0, 0, 0, 3336, 3337, 3402, 3404, 0, 3432,…\n$ LONGITUDE_ &lt;dbl&gt; 8806, 0, 0, 0, 0, 0, 0, 0, 8738, 8737, 8644, 8640, 0, 8540,…\n$ REMARKS    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ REFNUM     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n\n\nNext, since the most important variable in this analysis is the EVTYPE variable, there’s a need to ensure that there are no missing values. I may plot or do summary statistics and missing values could cause problems in either of these activities.\nI also got the sum of fatalities, injuries and property damage as these are important in determining health and economic impact of the events.\n\n# Data Cleaning\ncleaned_storm_data &lt;- storm_data |&gt; \n  filter(!is.na(EVTYPE)) |&gt; \n  mutate(PROPDMG = PROPDMG * ifelse(PROPDMGEXP %in% c(\"K\", \"M\"), 1000, 1))\n\n\n# Summary Statistics\nsummary_stats &lt;- cleaned_storm_data |&gt; \n  summarise(\n    total_fatalities = sum(FATALITIES),\n    total_injuries = sum(INJURIES),\n    total_damage = sum(PROPDMG)\n  )\nsummary_stats\n\n# A tibble: 1 × 3\n  total_fatalities total_injuries total_damage\n             &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;\n1            15145         140528 10875995063."
  },
  {
    "objectID": "posts/project/index.html#results",
    "href": "posts/project/index.html#results",
    "title": "Reproducible Research Final Project",
    "section": "Results",
    "text": "Results\n\nPopulation Health Impact\nNext, to properly look at the population health impact, the dataset is grouped by event type and sum of the fatalities and injuries are compared in a variable called event_harm. This variable is then plotted, taking a look at the top 10.\n\nevent_harm &lt;- cleaned_storm_data |&gt; \n  group_by(EVTYPE) |&gt; \n  summarise(total_harm = sum(FATALITIES + INJURIES)) |&gt; arrange(desc(total_harm))\n\n# Visualization\nevent_harm |&gt; \n  top_n(10, total_harm) |&gt; \n  ggplot(aes(x = reorder(EVTYPE, -total_harm), y = total_harm)) +\n  geom_bar(stat = \"identity\", fill = \"blue\") +\n  labs(\n    title = \"Top 10 Events with Highest Population Health Impact\",\n    x = \"Event Type\",\n    y = \"Total Harm\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\nEconomic Consequences\nThe final aspect is to analyse and check the economic consequences. Naturally, property damage is one of the metrics for measuring economic consequences of events like these. The data is grouped by event type and the property damage associated with each event is calculated. This is stored in a variable called event_damage that is then visualized.\n\nevent_damage &lt;- cleaned_storm_data |&gt; \n  group_by(EVTYPE) |&gt; \n  summarise(total_damage = sum(PROPDMG)) |&gt; \n  arrange(desc(total_damage))\n\n# Visualization\nevent_damage |&gt; \n  top_n(10, total_damage) |&gt; \n  ggplot(aes(x = reorder(EVTYPE, -total_damage), y = total_damage)) +\n  geom_bar(stat = \"identity\", fill = \"red\") +\n  labs(\n    title = \"Top 10 Events with Greatest Economic Consequences\",\n    x = \"Event Type\",\n    y = \"Total Damage\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "posts/project/index.html#conclusion",
    "href": "posts/project/index.html#conclusion",
    "title": "Reproducible Research Final Project",
    "section": "Conclusion",
    "text": "Conclusion\nLooking at population health impact, tornadoes are the most harmful. Also, on economic impact, tornadoes are still the most damaging event followed in order by flash floods, TSTM wind, flood, thunderstorm wind, hail,lightning, thunderstorm winds, high wind and winterstorm as the top ten events that cause the most economic damage."
  },
  {
    "objectID": "posts/pytorch-intro/index.html",
    "href": "posts/pytorch-intro/index.html",
    "title": "A Beginner’s Comprehensive Guide to PyTorch",
    "section": "",
    "text": "Welcome to the world of PyTorch, a dynamic and powerful machine learning framework that’s revolutionizing the way we approach deep learning. At the core of PyTorch are tensors, versatile structures that extend beyond simple matrices, allowing for more complex and efficient data representations. For instance, a color image with dimensions 64x64 pixels, represented in three color channels (red, green, and blue), is effectively a tensor in PyTorch."
  },
  {
    "objectID": "posts/pytorch-intro/index.html#introduction",
    "href": "posts/pytorch-intro/index.html#introduction",
    "title": "A Beginner’s Comprehensive Guide to PyTorch",
    "section": "",
    "text": "Welcome to the world of PyTorch, a dynamic and powerful machine learning framework that’s revolutionizing the way we approach deep learning. At the core of PyTorch are tensors, versatile structures that extend beyond simple matrices, allowing for more complex and efficient data representations. For instance, a color image with dimensions 64x64 pixels, represented in three color channels (red, green, and blue), is effectively a tensor in PyTorch."
  },
  {
    "objectID": "posts/pytorch-intro/index.html#getting-started-with-pytorch",
    "href": "posts/pytorch-intro/index.html#getting-started-with-pytorch",
    "title": "A Beginner’s Comprehensive Guide to PyTorch",
    "section": "Getting Started with PyTorch",
    "text": "Getting Started with PyTorch\nTo dive into PyTorch, start by importing it in your Python code with import torch, along with your other dependencies. This guide will serve as an introduction to PyTorch’s capabilities. Don’t worry about memorizing everything.\n\n# importing PyTorch\nimport torch\n# how to import the nn module\nimport torch.nn as nn"
  },
  {
    "objectID": "posts/pytorch-intro/index.html#why-choose-pytorch",
    "href": "posts/pytorch-intro/index.html#why-choose-pytorch",
    "title": "A Beginner’s Comprehensive Guide to PyTorch",
    "section": "Why Choose PyTorch?",
    "text": "Why Choose PyTorch?\nA detailed analysis by the Gradient explains it well: PyTorch offers a more Pythonic experience, is easier to debug, and remains the leading choice in machine learning research. Despite TensorFlow’s efforts to integrate similar features, PyTorch’s intuitive design and growing dominance in both research and industry make it an optimal choice for education and forward-thinking development."
  },
  {
    "objectID": "posts/pytorch-intro/index.html#exploring-tensor-properties",
    "href": "posts/pytorch-intro/index.html#exploring-tensor-properties",
    "title": "A Beginner’s Comprehensive Guide to PyTorch",
    "section": "Exploring Tensor Properties",
    "text": "Exploring Tensor Properties\nCreating tensors in PyTorch is straightforward. For instance, you can initiate a tensor using torch.Tensor. A simple example is:\n\nexample_tensor = torch.Tensor([\n                                [[1, 2], [3, 4]],\n                                [[5, 6], [7, 8]],\n                                [[9, 0], [1, 2]]\n                             ])\nexample_tensor\n\ntensor([[[1., 2.],\n         [3., 4.]],\n\n        [[5., 6.],\n         [7., 8.]],\n\n        [[9., 0.],\n         [1., 2.]]])\n\n\nUnderstanding a tensor’s properties, such as its device (CPU or GPU) and shape, is crucial. You can explore these properties using methods like example_tensor.device and example_tensor.shape. These properties give insights into where the tensor resides (CPU or GPU) and its dimensional structure. For example, torch.Size([3, 2, 2]) indicates a tensor of rank 3 with specific dimensions.\n\nprint(f\"tensor_shape: {example_tensor.shape}\")\nprint(f\"tensor device: {example_tensor.device}\")\n\ntensor_shape: torch.Size([3, 2, 2])\ntensor device: cpu"
  },
  {
    "objectID": "posts/pytorch-intro/index.html#indexing-and-initializing-tensors",
    "href": "posts/pytorch-intro/index.html#indexing-and-initializing-tensors",
    "title": "A Beginner’s Comprehensive Guide to PyTorch",
    "section": "Indexing and Initializing Tensors",
    "text": "Indexing and Initializing Tensors\nManipulating tensors in PyTorch is akin to handling NumPy arrays. You can access elements or slices of tensors using standard Python indexing. Moreover, initializing tensors is versatile in PyTorch. Functions like torch.ones_like and torch.zeros_like help create tensors filled with ones or zeros, mimicking the shape and device of a reference tensor."
  },
  {
    "objectID": "posts/pytorch-intro/index.html#pytorchs-neural-network-module-torch.nn",
    "href": "posts/pytorch-intro/index.html#pytorchs-neural-network-module-torch.nn",
    "title": "A Beginner’s Comprehensive Guide to PyTorch",
    "section": "PyTorch’s Neural Network Module (torch.nn)",
    "text": "PyTorch’s Neural Network Module (torch.nn)\nPyTorch’s torch.nn module is a treasure trove for neural network enthusiasts. It offers a plethora of classes to build and transform tensors efficiently. For example, nn.Linear for linear transformations, nn.ReLU for applying the ReLU activation function, and nn.BatchNorm1d for batch normalization in one-dimensional data."
  },
  {
    "objectID": "posts/pytorch-intro/index.html#optimization-techniques",
    "href": "posts/pytorch-intro/index.html#optimization-techniques",
    "title": "A Beginner’s Comprehensive Guide to PyTorch",
    "section": "Optimization Techniques",
    "text": "Optimization Techniques\nOne of PyTorch’s strengths is its optimization capabilities, crucial in machine learning. The torch.optim module provides various optimizers like Adam, essential for updating model parameters during training. A typical training loop in PyTorch involves setting gradients to zero, computing loss, backpropagating to calculate gradients, and then updating the parameters."
  },
  {
    "objectID": "posts/pytorch-intro/index.html#extending-with-custom-nn-modules",
    "href": "posts/pytorch-intro/index.html#extending-with-custom-nn-modules",
    "title": "A Beginner’s Comprehensive Guide to PyTorch",
    "section": "Extending with Custom nn Modules",
    "text": "Extending with Custom nn Modules\nPyTorch allows for the creation of custom classes extending the nn module. This feature lets you define unique model architectures suited to your specific problems. You can define the structure in the init method and specify the computation in the forward method."
  },
  {
    "objectID": "posts/pytorch-intro/index.html#conclusion",
    "href": "posts/pytorch-intro/index.html#conclusion",
    "title": "A Beginner’s Comprehensive Guide to PyTorch",
    "section": "Conclusion",
    "text": "Conclusion\nPyTorch is not just a tool but a playground for machine learning enthusiasts and researchers. Its intuitive design, Pythonic nature, and robust features make it a top choice for learning and developing cutting-edge machine learning models. As you embark on this journey, PyTorch will undoubtedly be a valuable ally in turning your machine learning aspirations into reality. Subsequent articles will put all these concepts into practice as we delve deeper into the world of PyTorch."
  },
  {
    "objectID": "posts/pytorch-regression/index.html",
    "href": "posts/pytorch-regression/index.html",
    "title": "Revving Up: MPG Regression Unleashed through Deep Learning in PyTorch",
    "section": "",
    "text": "Introduction\nIn this article, we’ll dive into the world of deep learning with PyTorch by constructing a multiple linear regression model to predict a vehicle’s miles per gallon (MPG) based on various features. We’ll explore the preprocessing steps, model architecture, training process, and evaluation of the model’s performance.\n\n\nPreparing the Data and Data Preprocessing\nOur journey begins by loading the auto MPG dataset, which contains information about vehicle characteristics and their corresponding MPG values. We’ll focus on features like the number of cylinders, displacement, horsepower, weight, acceleration, manufacturing origin, and model year.\nTo ensure our data is suitable for training, we perform necessary preprocessing steps. We drop rows with missing values, standardize continuous features, and transform categorical features into one-hot encoded vectors.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport sklearn\nimport sklearn.model_selection\nimport torch\nimport torch.nn as nn\nfrom torch.nn.functional import one_hot\nfrom torch.utils.data import DataLoader,Dataset,TensorDataset\n# Load the dataset\nurl = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\ncolumn_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight', 'Acceleration', 'Model Year', 'Origin']\n\ndf = pd.read_csv(url, names=column_names,\n                 na_values = \"?\", comment='\\t',\n                 sep=\" \", skipinitialspace=True)\n\n\n# Dropping rows with missing values\n\ndf = df.dropna().reset_index(drop=True)\n\n# Splitting the data into train and test sets\ndf_train, df_test = sklearn.model_selection.train_test_split(df, train_size=0.8, random_state=1)\n\n# Standardizing continuous features\nnumeric_column_names = ['Cylinders', 'Displacement', 'Horsepower', 'Weight', 'Acceleration']\ntrain_stats = df_train.describe().transpose()\n\ndf_train_norm, df_test_norm = df_train.copy(), df_test.copy()\nfor col_name in numeric_column_names:\n    mean = train_stats.loc[col_name, 'mean']\n    std  = train_stats.loc[col_name, 'std']\n    df_train_norm[col_name] = (df_train_norm[col_name] - mean) / std\n    df_test_norm[col_name] = (df_test_norm[col_name] - mean) / std\n\n# Bucketing the model year categories\nboundaries = torch.tensor([73, 76, 79])\n \nv = torch.tensor(df_train_norm['Model Year'].values)\ndf_train_norm['Model Year Bucketed'] = torch.bucketize(v, boundaries, right=True)\n\nv = torch.tensor(df_test_norm['Model Year'].values)\ndf_test_norm['Model Year Bucketed'] = torch.bucketize(v, boundaries, right=True)\n\nnumeric_column_names.append('Model Year Bucketed')\n\n# One-hot encoding the origin feature\n\ntotal_origin = len(set(df_train_norm['Origin']))\n\norigin_encoded = one_hot(torch.from_numpy(df_train_norm['Origin'].values) % total_origin)\n\n# Creating the train and test feature and label tensors\n\nx_train_numeric = torch.tensor(df_train_norm[numeric_column_names].values)\nx_train = torch.cat([x_train_numeric, origin_encoded], 1).float()\n \norigin_encoded = one_hot(torch.from_numpy(df_test_norm['Origin'].values) % total_origin)\nx_test_numeric = torch.tensor(df_test_norm[numeric_column_names].values)\nx_test = torch.cat([x_test_numeric, origin_encoded], 1).float()\n\n\ny_train = torch.tensor(df_train_norm['MPG'].values).float()\ny_test = torch.tensor(df_test_norm['MPG'].values).float()\n\n# Creating a data loader to load the train dataset in batches\ntrain_ds = TensorDataset(x_train, y_train)\nbatch_size = 8\ntorch.manual_seed(1)\ntrain_dl = DataLoader(train_ds, batch_size, shuffle=True)\n\n\n\nBuilding the DNN Regression Model\nWith our data prepared, we move on to constructing our Deep Neural Network (DNN) regression model using PyTorch. This model will predict MPG values based on the vehicle’s features.\n\n# Define the model architecture\nhidden_units = [8, 4]\ninput_size = x_train.shape[1]\n\nall_layers = []\nfor hidden_unit in hidden_units:\n    layer = nn.Linear(input_size, hidden_unit)\n    all_layers.append(layer)\n    all_layers.append(nn.ReLU())\n    input_size = hidden_unit\n\nall_layers.append(nn.Linear(hidden_units[-1], 1))\nmodel = nn.Sequential(*all_layers)\n\n# Define the loss function and optimizer\nloss_fn = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n\n\n\nTraining the Model\nIt’s time to train our DNN regression model on the training data. We iterate through the data for a specified number of epochs, adjusting the model’s weights to minimize the mean squared error loss.\n\nnum_epochs = 200\nlog_epochs = 20 \n\nfor epoch in range(num_epochs):\n    loss_hist_train = 0\n    for x_batch, y_batch in train_dl:\n        pred = model(x_batch)[:, 0]\n        loss = loss_fn(pred, y_batch)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        loss_hist_train += loss.item()\n    if epoch % log_epochs == 0:\n        print(f'Epoch {epoch}  Loss {loss_hist_train/len(train_dl):.4f}')\n\nEpoch 0  Loss 530.7308\nEpoch 20  Loss 7.8103\nEpoch 40  Loss 7.7546\nEpoch 60  Loss 6.9081\nEpoch 80  Loss 6.8482\nEpoch 100  Loss 6.7144\nEpoch 120  Loss 6.4509\nEpoch 140  Loss 7.1134\nEpoch 160  Loss 6.4428\nEpoch 180  Loss 6.2078\n\n\n\n\nEvaluating the Model\nOnce the model is trained, we assess its performance on the test dataset. This helps us understand how well the model generalizes to new, unseen data.\n\nwith torch.no_grad():\n    pred = model(x_test.float())[:, 0]\n    loss = loss_fn(pred, y_test)\n    mae = nn.L1Loss()(pred, y_test)\n    print(f'Test MSE: {loss.item():.4f}')\n    print(f'Test MAE: {mae.item():.4f}')\n\nTest MSE: 13.1923\nTest MAE: 2.6507\n\n\nSeeing the good metrics, let’s confirm by plotting the actual and predicted values\n\n# Plotting actual vs. predicted MPG values\nplt.figure(figsize=(10, 6))\nplt.scatter(y_test, pred, color='blue', label='Predicted')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linewidth=2, linestyle='--', label='Perfect Prediction')\nplt.xlabel('Actual MPG')\nplt.ylabel('Predicted MPG')\nplt.title('Actual vs. Predicted MPG Values')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nConclusion\nOur DNN regression model demonstrates promising results in predicting MPG values based on vehicle features. By carefully preprocessing the data, constructing an appropriate model architecture, and iteratively training the model, we achieve a model that generalizes reasonably well to new data. This article serves as a starting point for your journey into deep learning with PyTorch, enabling you to build more advanced models and tackle a variety of data analysis challenges."
  },
  {
    "objectID": "posts/responsible-ai/index.html",
    "href": "posts/responsible-ai/index.html",
    "title": "Understanding Responsible AI: A Deep Dive into Ethical AI Development",
    "section": "",
    "text": "Artificial intelligence (AI) is rapidly advancing, transforming industries from healthcare to transportation. With its growing influence, there is an increasing need to ensure that AI is developed and deployed responsibly. This is where Responsible AI (RAI) comes in—a framework designed to guide the ethical use of AI, ensuring fairness, transparency, accountability, and respect for human rights. In this article, I explore the key concepts of Responsible AI, its importance, and how organizations can implement strategies that align AI development with ethical principles.\nThis article is primarily a summary of the content I learnt in the course Responsible AI Practices on DataCamp. I enrolled in this course after I had the privilege of reviewing the notebooks for the Responsible AI practicals for the upcoming Deep Learning Indaba 2024 in Dakar, Senegal. This combination of learning and practical experience has given me a well-rounded perspective on the principles of Responsible AI, which I am excited to share. I must also thank Arewa Data Science Academy for giving me the opportunity to enjoy a Data Camp scholarship, which allowed me to learn several concepts and earn relevant certifications for free."
  },
  {
    "objectID": "posts/responsible-ai/index.html#defining-responsible-ai",
    "href": "posts/responsible-ai/index.html#defining-responsible-ai",
    "title": "Understanding Responsible AI: A Deep Dive into Ethical AI Development",
    "section": "Defining Responsible AI",
    "text": "Defining Responsible AI\nResponsible AI refers to the practice of designing, developing, and deploying AI systems with an understanding of their ethical implications and societal impacts. It is about ensuring that AI systems are fair, transparent, accountable, and respectful of privacy and human rights. The goal is to create AI that benefits society without causing harm, focusing not only on the technology itself but also on the values guiding its use."
  },
  {
    "objectID": "posts/responsible-ai/index.html#why-responsible-ai-is-crucial",
    "href": "posts/responsible-ai/index.html#why-responsible-ai-is-crucial",
    "title": "Understanding Responsible AI: A Deep Dive into Ethical AI Development",
    "section": "Why Responsible AI is Crucial",
    "text": "Why Responsible AI is Crucial\nAI’s ability to process vast amounts of data and learn from it is revolutionizing fields like healthcare, education, and finance. However, these advancements come with risks. AI can perpetuate biases, invade privacy, or be used unethically if not developed responsibly. For example, facial recognition systems have been found to have higher error rates for people of color, leading to discriminatory outcomes. Similarly, automated hiring systems can reflect and amplify existing biases in the data they are trained on.\nThese risks highlight the need for Responsible AI practices that mitigate negative impacts and promote fairness and inclusivity. The stakes are high, making Responsible AI a critical aspect of any AI development strategy."
  },
  {
    "objectID": "posts/responsible-ai/index.html#key-principles-of-responsible-ai",
    "href": "posts/responsible-ai/index.html#key-principles-of-responsible-ai",
    "title": "Understanding Responsible AI: A Deep Dive into Ethical AI Development",
    "section": "Key Principles of Responsible AI",
    "text": "Key Principles of Responsible AI\nThe principles of Responsible AI are based on globally recognized guidelines, such as those from the Organisation for Economic Co-operation and Development (OECD). Key principles include:\n\nTransparency and Explainability: AI systems should be designed so that their decisions can be understood and explained.\nFairness and Non-Discrimination: AI should be free from biases and discrimination, promoting inclusivity and equity.\nRobustness and Safety: AI systems must be reliable, secure, and capable of operating safely in various conditions.\nPrivacy and Data Governance: AI should respect privacy and ensure secure data handling.\nAccountability: Organizations should be accountable for the outcomes of their AI systems.\nInclusive Growth and Sustainability: AI should bridge digital divides and contribute to sustainable development."
  },
  {
    "objectID": "posts/responsible-ai/index.html#responsible-ai-vs.-ai-ethics",
    "href": "posts/responsible-ai/index.html#responsible-ai-vs.-ai-ethics",
    "title": "Understanding Responsible AI: A Deep Dive into Ethical AI Development",
    "section": "Responsible AI vs. AI Ethics",
    "text": "Responsible AI vs. AI Ethics\nWhile Responsible AI and AI Ethics are closely related, they are not identical. AI Ethics focuses broadly on the philosophical and moral questions surrounding AI, while Responsible AI is more practical, emphasizing the implementation of ethical considerations in measurable ways. It involves using clear metrics and frameworks to ensure that AI systems align with ethical principles."
  },
  {
    "objectID": "posts/responsible-ai/index.html#the-global-regulatory-landscape",
    "href": "posts/responsible-ai/index.html#the-global-regulatory-landscape",
    "title": "Understanding Responsible AI: A Deep Dive into Ethical AI Development",
    "section": "The Global Regulatory Landscape",
    "text": "The Global Regulatory Landscape\nRegulation of AI is still evolving, but several regions are leading the way. The European Union’s AI Act classifies AI systems based on risk levels and imposes stringent requirements on high-risk applications. In the United States, initiatives like the AI Bill of Rights and various state-level regulations are shaping AI governance. Countries like Canada, the UK, and China are also developing frameworks tailored to their societal and economic needs.\nThese regulatory efforts share a common goal: to protect human rights while fostering innovation in AI. As AI technology continues to advance, these regulations will play a critical role in ensuring that AI systems are used responsibly."
  },
  {
    "objectID": "posts/responsible-ai/index.html#implementing-responsible-ai-an-8-step-approach",
    "href": "posts/responsible-ai/index.html#implementing-responsible-ai-an-8-step-approach",
    "title": "Understanding Responsible AI: A Deep Dive into Ethical AI Development",
    "section": "Implementing Responsible AI: An 8-Step Approach",
    "text": "Implementing Responsible AI: An 8-Step Approach\nFor organizations looking to implement Responsible AI, the following eight-step approach is recommended:\n\nEmbrace AI Governance: Secure organizational commitment to align AI development with ethical standards and business goals.\nBuild an AI Playbook: Develop a strategic plan that documents AI governance practices, keeping it flexible and up to date.\nIdentify Key Stakeholders: Engage both internal and external stakeholders to ensure inclusivity and address diverse perspectives.\nLeverage Internal Support Mechanisms: Use ethics boards, training programs, and AI squads to guide ethical AI development.\nEmbrace a Multi-Stakeholder Approach: Include community members, industry experts, and policymakers in AI governance.\nExplore Additional Responsible Behavior Indicators: Demonstrate a commitment to Responsible AI through certifications, CSR standards, and transparent ESG reporting.\nImplement Governance Tools: Develop guidelines and procedures that uphold Responsible AI principles.\nMonitor, Audit, and Evaluate AI Systems: Continuously monitor and audit AI systems to ensure they remain aligned with ethical standards."
  },
  {
    "objectID": "posts/responsible-ai/index.html#the-importance-of-diversity-in-ai-development",
    "href": "posts/responsible-ai/index.html#the-importance-of-diversity-in-ai-development",
    "title": "Understanding Responsible AI: A Deep Dive into Ethical AI Development",
    "section": "The Importance of Diversity in AI Development",
    "text": "The Importance of Diversity in AI Development\nDiversity, equity, and inclusion (DE&I) are essential for creating fair and unbiased AI systems. A lack of diversity in AI development teams can lead to systems that reflect and reinforce societal biases. For instance, facial recognition systems and hiring algorithms have been shown to produce biased results when developed without diverse perspectives.\nResearch shows that diverse teams are more innovative and perform better. Prioritizing DE&I in AI development is not just ethically sound but also beneficial for business, leading to more robust and inclusive AI solutions."
  },
  {
    "objectID": "posts/responsible-ai/index.html#conclusion-the-path-forward-for-responsible-ai",
    "href": "posts/responsible-ai/index.html#conclusion-the-path-forward-for-responsible-ai",
    "title": "Understanding Responsible AI: A Deep Dive into Ethical AI Development",
    "section": "Conclusion: The Path Forward for Responsible AI",
    "text": "Conclusion: The Path Forward for Responsible AI\nResponsible AI is a continuous journey that requires ongoing evaluation, monitoring, and adaptation. My recent experience reviewing content for the upcoming Deep Learning Indaba 2024 reinforced the importance of this topic. The principles I learned in the DataCamp course on Responsible AI practices, coupled with my practical experience, have deepened my understanding of the need for ethical AI governance.\nAs AI continues to evolve, it is our collective responsibility to guide its development in ways that reflect our highest ethical standards. By embracing Responsible AI principles, we can ensure that AI serves humanity in a way that is fair, transparent, and accountable, ultimately contributing to a more equitable and sustainable future."
  },
  {
    "objectID": "posts/sciences-writing/index.html",
    "href": "posts/sciences-writing/index.html",
    "title": "Mastering Scientific Writing: Key Takeaways from Coursera’s Writing in the Sciences MOOC by Kristin-Sainani",
    "section": "",
    "text": "Since written communication is the primary way that scientific discoveries are disseminated to the public, scientific writing is a crucial skill for scientists and researchers. For effectively communicating ideas, presenting the findings of research investigations, and expanding scientific knowledge, the ability to write clear and concise scientific articles is essential.\nIn addition to presenting facts and findings, scientific writing often entails interpreting findings and coming up with brand-new theories. Furthermore, objectiveness, precision, and clarity are qualities of good scientific writing that make it possible for subsequent researchers to replicate and improve on the reported work.\nScientists and researchers can improve their writing abilities, learn how to create compelling abstracts, opening statements, and conclusions, and obtain a better understanding of the peer-review procedure by taking a course on scientific writing. Scientists and researchers can increase the exposure and influence of their work by writing more effectively and communicating their findings to peers, collaborators, and the larger scientific community.\nSuccessful scientific communication is fundamentally dependent on effective scientific writing; however, many researchers and scientists find it difficult to successfully convey their study findings. The Coursera MOOC “Writing in the Sciences” addresses this difficulty by giving students the tools and strategies they need to develop their scientific writing abilities.\nAnyone wishing to improve their ability to write scientific publications must take this course, which is instructed by renowned Stanford professor Kristin Sainani.\nI recently finished the course, which was a requirement for our current Arewa Data Science Fellowship. I describe my key takeaways from the course and underline the crucial skills and knowledge discussed in this post."
  },
  {
    "objectID": "posts/sciences-writing/index.html#introduction",
    "href": "posts/sciences-writing/index.html#introduction",
    "title": "Mastering Scientific Writing: Key Takeaways from Coursera’s Writing in the Sciences MOOC by Kristin-Sainani",
    "section": "",
    "text": "Since written communication is the primary way that scientific discoveries are disseminated to the public, scientific writing is a crucial skill for scientists and researchers. For effectively communicating ideas, presenting the findings of research investigations, and expanding scientific knowledge, the ability to write clear and concise scientific articles is essential.\nIn addition to presenting facts and findings, scientific writing often entails interpreting findings and coming up with brand-new theories. Furthermore, objectiveness, precision, and clarity are qualities of good scientific writing that make it possible for subsequent researchers to replicate and improve on the reported work.\nScientists and researchers can improve their writing abilities, learn how to create compelling abstracts, opening statements, and conclusions, and obtain a better understanding of the peer-review procedure by taking a course on scientific writing. Scientists and researchers can increase the exposure and influence of their work by writing more effectively and communicating their findings to peers, collaborators, and the larger scientific community.\nSuccessful scientific communication is fundamentally dependent on effective scientific writing; however, many researchers and scientists find it difficult to successfully convey their study findings. The Coursera MOOC “Writing in the Sciences” addresses this difficulty by giving students the tools and strategies they need to develop their scientific writing abilities.\nAnyone wishing to improve their ability to write scientific publications must take this course, which is instructed by renowned Stanford professor Kristin Sainani.\nI recently finished the course, which was a requirement for our current Arewa Data Science Fellowship. I describe my key takeaways from the course and underline the crucial skills and knowledge discussed in this post."
  },
  {
    "objectID": "posts/sciences-writing/index.html#the-value-of-clarity-in-writing",
    "href": "posts/sciences-writing/index.html#the-value-of-clarity-in-writing",
    "title": "Mastering Scientific Writing: Key Takeaways from Coursera’s Writing in the Sciences MOOC by Kristin-Sainani",
    "section": "The Value of Clarity in Writing",
    "text": "The Value of Clarity in Writing\nThe value of concise writing in scientific communication is one of the major concepts stressed throughout the course. Scientific writers must avoid frequent errors, such as the use of technical jargon or needlessly complex language, in order to successfully communicate study findings. Instead, authors should strive for simplicity and clarity while utilizing language that is clear to readers."
  },
  {
    "objectID": "posts/sciences-writing/index.html#scientific-paper-structure",
    "href": "posts/sciences-writing/index.html#scientific-paper-structure",
    "title": "Mastering Scientific Writing: Key Takeaways from Coursera’s Writing in the Sciences MOOC by Kristin-Sainani",
    "section": "Scientific Paper Structure",
    "text": "Scientific Paper Structure\nEffective scientific paper structure is a crucial skill that is addressed in the course. Several important elements of scientific publications often appear in the introduction, methods, results, and discussion sections. Each component needs to be well-written and concise, with a seamless flow between them."
  },
  {
    "objectID": "posts/sciences-writing/index.html#effective-evidence-use",
    "href": "posts/sciences-writing/index.html#effective-evidence-use",
    "title": "Mastering Scientific Writing: Key Takeaways from Coursera’s Writing in the Sciences MOOC by Kristin-Sainani",
    "section": "Effective Evidence Use",
    "text": "Effective Evidence Use\nThe use of evidence to back up claims and conclusions is essential to good scientific writing. I gained knowledge on how to successfully use data, statistics, and literature citations in their scientific writing. I also know how to assess the reliability of the evidence and how to use it to bolster my claims."
  },
  {
    "objectID": "posts/sciences-writing/index.html#peer-editing-and-review",
    "href": "posts/sciences-writing/index.html#peer-editing-and-review",
    "title": "Mastering Scientific Writing: Key Takeaways from Coursera’s Writing in the Sciences MOOC by Kristin-Sainani",
    "section": "Peer editing and review",
    "text": "Peer editing and review\nAdditionally, the course highlighted the value of peer review as well as effective methods for reviewing and editing scientific writing. I gained knowledge on how to give constructive criticism and how to use criticism to enhance my work. The accuracy, clarity, and effectiveness of scientific writing depend on this ability."
  },
  {
    "objectID": "posts/sciences-writing/index.html#research-ethics-and-responsible-conduct",
    "href": "posts/sciences-writing/index.html#research-ethics-and-responsible-conduct",
    "title": "Mastering Scientific Writing: Key Takeaways from Coursera’s Writing in the Sciences MOOC by Kristin-Sainani",
    "section": "Research Ethics and Responsible Conduct",
    "text": "Research Ethics and Responsible Conduct\nThe course placed a strong emphasis on writing abilities as well as the value of ethical behavior in scientific writing and research. I gained knowledge on how to avoid conflicts of interest, data fabrication, and plagiarism."
  },
  {
    "objectID": "posts/sciences-writing/index.html#animated-and-educational-lectures",
    "href": "posts/sciences-writing/index.html#animated-and-educational-lectures",
    "title": "Mastering Scientific Writing: Key Takeaways from Coursera’s Writing in the Sciences MOOC by Kristin-Sainani",
    "section": "Animated and Educational Lectures",
    "text": "Animated and Educational Lectures\nThe interesting and educational lectures given by Professor Sainani are one of the highlights of the Coursera course “Writing in the Sciences.” She uses a range of examples and real-world settings to illustrate important topics in her lectures, which are given in a straightforward and succinct manner. For instance, when demonstrating how to create personal statements, she provided examples of previous personal statements she had written, illustrating how including tales can help the reader relate to the content more effectively. Her personable and encouraging teaching style makes the training approachable and fun."
  },
  {
    "objectID": "posts/sciences-writing/index.html#additional-fundamental-skills-taught-in-the-course",
    "href": "posts/sciences-writing/index.html#additional-fundamental-skills-taught-in-the-course",
    "title": "Mastering Scientific Writing: Key Takeaways from Coursera’s Writing in the Sciences MOOC by Kristin-Sainani",
    "section": "Additional Fundamental Skills Taught in the Course",
    "text": "Additional Fundamental Skills Taught in the Course\nThe “Writing in the Sciences” course on Coursera includes several other crucial abilities for scientists and researchers in addition to the ones mentioned above. These consist of:\nWriting grants: How to create a compelling research plan, find funding possibilities, and write excellent grant proposals that address reviewer comments.\nRecommendation Letters: The course instructs students on how to ask for and compose effective recommendation letters, including how to pick the best referees, supply the required details, and highlight the applicant’s qualities.\nReview Article: How to produce review articles that summarize and evaluate current research on a certain subject. Additionally, the course taught how to recognize important research and points of contention, assess the reliability of the data, and write an organized and succinct review article.\nWriting Effective Personal Statements for Graduate School Applications: The course discussed how to create great personal statements for graduate school applications, including how to highlight pertinent experience and skills, describe your research interests, and show that you are a good fit for the program."
  },
  {
    "objectID": "posts/sciences-writing/index.html#conclusion",
    "href": "posts/sciences-writing/index.html#conclusion",
    "title": "Mastering Scientific Writing: Key Takeaways from Coursera’s Writing in the Sciences MOOC by Kristin-Sainani",
    "section": "Conclusion",
    "text": "Conclusion\nFor scientists and researchers who want to strengthen their writing abilities in the scientific field, Coursera’s “Writing in the Sciences” course is an excellent resource. The course offers a thorough introduction to the rules and methods of effective scientific writing, covering topics like the rules of clear writing, the format of scientific papers, the use of evidence, peer review and editing, ethics and responsible research practices, as well as crucial abilities like grant writing, recommendation letter writing, review article writing, and personal statement writing.\nKristin Sainani’s intelligent and captivating presentations add interest and fun to the training. Because of her direct and plain teaching style, utilization of examples and real-world challenges, and supportive approach, students may confidently enhance their writing talents.\nBy improving these skills, scientists and researchers can more effectively communicate their discoveries, establish their knowledge and authority in the subject, and advance their scientific careers. The “Writing in the Sciences” course is a smart investment for anyone looking to develop their scientific writing skills and flourish in the sciences and beyond."
  },
  {
    "objectID": "posts/starting-python/index.html",
    "href": "posts/starting-python/index.html",
    "title": "How I Started Learning Python",
    "section": "",
    "text": "I became interested in Python about a year after I started my first full time job. I just felt that I should have some programming skills since I work with the internet and a laptop almost all the time. I started taking some informal courses on different platforms.\nI remember briefly attempting the ‘Data Analysis with Python certification’ on Freecodecamp but I never got to complete it. My interest was still there but I never took the next step. I then took the Google Data Analytics certification on coursera where I got introduced to R. After that, I took the IBM Data Science 10-course specialization and I got my first contact with Python. I learnt but I did not practice enough and after a few months, I only knew the basics but I could not apply the concepts to build projects on my own. The IBM Data Science Certificate was quite Pythonic as out of the ten courses in the specialization, five were Python courses:\n\nPython for data science\nData Analysis with Python\nPython project for data science\nData Visualization with Python\nMachine Learning with Python\n\nIn the end, I didn’t have a clear pathway to a future but I am happy that I am now in a fellowship with a community that can support me as I strive to achieve the programming goal. Alhamdulillah."
  },
  {
    "objectID": "posts/sympy/index.html",
    "href": "posts/sympy/index.html",
    "title": "Introduction to Symbolic Computation with SymPy",
    "section": "",
    "text": "Symbolic computation is a powerful approach in mathematics and computer science that deals with manipulating expressions and equations in their symbolic form. Unlike numerical computation, where values are approximated and manipulated, symbolic computation focuses on maintaining expressions with variables, allowing for precise mathematical manipulation. In this article, we will delve into symbolic differentiation using the popular SymPy library in Python."
  },
  {
    "objectID": "posts/sympy/index.html#basic-numeric-approximation",
    "href": "posts/sympy/index.html#basic-numeric-approximation",
    "title": "Introduction to Symbolic Computation with SymPy",
    "section": "Basic Numeric Approximation",
    "text": "Basic Numeric Approximation\nWhen approximating the square root of 18, you might typically use the math module in Python:\n\nimport math\n\nmath.sqrt(18)\n\n4.242640687119285\n\n\nHowever, this result is an approximation. To work symbolically, we turn to SymPy:\n\nimport sympy\n\nsqrt_expr = sympy.sqrt(18)\nsqrt_expr\n\n\\(\\displaystyle 3 \\sqrt{2}\\)\n\n\nSymPy provides an exact symbolic representation of the square root of 18. You can also evaluate this expression numerically to a specified number of decimal places:\n\napprox_val = sympy.N(sqrt_expr, 8)\napprox_val\n\n\\(\\displaystyle 4.2426407\\)"
  },
  {
    "objectID": "posts/sympy/index.html#symbolic-manipulation",
    "href": "posts/sympy/index.html#symbolic-manipulation",
    "title": "Introduction to Symbolic Computation with SymPy",
    "section": "Symbolic Manipulation",
    "text": "Symbolic Manipulation\nIn SymPy, expressions are built using symbols. Here’s an example of creating a symbolic expression corresponding to the mathematical expression \\(2x^{2}-xy\\) :\n\nx, y = sympy.symbols('x y')\nexpr = 2 * x**2 - x * y\nexpr\n\n\\(\\displaystyle 2 x^{2} - x y\\)\n\n\nWith symbolic expressions, you can perform various manipulations, such as addition, subtraction, and multiplication:\n\nexpr_manip = x * (expr + x * y + x**3)\nexpr_manip\n\n\\(\\displaystyle x \\left(x^{3} + 2 x^{2}\\right)\\)\n\n\nExpressions can also be expanded and factored using the expand and factor functions, respectively.\n\nSubstitution and Evaluation\nYou can substitute specific values into expressions using the evalf method:\n\nval = expr.evalf(subs={x: -1, y: 2})\nval\n\n\\(\\displaystyle 4.0\\)\n\n\nThis allows you to evaluate expressions as functions:\n\nf_symb = x ** 2\nf_val = f_symb.evalf(subs={x: 3})\nf_val \n\n\\(\\displaystyle 9.0\\)"
  },
  {
    "objectID": "posts/sympy/index.html#numeric-operations-on-symbolic-functions",
    "href": "posts/sympy/index.html#numeric-operations-on-symbolic-functions",
    "title": "Introduction to Symbolic Computation with SymPy",
    "section": "Numeric Operations on Symbolic Functions",
    "text": "Numeric Operations on Symbolic Functions\nTo evaluate a symbolic function for each element of an array, you need to make it NumPy-compatible:\n\nimport numpy as np\nfrom sympy.utilities.lambdify import lambdify\n\nx_array = np.array([1, 2, 3])\nf_symb_numpy = lambdify(x, f_symb, 'numpy')\n\nresult_array = f_symb_numpy(x_array)\nresult_array\n\narray([1, 4, 9])"
  },
  {
    "objectID": "posts/sympy/index.html#symbolic-differentiation-with-sympy",
    "href": "posts/sympy/index.html#symbolic-differentiation-with-sympy",
    "title": "Introduction to Symbolic Computation with SymPy",
    "section": "Symbolic Differentiation with SymPy",
    "text": "Symbolic Differentiation with SymPy\nSymPy excels in symbolic differentiation. Finding derivatives is straightforward:\n\ndiff_result = sympy.diff(x**3, x)\ndiff_result\n\n\\(\\displaystyle 3 x^{2}\\)\n\n\nSymPy handles standard functions and applies necessary rules for differentiation:\n\ndfdx_composed = sympy.diff(sympy.exp(-2*x) + 3*sympy.sin(3*x), x)\nprint(dfdx_composed)\n\n9*cos(3*x) - 2*exp(-2*x)\n\n\nYou can even differentiate the symbolic expression from before and make it NumPy-friendly:\n\ndfdx_symb = sympy.diff(expr, x)\ndfdx_symb_numpy = lambdify(x, dfdx_symb, 'numpy')\n\ndiff_result_array = dfdx_symb_numpy(x_array)\ndiff_result_array\n\narray([4 - y, 8 - y, 12 - y], dtype=object)"
  },
  {
    "objectID": "posts/sympy/index.html#limitations-of-symbolic-differentiation",
    "href": "posts/sympy/index.html#limitations-of-symbolic-differentiation",
    "title": "Introduction to Symbolic Computation with SymPy",
    "section": "Limitations of Symbolic Differentiation",
    "text": "Limitations of Symbolic Differentiation\nDespite its advantages, symbolic differentiation has limitations. Complex expressions might lead to inefficient or unevaluable results. For example, consider differentiating \\(|x|\\) :\n\ndfdx_abs = sympy.diff(abs(x), x)\nprint(dfdx_abs)\n\n(re(x)*Derivative(re(x), x) + im(x)*Derivative(im(x), x))*sign(x)/x\n\n\nEvaluating we get:\n\neval_result = dfdx_abs.evalf(subs={x: -2})\neval_result\n\n\\(\\displaystyle - \\left. \\frac{d}{d x} \\operatorname{re}{\\left(x\\right)} \\right|_{\\substack{ x=-2 }}\\)\n\n\n\ntry:\n    dfdx_abs_numpy = lambdify(x, dfdx_abs, 'numpy')\n    dfdx_abs_numpy(np.array([1, -2, 0]))\nexcept NameError as err:\n    print(err)\n\nname 'Derivative' is not defined"
  },
  {
    "objectID": "posts/sympy/index.html#numerical-differentiation",
    "href": "posts/sympy/index.html#numerical-differentiation",
    "title": "Introduction to Symbolic Computation with SymPy",
    "section": "Numerical Differentiation",
    "text": "Numerical Differentiation\nNumerical differentiation approximates derivatives using nearby points and is available through libraries like NumPy. This approach focuses on function evaluation rather than symbolic expressions:\ndelta_x = 0.01\nnumerical_derivative = (f(x + delta_x) - f(x)) / delta_x\n\nNumerical Differentiation with NumPy\nNumPy provides the np.gradient function for numerical differentiation:\n\nimport numpy as np\n\nx_vals = np.linspace(0, 10, 100)\ny_vals = np.sin(x_vals)\n\nderivatives = np.gradient(y_vals, x_vals)\nprint(derivatives)\n\n[ 0.99830036  0.99321184  0.97799815  0.95281439  0.91791729  0.8736626\n  0.82050147  0.75897585  0.68971295  0.61341886  0.53087135  0.44291195\n  0.35043734  0.25439024  0.15574979  0.05552157 -0.04527265 -0.14560535\n -0.2444537  -0.34080999 -0.43369194 -0.52215268 -0.6052904  -0.68225756\n -0.75226954 -0.81461261 -0.86865122 -0.91383447 -0.94970177 -0.97588745\n -0.99212457 -0.99824762 -0.99419416 -0.98000551 -0.95582633 -0.92190311\n -0.87858166 -0.82630363 -0.76560196 -0.69709546 -0.62148251 -0.53953394\n -0.45208516 -0.36002765 -0.2642999  -0.16587777 -0.06576463  0.03501895\n  0.13544553  0.23449132  0.33114663  0.4244261   0.51337882  0.59709797\n  0.67473008  0.74548374  0.80863767  0.86354805  0.9096551   0.94648879\n  0.97367362  0.99093247  0.99808939  0.99507142  0.98190932  0.95873728\n  0.92579151  0.88340789  0.83201848  0.77214717  0.7044043   0.62948048\n  0.54813951  0.46121059  0.36957993  0.27418163  0.17598823  0.07600073\n -0.02476154 -0.12527139 -0.22450417 -0.32144828 -0.41511542 -0.50455072\n -0.58884245 -0.6671313  -0.73861917 -0.80257728 -0.85835363 -0.9053796\n -0.9431758  -0.97135691 -0.98963566 -0.9978257  -0.99584353 -0.98370937\n -0.96154691 -0.92958209 -0.88814077 -0.86509787]"
  },
  {
    "objectID": "posts/sympy/index.html#conclusion",
    "href": "posts/sympy/index.html#conclusion",
    "title": "Introduction to Symbolic Computation with SymPy",
    "section": "Conclusion",
    "text": "Conclusion\nSymbolic computation with SymPy offers a versatile way to manipulate mathematical expressions and perform differentiation symbolically. While powerful, it has limitations in handling complex expressions and might lead to inefficient computations. Numerical differentiation, on the other hand, provides an alternative for cases where symbolic computation might fall short. By combining these techniques, you can effectively explore and analyze mathematical functions in Python."
  },
  {
    "objectID": "posts/tensorflow-fizzbuzz/index.html",
    "href": "posts/tensorflow-fizzbuzz/index.html",
    "title": "Computation Graphs, Eager Execution and Flow Control in TensorFlow",
    "section": "",
    "text": "TensorFlow is a popular deep learning framework that provides a robust platform for the creation and execution of computational graphs. Understanding how TensorFlow handles computation through graphs, eager execution, and flow control is pivotal for effectively deploying machine learning/deep learning models.\n1. Computation Graphs in TensorFlow:\nA computation graph is a series of TensorFlow operations arranged into a graph of nodes. Each node represents an operation, while the edges represent the data consumed or produced by an operation. This structure allows TensorFlow to optimize the computation, especially in deep learning models.\nBenefits of Computation Graphs:\n\nEfficiency: Operations can be parallelized across different processors (CPUs, GPUs).\nPortability: The graph can be executed on different devices and platforms.\n\n2. Eager Execution in TensorFlow:\nEager execution is an imperative programming environment that evaluates operations immediately. It contrasts with graph execution in that it doesn’t require a computational graph to be defined before running operations.\nAdvantages of Eager Execution:\n\nInteractive Debugging: Operations are executed as they are defined, facilitating easy debugging.\nIntuitive Interface: It aligns with the way programmers are used to thinking about their programs.\n\n\nFlow Control in TensorFlow:\n\nTensorFlow provides various tools for flow control, enabling the creation of dynamic models. This includes conditionals and loops, which are essential in many machine learning algorithms.\n\n\n\ntf.cond: Provides a way to perform conditional execution.\ntf.while_loop: Allows for the creation of dynamic loops in the graph.\ntf.switch_case: Used for creating conditional branching.\n\n\n\nIn a few lines of code, I try to demonstate some tensorflow functionalities using the popular FizzBuzz.\n\nimport tensorflow as tf\n\ndef fizzbuzz(max_num):\n    for num in range(1, max_num + 1):\n        num_tf = tf.constant(num)\n        fizz = tf.equal(tf.math.mod(num_tf, 3), 0)\n        buzz = tf.equal(tf.math.mod(num_tf, 5), 0)\n        fizzbuzz = tf.logical_and(fizz, buzz)\n\n        print(tf.switch_case(tf.cast(fizzbuzz, tf.int32) + tf.cast(fizz, tf.int32) + 2 * tf.cast(buzz, tf.int32),\n                             branch_fns={\n                                 0: lambda: num_tf.numpy(),\n                                 1: lambda: tf.constant(\"Fizz\").numpy().decode(\"utf-8\"),\n                                 2: lambda: tf.constant(\"Buzz\").numpy().decode(\"utf-8\"),\n                                 3: lambda: tf.constant(\"FizzBuzz\").numpy().decode(\"utf-8\")\n                             }))\n\nfizzbuzz(15)\n\n2023-11-05 21:07:15.366672: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-11-05 21:07:22.171045: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n\n\n1\n2\nFizz\n4\nBuzz\nFizz\n7\n8\nFizz\nBuzz\n11\nFizz\n13\n14\nFizzBuzz"
  },
  {
    "objectID": "posts/tensorflow-fizzbuzz/index.html#introduction",
    "href": "posts/tensorflow-fizzbuzz/index.html#introduction",
    "title": "Computation Graphs, Eager Execution and Flow Control in TensorFlow",
    "section": "",
    "text": "TensorFlow is a popular deep learning framework that provides a robust platform for the creation and execution of computational graphs. Understanding how TensorFlow handles computation through graphs, eager execution, and flow control is pivotal for effectively deploying machine learning/deep learning models.\n1. Computation Graphs in TensorFlow:\nA computation graph is a series of TensorFlow operations arranged into a graph of nodes. Each node represents an operation, while the edges represent the data consumed or produced by an operation. This structure allows TensorFlow to optimize the computation, especially in deep learning models.\nBenefits of Computation Graphs:\n\nEfficiency: Operations can be parallelized across different processors (CPUs, GPUs).\nPortability: The graph can be executed on different devices and platforms.\n\n2. Eager Execution in TensorFlow:\nEager execution is an imperative programming environment that evaluates operations immediately. It contrasts with graph execution in that it doesn’t require a computational graph to be defined before running operations.\nAdvantages of Eager Execution:\n\nInteractive Debugging: Operations are executed as they are defined, facilitating easy debugging.\nIntuitive Interface: It aligns with the way programmers are used to thinking about their programs.\n\n\nFlow Control in TensorFlow:\n\nTensorFlow provides various tools for flow control, enabling the creation of dynamic models. This includes conditionals and loops, which are essential in many machine learning algorithms.\n\n\n\ntf.cond: Provides a way to perform conditional execution.\ntf.while_loop: Allows for the creation of dynamic loops in the graph.\ntf.switch_case: Used for creating conditional branching.\n\n\n\nIn a few lines of code, I try to demonstate some tensorflow functionalities using the popular FizzBuzz.\n\nimport tensorflow as tf\n\ndef fizzbuzz(max_num):\n    for num in range(1, max_num + 1):\n        num_tf = tf.constant(num)\n        fizz = tf.equal(tf.math.mod(num_tf, 3), 0)\n        buzz = tf.equal(tf.math.mod(num_tf, 5), 0)\n        fizzbuzz = tf.logical_and(fizz, buzz)\n\n        print(tf.switch_case(tf.cast(fizzbuzz, tf.int32) + tf.cast(fizz, tf.int32) + 2 * tf.cast(buzz, tf.int32),\n                             branch_fns={\n                                 0: lambda: num_tf.numpy(),\n                                 1: lambda: tf.constant(\"Fizz\").numpy().decode(\"utf-8\"),\n                                 2: lambda: tf.constant(\"Buzz\").numpy().decode(\"utf-8\"),\n                                 3: lambda: tf.constant(\"FizzBuzz\").numpy().decode(\"utf-8\")\n                             }))\n\nfizzbuzz(15)\n\n2023-11-05 21:07:15.366672: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-11-05 21:07:22.171045: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n\n\n1\n2\nFizz\n4\nBuzz\nFizz\n7\n8\nFizz\nBuzz\n11\nFizz\n13\n14\nFizzBuzz"
  },
  {
    "objectID": "posts/tensorflow-fizzbuzz/index.html#conclusion",
    "href": "posts/tensorflow-fizzbuzz/index.html#conclusion",
    "title": "Computation Graphs, Eager Execution and Flow Control in TensorFlow",
    "section": "Conclusion:",
    "text": "Conclusion:\nThe versatility of TensorFlow lies in its ability to seamlessly switch between a static computation graph and eager execution, providing both efficiency and flexibility. Understanding these concepts is essential for any machine learning practitioner working with TensorFlow. Whether you’re implementing simple programs like FizzBuzz or developing complex neural networks, mastering these aspects of TensorFlow will greatly enhance your ability to develop and optimize machine learning models.\nOn a final note, readers should remember that TensorFlow is an evolving platform and therefore try to refer to the latest documentation for up-to-date practices and API usage."
  },
  {
    "objectID": "posts/tensorflow-pytorch-fizzbuzz/index.html",
    "href": "posts/tensorflow-pytorch-fizzbuzz/index.html",
    "title": "Exploring Deep Learning Frameworks: FizzBuzz with PyTorch, TensorFlow, and Keras",
    "section": "",
    "text": "The world of deep learning is dominated by a few key frameworks, each with its unique strengths and idiosyncrasies. PyTorch and TensorFlow are two of the most popular tools in this space, widely used by researchers and industry professionals alike. In this article, we’ll explore the differences between these frameworks by implementing the classic FizzBuzz problem in both PyTorch and TensorFlow. Additionally, we’ll touch upon Keras, a high-level API for TensorFlow, towards the end."
  },
  {
    "objectID": "posts/tensorflow-pytorch-fizzbuzz/index.html#introduction",
    "href": "posts/tensorflow-pytorch-fizzbuzz/index.html#introduction",
    "title": "Exploring Deep Learning Frameworks: FizzBuzz with PyTorch, TensorFlow, and Keras",
    "section": "",
    "text": "The world of deep learning is dominated by a few key frameworks, each with its unique strengths and idiosyncrasies. PyTorch and TensorFlow are two of the most popular tools in this space, widely used by researchers and industry professionals alike. In this article, we’ll explore the differences between these frameworks by implementing the classic FizzBuzz problem in both PyTorch and TensorFlow. Additionally, we’ll touch upon Keras, a high-level API for TensorFlow, towards the end."
  },
  {
    "objectID": "posts/tensorflow-pytorch-fizzbuzz/index.html#fizzbuzz-in-pytorch",
    "href": "posts/tensorflow-pytorch-fizzbuzz/index.html#fizzbuzz-in-pytorch",
    "title": "Exploring Deep Learning Frameworks: FizzBuzz with PyTorch, TensorFlow, and Keras",
    "section": "FizzBuzz in PyTorch:",
    "text": "FizzBuzz in PyTorch:\nPyTorch, developed by Facebook’s AI Research lab, is known for its simplicity, ease of use, and dynamic computational graph.\n\nimport torch\n\ndef fizzbuzz_pytorch(max_num):\n    for num in range(1, max_num + 1):\n        num_torch = torch.tensor(num)\n        fizz = torch.eq(num_torch % 3, 0)\n        buzz = torch.eq(num_torch % 5, 0)\n        fizzbuzz = torch.logical_and(fizz, buzz)\n\n        if fizzbuzz.item():\n            print(\"FizzBuzz\")\n        elif fizz.item():\n            print(\"Fizz\")\n        elif buzz.item():\n            print(\"Buzz\")\n        else:\n            print(num)\n\nfizzbuzz_pytorch(15)\n\n1\n2\nFizz\n4\nBuzz\nFizz\n7\n8\nFizz\nBuzz\n11\nFizz\n13\n14\nFizzBuzz\n\n\n\nKey Points:\n\nDynamic Graphs: PyTorch uses dynamic computational graphs (also known as define-by-run graphs). This means that the graph is built on the fly as operations are executed. This is evident in the way PyTorch handles the FizzBuzz logic, providing a more intuitive Pythonic feel.\nEase of Debugging: Thanks to its dynamic nature, debugging in PyTorch can be more straightforward using standard Python debugging tools."
  },
  {
    "objectID": "posts/tensorflow-pytorch-fizzbuzz/index.html#fizzbuzz-in-tensorflow",
    "href": "posts/tensorflow-pytorch-fizzbuzz/index.html#fizzbuzz-in-tensorflow",
    "title": "Exploring Deep Learning Frameworks: FizzBuzz with PyTorch, TensorFlow, and Keras",
    "section": "FizzBuzz in TensorFlow:",
    "text": "FizzBuzz in TensorFlow:\nTensorFlow, developed by the Google Brain team, is renowned for its powerful, scalable, and production-ready features.\n\nimport tensorflow as tf\n\ndef fizzbuzz_tensorflow(max_num):\n    for num in range(1, max_num + 1):\n        num_tf = tf.constant(num)\n        fizz = tf.equal(tf.math.mod(num_tf, 3), 0)\n        buzz = tf.equal(tf.math.mod(num_tf, 5), 0)\n        fizzbuzz = tf.logical_and(fizz, buzz)\n\n        print(tf.switch_case(tf.cast(fizzbuzz, tf.int32) + tf.cast(fizz, tf.int32) + 2 * tf.cast(buzz, tf.int32),\n                             branch_fns={\n                                 0: lambda: num_tf.numpy(),\n                                 1: lambda: tf.constant(\"Fizz\").numpy().decode(\"utf-8\"),\n                                 2: lambda: tf.constant(\"Buzz\").numpy().decode(\"utf-8\"),\n                                 3: lambda: tf.constant(\"FizzBuzz\").numpy().decode(\"utf-8\")\n                             }))\n\nfizzbuzz_tensorflow(15)\n\n2023-11-07 21:59:05.053558: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-11-07 21:59:35.805842: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n\n\n1\n2\nFizz\n4\nBuzz\nFizz\n7\n8\nFizz\nBuzz\n11\nFizz\n13\n14\nFizzBuzz\n\n\n\nKey Points:\n\nStatic Graphs: TensorFlow traditionally used static computational graphs, where the graph is defined before it is executed. TensorFlow 2.x, however, introduced eager execution, which allows a more dynamic approach, similar to PyTorch.\nScalability and Deployment: TensorFlow shines in scalability and deployment, especially in distributed settings and production environments."
  },
  {
    "objectID": "posts/tensorflow-pytorch-fizzbuzz/index.html#understanding-the-differences",
    "href": "posts/tensorflow-pytorch-fizzbuzz/index.html#understanding-the-differences",
    "title": "Exploring Deep Learning Frameworks: FizzBuzz with PyTorch, TensorFlow, and Keras",
    "section": "Understanding the Differences:",
    "text": "Understanding the Differences:\nWhile both implementations achieve the same goal, they highlight some fundamental differences between the two frameworks:\n\nGraph Building: In TensorFlow, you often define placeholders and sessions (though less so with eager execution), whereas PyTorch adopts a more straightforward approach using regular Python variables and loops.\nTensors: Both frameworks use tensors as their fundamental data structure, but the way they handle these tensors varies, reflecting their different approaches to graph computation."
  },
  {
    "objectID": "posts/tensorflow-pytorch-fizzbuzz/index.html#a-note-on-keras",
    "href": "posts/tensorflow-pytorch-fizzbuzz/index.html#a-note-on-keras",
    "title": "Exploring Deep Learning Frameworks: FizzBuzz with PyTorch, TensorFlow, and Keras",
    "section": "A Note on Keras:",
    "text": "A Note on Keras:\nKeras, now fully integrated into TensorFlow as tf.keras, offers a high-level, user-friendly API. It abstracts many details of TensorFlow, making it accessible for beginners. While Keras might not be the first choice for implementing a simple program like FizzBuzz, it’s an invaluable tool for more complex deep learning models, offering pre-built layers, models, and a wealth of utilities."
  },
  {
    "objectID": "posts/tensorflow-pytorch-fizzbuzz/index.html#conclusion",
    "href": "posts/tensorflow-pytorch-fizzbuzz/index.html#conclusion",
    "title": "Exploring Deep Learning Frameworks: FizzBuzz with PyTorch, TensorFlow, and Keras",
    "section": "Conclusion:",
    "text": "Conclusion:\nIn conclusion, both PyTorch and TensorFlow have their distinct advantages, with PyTorch often being praised for its user-friendly approach and TensorFlow for its scalability and robust deployment capabilities. Keras, as part of TensorFlow, further simplifies the deep learning process, allowing developers to build complex models with ease. Understanding these differences and strengths is crucial for any aspiring or practicing data scientist or AI engineer, helping them choose the right tool for their specific needs and projects."
  },
  {
    "objectID": "posts/visualization/index.html",
    "href": "posts/visualization/index.html",
    "title": "From Data to Insight: Visualizing Quantities, Proportions, Relationships, and Distributions with Python’s Matplotlib and Seaborn",
    "section": "",
    "text": "Data visualization is a crucial part of data analysis as it aids in our ability to comprehend and convey to a larger audience complicated data correlations, patterns, and insights. Thanks to its potent modules, such as Matplotlib and Seaborn, Python has gained popularity as a language for data visualization. In this post, we’ll look at how to visualize quantities, proportions, relationships, and distributions using these libraries and how to get useful insights out of them."
  },
  {
    "objectID": "posts/visualization/index.html#visualizing-quantities",
    "href": "posts/visualization/index.html#visualizing-quantities",
    "title": "From Data to Insight: Visualizing Quantities, Proportions, Relationships, and Distributions with Python’s Matplotlib and Seaborn",
    "section": "Visualizing Quantities",
    "text": "Visualizing Quantities\nAny information that is measurable and able to be expressed numerically is considered quantitative. Understanding the distribution and dispersion of values, the existence of outliers, and the link between various variables is made easier by visualizing quantitative data.\nHistograms are one of the most used tools for visualizing quantitative data. A graph that displays the frequency distribution of a collection of continuous data is called a histogram. Histograms can be easily made with only a few lines of code thanks to Matplotlib and Seaborn. Let’s look at an illustration:\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\ndata = np.random.normal(size=1000)\n\n# Create a histogram using Matplotlib\nplt.hist(data, bins=30)\nplt.title('Histogram of Random Data')\nplt.xlabel('Values')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Create a histogram using Seaborn\nsns.histplot(data, bins=30)\nplt.title('Histogram of Random Data')\nplt.xlabel('Values')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\n\nUsing NumPy’s random.normal function, we produce some random data in this example. Then, we use Matplotlib and Seaborn to produce two histograms, one using each program. While the histograms generated by the two libraries are comparable, Seaborn’s histplot function includes a few more capabilities, such as the ability to plot a KDE (Kernel Density Estimate) in addition to the histogram."
  },
  {
    "objectID": "posts/visualization/index.html#visualizing-proportions",
    "href": "posts/visualization/index.html#visualizing-proportions",
    "title": "From Data to Insight: Visualizing Quantities, Proportions, Relationships, and Distributions with Python’s Matplotlib and Seaborn",
    "section": "Visualizing Proportions",
    "text": "Visualizing Proportions\nAny information that may be stated as a percentage or a fraction of a whole is considered proportional data. Understanding the relative sizes of several categories or groups and how they affect the overall picture is made easier with the aid of proportional data visualization. Using a pie chart, donut chart, or waffle chart, you can see proportions.\nPie charts are one of the most commonly used tools for representing proportional data. A circular graph called a pie chart demonstrates the proportional breakdown of a group of categorical data. Both Matplotlib and Seaborn provide pie chart creation routines. Let’s look at an illustration:\n\ndata = [25, 40, 10, 25]\nlabels = ['A', 'B', 'C', 'D']\n\n# Create a pie chart using Matplotlib\nplt.pie(data, labels=labels)\nplt.title('Proportional Data')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Create a pie chart using Seaborn\nsns.set_style('whitegrid')\nplt.title('Proportional Data')\nsns.color_palette('pastel')\nplt.pie(data, labels=labels, colors=sns.color_palette())\nplt.show()\n\n\n\n\n\n\n\n\nIn this illustration, we generate a set of labels and random data. After that, we use Seaborn and Matplotlib, respectively, to construct two pie charts. We can color-code each category using a specified palette thanks to Seaborn’s color_palette function."
  },
  {
    "objectID": "posts/visualization/index.html#visualizing-relationships",
    "href": "posts/visualization/index.html#visualizing-relationships",
    "title": "From Data to Insight: Visualizing Quantities, Proportions, Relationships, and Distributions with Python’s Matplotlib and Seaborn",
    "section": "Visualizing Relationships",
    "text": "Visualizing Relationships\nAny data that demonstrates the connection between two or more variables is referred to as relationship data. Visualizing connection data enables us to comprehend the correlation, directionality, and linear or nonlinear nature of the relationship.\nScatter plots are among the most commonly used tools for displaying relationship data. An illustration of the relationship between two sets of data is a scatter plot. Both Matplotlib and Seaborn include tools for making scatter plots. Let’s look at an illustration:\n\n# Create some random data\nx = np.random.normal(size=1000)\ny = 2 * x + np.random.normal(size=1000)\n\n# Create a scatter plot using Matplotlib\nplt.scatter(x, y)\nplt.title('Scatter Plot of Random Data')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Create a scatter plot using Seaborn\nsns.scatterplot(x=x, y=y)\nplt.title('Scatter Plot of Random Data')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.show()\n\n\n\n\n\n\n\n\nIn this illustration, we generate the random data sets x and y. Then, we use Matplotlib and Seaborn to construct two scatter plots, one using each program. The scatterplot function in Seaborn contains a few extra capabilities, like the ability to color-code points according to a third variable."
  },
  {
    "objectID": "posts/visualization/index.html#visualizing-distributions",
    "href": "posts/visualization/index.html#visualizing-distributions",
    "title": "From Data to Insight: Visualizing Quantities, Proportions, Relationships, and Distributions with Python’s Matplotlib and Seaborn",
    "section": "Visualizing Distributions",
    "text": "Visualizing Distributions\nAny data that demonstrates the frequency of occurrence of various values or ranges of values is referred to as distribution data. Understanding the distribution’s shape, the existence of outliers, and the likelihood of specific values or ranges of values are all made easier by visualizing distribution data.\nDensity plots are one of the most popular tools for displaying distribution data. A graph that displays the probability density function of a collection of data is called a density plot. Both Matplotlib and Seaborn include tools for making density graphs. Let’s look at an illustration:\n\ndata = np.random.normal(size=1000)\n\n# Create a density plot using Matplotlib\nplt.hist(data, density=True, alpha=0.5, bins=30)\nsns.kdeplot(data, color='red')\nplt.title('Density Plot of Random Data')\nplt.xlabel('Values')\nplt.ylabel('Probability Density')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Create a density plot using Seaborn\nsns.histplot(data, kde=True, stat='density', alpha=0.5, bins=30)\nplt.title('Density Plot of Random Data')\nplt.xlabel('Values')\nplt.ylabel('Probability Density')\nplt.show()\n\n\n\n\n\n\n\n\nUsing NumPy’s random.normal function, we generate a set of random data in this example. Then, we use Matplotlib and Seaborn to produce two density charts, one using each program. We may combine a histogram and a density plot into one visualization using Seaborn’s histplot function."
  },
  {
    "objectID": "posts/web-scraping/index.html",
    "href": "posts/web-scraping/index.html",
    "title": "Web Scraping a Daily Trust Article using Requests and Beautiful Soup Libraries",
    "section": "",
    "text": "Web scraping is a key skill all data scientists should have. This is why day 22 of our ongoing Arewa Data Science Academy fellowship covered web scraping using BeautifulSoup. In this article, I try to explain and practice this important skill by scraping a daily trust page that contains an article related to the just concluded Nigerian presidential election.  Firstly, for this task, one needs the following: - A Python installation -I have Python 3.10 installed,  - requests library - BeautifulSoup4 library\nBoth libraries can be installed using pip, or if one uses a conda environment - like I do - the libraries can be installed using conda install . In addition, the documentation for both libraries, which are hyperlinked above, provide description for how to install the libraries.  For the purpose of this article, I used a colab Jupyter notebook, which is free from google and only requires a google account and an internet connection. There’s no need to install anything else.  We first begin by importing the two required libraries\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nRequests library enables us to get the content of the page as a html and BeautifulSoup allows us to parse the html. \nWe then create a beautifulSoup object that we can use to navigate the content of the page as follows:\n\nurl = 'https://dailytrust.com/last-saturday-polls-raised-issues-that-require-immediate-solutions-inec/'\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.content,'html.parser')\nsoup.prettify()\n\n'&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"en-GB\"&gt;\\n &lt;head&gt;\\n  &lt;style&gt;\\n   img.lazy{min-height:1px}\\n  &lt;/style&gt;\\n  &lt;link as=\"script\" href=\"https://dailytrust.com/wp-content/plugins/w3-total-cache/pub/js/lazyload.min.js\" rel=\"preload\"/&gt;\\n  &lt;meta charset=\"utf-8\"/&gt;\\n  &lt;meta content=\"IE=edge\" httpequiv=\"X-UA-Compatible\"/&gt;\\n  &lt;style id=\"\" media=\"all\"&gt;\\n   @font-face {\\n  font-family: \\'Caveat\\';\\n  font-style: normal;\\n  font-weight: 400;\\n  font-display: swap;\\n  src: url(/fonts.gstatic.com/s/caveat/v17/WnznHAc5bAfYB2QRah7pcpNvOx-pjfJ9SII.ttf) format(\\'truetype\\');\\n}\\n  &lt;/style&gt;\\n  &lt;meta content=\"minimum-scale=1, initial-scale=1, width=device-width, shrink-to-fit=no, user-scalable=yes, viewport-fit=cover\" name=\"viewport\"/&gt;\\n  &lt;link href=\"/manifest.json\" rel=\"manifest\"/&gt;\\n  &lt;meta content=\"#000000\" name=\"theme-color\"/&gt;\\n  &lt;title&gt;\\n   Last Saturday polls raised issues that require immediate solutions – INEC - Daily Trust\\n  &lt;/title&gt;\\n  &lt;meta content=\"max-image-preview:large\" name=\"robots\"/&gt;\\n  &lt;link href=\"//www.googletagmanager.com\" rel=\"dns-prefetch\"/&gt;\\n  &lt;link href=\"//stats.wp.com\" rel=\"dns-prefetch\"/&gt;\\n  &lt;link href=\"//widgets.wp.com\" rel=\"dns-prefetch\"/&gt;\\n  &lt;link href=\"//s0.wp.com\" rel=\"dns-prefetch\"/&gt;\\n  &lt;link href=\"//0.gravatar.com\" rel=\"dns-prefetch\"/&gt;\\n  &lt;link href=\"//1.gravatar.com\" rel=\"dns-prefetch\"/&gt;\\n  &lt;link href=\"//2.gravatar.com\" rel=\"dns-prefetch\"/&gt;\\n  &lt;link href=\"//pagead2.googlesyndication.com\" rel=\"dns-prefetch\"/&gt;\\n  &lt;link href=\"https://dailytrust.com/wp-content/cache/minify/19c28.css\" media=\"all\" rel=\"stylesheet\"/&gt;\\n  &lt;style id=\"global-styles-inline-css\"&gt;\\n   /*&lt;![CDATA[*/body{--wp--preset--color--black:#000;--wp--preset--color--cyan-bluish-gray:#abb8c3;--wp--preset--color--white:#fff;--wp--preset--color--pale-pink:#f78da7;--wp--preset--color--vivid-red:#cf2e2e;--wp--preset--color--luminous-vivid-orange:#ff6900;--wp--preset--color--luminous-vivid-amber:#fcb900;--wp--preset--color--light-green-cyan:#7bdcb5;--wp--preset--color--vivid-green-cyan:#00d084;--wp--preset--color--pale-cyan-blue:#8ed1fc;--wp--preset--color--vivid-cyan-blue:#0693e3;--wp--preset--color--vivid-purple:#9b51e0;--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple:linear-gradient(135deg,rgba(6,147,227,1) 0%,rgb(155,81,224) 100%);--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan:linear-gradient(135deg,rgb(122,220,180) 0%,rgb(0,208,130) 100%);--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange:linear-gradient(135deg,rgba(252,185,0,1) 0%,rgba(255,105,0,1) 100%);--wp--preset--gradient--luminous-vivid-orange-to-vivid-red:linear-gradient(135deg,rgba(255,105,0,1) 0%,rgb(207,46,46) 100%);--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray:linear-gradient(135deg,rgb(238,238,238) 0%,rgb(169,184,195) 100%);--wp--preset--gradient--cool-to-warm-spectrum:linear-gradient(135deg,rgb(74,234,220) 0%,rgb(151,120,209) 20%,rgb(207,42,186) 40%,rgb(238,44,130) 60%,rgb(251,105,98) 80%,rgb(254,248,76) 100%);--wp--preset--gradient--blush-light-purple:linear-gradient(135deg,rgb(255,206,236) 0%,rgb(152,150,240) 100%);--wp--preset--gradient--blush-bordeaux:linear-gradient(135deg,rgb(254,205,165) 0%,rgb(254,45,45) 50%,rgb(107,0,62) 100%);--wp--preset--gradient--luminous-dusk:linear-gradient(135deg,rgb(255,203,112) 0%,rgb(199,81,192) 50%,rgb(65,88,208) 100%);--wp--preset--gradient--pale-ocean:linear-gradient(135deg,rgb(255,245,203) 0%,rgb(182,227,212) 50%,rgb(51,167,181) 100%);--wp--preset--gradient--electric-grass:linear-gradient(135deg,rgb(202,248,128) 0%,rgb(113,206,126) 100%);--wp--preset--gradient--midnight:linear-gradient(135deg,rgb(2,3,129) 0%,rgb(40,116,252) 100%);--wp--preset--duotone--dark-grayscale:url(\\'#wp-duotone-dark-grayscale\\');--wp--preset--duotone--grayscale:url(\\'#wp-duotone-grayscale\\');--wp--preset--duotone--purple-yellow:url(\\'#wp-duotone-purple-yellow\\');--wp--preset--duotone--blue-red:url(\\'#wp-duotone-blue-red\\');--wp--preset--duotone--midnight:url(\\'#wp-duotone-midnight\\');--wp--preset--duotone--magenta-yellow:url(\\'#wp-duotone-magenta-yellow\\');--wp--preset--duotone--purple-green:url(\\'#wp-duotone-purple-green\\');--wp--preset--duotone--blue-orange:url(\\'#wp-duotone-blue-orange\\');--wp--preset--font-size--small:13px;--wp--preset--font-size--medium:20px;--wp--preset--font-size--large:36px;--wp--preset--font-size--x-large:42px;--wp--preset--spacing--20:0.44rem;--wp--preset--spacing--30:0.67rem;--wp--preset--spacing--40:1rem;--wp--preset--spacing--50:1.5rem;--wp--preset--spacing--60:2.25rem;--wp--preset--spacing--70:3.38rem;--wp--preset--spacing--80:5.06rem;--wp--preset--shadow--natural:6px 6px 9px rgba(0, 0, 0, 0.2);--wp--preset--shadow--deep:12px 12px 50px rgba(0, 0, 0, 0.4);--wp--preset--shadow--sharp:6px 6px 0px rgba(0, 0, 0, 0.2);--wp--preset--shadow--outlined:6px 6px 0px -3px rgba(255, 255, 255, 1), 6px 6px rgba(0, 0, 0, 1);--wp--preset--shadow--crisp:6px 6px 0px rgba(0, 0, 0, 1)}:where(.is-layout-flex){gap:0.5em}body .is-layout-flow&gt;.alignleft{float:left;margin-inline-start:0;margin-inline-end:2em}body .is-layout-flow&gt;.alignright{float:right;margin-inline-start:2em;margin-inline-end:0}body .is-layout-flow&gt;.aligncenter{margin-left:auto !important;margin-right:auto !important}body .is-layout-constrained&gt;.alignleft{float:left;margin-inline-start:0;margin-inline-end:2em}body .is-layout-constrained&gt;.alignright{float:right;margin-inline-start:2em;margin-inline-end:0}body .is-layout-constrained&gt;.aligncenter{margin-left:auto !important;margin-right:auto !important}body .is-layout-constrained&gt;:where(:not(.alignleft):not(.alignright):not(.alignfull)){max-width:var(--wp--style--global--content-size);margin-left:auto !important;margin-right:auto !important}body .is-layout-constrained&gt;.alignwide{max-width:var(--wp--style--global--wide-size)}body .is-layout-flex{display:flex}body .is-layout-flex{flex-wrap:wrap;align-items:center}body .is-layout-flex&gt;*{margin:0}:where(.wp-block-columns.is-layout-flex){gap:2em}.has-black-color{color:var(--wp--preset--color--black) !important}.has-cyan-bluish-gray-color{color:var(--wp--preset--color--cyan-bluish-gray) !important}.has-white-color{color:var(--wp--preset--color--white) !important}.has-pale-pink-color{color:var(--wp--preset--color--pale-pink) !important}.has-vivid-red-color{color:var(--wp--preset--color--vivid-red) !important}.has-luminous-vivid-orange-color{color:var(--wp--preset--color--luminous-vivid-orange) !important}.has-luminous-vivid-amber-color{color:var(--wp--preset--color--luminous-vivid-amber) !important}.has-light-green-cyan-color{color:var(--wp--preset--color--light-green-cyan) !important}.has-vivid-green-cyan-color{color:var(--wp--preset--color--vivid-green-cyan) !important}.has-pale-cyan-blue-color{color:var(--wp--preset--color--pale-cyan-blue) !important}.has-vivid-cyan-blue-color{color:var(--wp--preset--color--vivid-cyan-blue) !important}.has-vivid-purple-color{color:var(--wp--preset--color--vivid-purple) !important}.has-black-background-color{background-color:var(--wp--preset--color--black) !important}.has-cyan-bluish-gray-background-color{background-color:var(--wp--preset--color--cyan-bluish-gray) !important}.has-white-background-color{background-color:var(--wp--preset--color--white) !important}.has-pale-pink-background-color{background-color:var(--wp--preset--color--pale-pink) !important}.has-vivid-red-background-color{background-color:var(--wp--preset--color--vivid-red) !important}.has-luminous-vivid-orange-background-color{background-color:var(--wp--preset--color--luminous-vivid-orange) !important}.has-luminous-vivid-amber-background-color{background-color:var(--wp--preset--color--luminous-vivid-amber) !important}.has-light-green-cyan-background-color{background-color:var(--wp--preset--color--light-green-cyan) !important}.has-vivid-green-cyan-background-color{background-color:var(--wp--preset--color--vivid-green-cyan) !important}.has-pale-cyan-blue-background-color{background-color:var(--wp--preset--color--pale-cyan-blue) !important}.has-vivid-cyan-blue-background-color{background-color:var(--wp--preset--color--vivid-cyan-blue) !important}.has-vivid-purple-background-color{background-color:var(--wp--preset--color--vivid-purple) !important}.has-black-border-color{border-color:var(--wp--preset--color--black) !important}.has-cyan-bluish-gray-border-color{border-color:var(--wp--preset--color--cyan-bluish-gray) !important}.has-white-border-color{border-color:var(--wp--preset--color--white) !important}.has-pale-pink-border-color{border-color:var(--wp--preset--color--pale-pink) !important}.has-vivid-red-border-color{border-color:var(--wp--preset--color--vivid-red) !important}.has-luminous-vivid-orange-border-color{border-color:var(--wp--preset--color--luminous-vivid-orange) !important}.has-luminous-vivid-amber-border-color{border-color:var(--wp--preset--color--luminous-vivid-amber) !important}.has-light-green-cyan-border-color{border-color:var(--wp--preset--color--light-green-cyan) !important}.has-vivid-green-cyan-border-color{border-color:var(--wp--preset--color--vivid-green-cyan) !important}.has-pale-cyan-blue-border-color{border-color:var(--wp--preset--color--pale-cyan-blue) !important}.has-vivid-cyan-blue-border-color{border-color:var(--wp--preset--color--vivid-cyan-blue) !important}.has-vivid-purple-border-color{border-color:var(--wp--preset--color--vivid-purple) !important}.has-vivid-cyan-blue-to-vivid-purple-gradient-background{background:var(--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple) !important}.has-light-green-cyan-to-vivid-green-cyan-gradient-background{background:var(--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan) !important}.has-luminous-vivid-amber-to-luminous-vivid-orange-gradient-background{background:var(--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange) !important}.has-luminous-vivid-orange-to-vivid-red-gradient-background{background:var(--wp--preset--gradient--luminous-vivid-orange-to-vivid-red) !important}.has-very-light-gray-to-cyan-bluish-gray-gradient-background{background:var(--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray) !important}.has-cool-to-warm-spectrum-gradient-background{background:var(--wp--preset--gradient--cool-to-warm-spectrum) !important}.has-blush-light-purple-gradient-background{background:var(--wp--preset--gradient--blush-light-purple) !important}.has-blush-bordeaux-gradient-background{background:var(--wp--preset--gradient--blush-bordeaux) !important}.has-luminous-dusk-gradient-background{background:var(--wp--preset--gradient--luminous-dusk) !important}.has-pale-ocean-gradient-background{background:var(--wp--preset--gradient--pale-ocean) !important}.has-electric-grass-gradient-background{background:var(--wp--preset--gradient--electric-grass) !important}.has-midnight-gradient-background{background:var(--wp--preset--gradient--midnight) !important}.has-small-font-size{font-size:var(--wp--preset--font-size--small) !important}.has-medium-font-size{font-size:var(--wp--preset--font-size--medium) !important}.has-large-font-size{font-size:var(--wp--preset--font-size--large) !important}.has-x-large-font-size{font-size:var(--wp--preset--font-size--x-large) !important}.wp-block-navigation a:where(:not(.wp-element-button)){color:inherit}:where(.wp-block-columns.is-layout-flex){gap:2em}.wp-block-pullquote{font-size:1.5em;line-height:1.6}/*]]&gt;*/\\n  &lt;/style&gt;\\n  &lt;link href=\"https://dailytrust.com/wp-content/cache/minify/9e8c0.css\" media=\"all\" rel=\"stylesheet\"/&gt;\\n  &lt;script id=\"jetpack_related-posts-js-extra\"&gt;\\n   var related_posts_js_options={\"post_heading\":\"h4\"};\\n  &lt;/script&gt;\\n  &lt;script src=\"https://dailytrust.com/wp-content/cache/minify/8efe6.js\"&gt;\\n  &lt;/script&gt;\\n  &lt;script async=\"\" id=\"google_gtagjs-js\" src=\"https://www.googletagmanager.com/gtag/js?id=G-PH182QF6V1\"&gt;\\n  &lt;/script&gt;\\n  &lt;script id=\"google_gtagjs-js-after\"&gt;\\n   window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}\\ngtag(\\'set\\',\\'linker\\',{\"domains\":[\"dailytrust.com\"]});gtag(\"js\",new Date());gtag(\"set\",\"developer_id.dZTNiMT\",true);gtag(\"config\",\"G-PH182QF6V1\");\\n  &lt;/script&gt;\\n  &lt;link href=\"https://dailytrust.com/wp-json/\" rel=\"https://api.w.org/\"/&gt;\\n  &lt;link href=\"https://dailytrust.com/wp-json/wp/v2/posts/1208528\" rel=\"alternate\" type=\"application/json\"/&gt;\\n  &lt;link href=\"https://dailytrust.com/last-saturday-polls-raised-issues-that-require-immediate-solutions-inec/\" rel=\"canonical\"/&gt;\\n  &lt;link href=\"https://wp.me/pepE4n-54oo\" rel=\"shortlink\"/&gt;\\n  &lt;link href=\"https://dailytrust.com/wp-json/oembed/1.0/embed?url=https%3A%2F%2Fdailytrust.com%2Flast-saturday-polls-raised-issues-that-require-immediate-solutions-inec%2F\" rel=\"alternate\" type=\"application/json+oembed\"/&gt;\\n  &lt;link href=\"https://dailytrust.com/wp-json/oembed/1.0/embed?url=https%3A%2F%2Fdailytrust.com%2Flast-saturday-polls-raised-issues-that-require-immediate-solutions-inec%2F&amp;format=xml\" rel=\"alternate\" type=\"text/xml+oembed\"/&gt;\\n  &lt;meta content=\"Site Kit by Google 1.105.0\" name=\"generator\"/&gt;\\n  &lt;meta content=\"yrq0u56l49i44ggzu9lnvhpt3n85ye\" name=\"facebook-domain-verification\"/&gt;\\n  &lt;script async=\"\" src=\"https://www.googletagmanager.com/gtag/js?id=AW-11061949242\"&gt;\\n  &lt;/script&gt;\\n  &lt;script&gt;\\n   window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}gtag(\\'js\\',new Date());gtag(\\'config\\',\\'AW-11061949242\\');\\n  &lt;/script&gt;\\n  &lt;script async=\"\" id=\"ebx\" src=\"//applets.ebxcdn.com/ebx.js\"&gt;\\n  &lt;/script&gt;\\n  &lt;script&gt;\\n   gtag(\\'event\\',\\'conversion\\',{\\'send_to\\':\\'AW-11061949242/XkLgCMrVzYcYELrm35op\\'});\\n  &lt;/script&gt;\\n  &lt;script&gt;\\n   !function(f,b,e,v,n,t,s)\\n{if(f.fbq)return;n=f.fbq=function(){n.callMethod?n.callMethod.apply(n,arguments):n.queue.push(arguments)};if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version=\\'2.0\\';n.queue=[];t=b.createElement(e);t.async=!0;t.src=v;s=b.getElementsByTagName(e)[0];s.parentNode.insertBefore(t,s)}(window,document,\\'script\\',\\'https://connect.facebook.net/en_US/fbevents.js\\');fbq(\\'init\\',\\'1226941774766934\\');fbq(\\'track\\',\\'PageView\\');\\n  &lt;/script&gt;\\n  &lt;noscript&gt;\\n   &lt;img class=\"lazy\" data-src=\"https://www.facebook.com/tr?id=1226941774766934&amp;ev=PageView&amp;noscript=1\" height=\"1\" src=\"data:image/svg+xml,%3Csvg%20xmlns=\\'http://www.w3.org/2000/svg\\'%20viewBox=\\'0%200%201%201\\'%3E%3C/svg%3E\" style=\"display:none\" width=\"1\"/&gt;\\n  &lt;/noscript&gt;\\n  &lt;script&gt;\\n   fbq(\\'track\\',\\'InitiateCheckout\\',{value:1200,});\\n  &lt;/script&gt;\\n  &lt;script&gt;\\n   fbq(\\'track\\',\\'ViewContent\\');\\n  &lt;/script&gt;\\n  &lt;meta content=\"yrq0u56l49i44ggzu9lnvhpt3n85ye\" name=\"facebook-domain-verification\"/&gt;\\n  &lt;style&gt;\\n   img#wpstats{display:none}\\n  &lt;/style&gt;\\n  &lt;meta content=\"ca-host-pub-2644536267352236\" name=\"google-adsense-platform-account\"/&gt;\\n  &lt;meta content=\"sitekit.withgoogle.com\" name=\"google-adsense-platform-domain\"/&gt;\\n  &lt;meta content=\"The Independent National Electoral Commission (INEC) has admitted that the last Saturday presidential and National Assembly elections raises a number of issues that require immediate, medium and long-term solutions. INEC Chairman, Prof. Mahmood Yakubu, said this on Saturday in Abuja at a meeting with the Resident Electoral commissioners (RECs) held at the INEC headquarters, Abuja.…\" name=\"description\"/&gt;\\n  &lt;ins class=\"adsbymahimeta\" data-size=\"Native_Widget\" id=\"mMTag_NativeWidget_48482158\" style=\"display:inline-block;\"&gt;\\n   &lt;script&gt;\\n    var cachebuster=Math.round(new Date().getTime()/1000);var mMTagScript=document.createElement(\\'script\\');mMTagScript.src=\\'//mahimeta.com/networks/tag.js?cache=\\'+cachebuster;document.getElementsByTagName(\"head\")[0].appendChild(mMTagScript);\\n   &lt;/script&gt;\\n  &lt;/ins&gt;\\n  &lt;style&gt;\\n   .dt-grid{display:flex;align-content:center;justify-content:space-around;margin:0;padding:0;width:100%}dt-row{width:100%}.col-1{width:8.33333333333333%}.col-2{width:16.6666666666667%}.col-3{width:25%}.col-4{width:33.3333333333333%}.col-5{width:41.6666666666667%}.col-6{width:50%}.col-7{width:58.3333333333333%}.col-8{width:66.6666666666667%}.col-9{width:75%}.col-10{width:83.3333333333333%}.col-11{width:91.6666666666667%}.col-12{width:100%}.hidden-desktop{display:none}.hidden-mobile{display:block !important}@media screen and (max-width:428px){.dt-grid{flex-direction:column}.col-1.col-sm-1,.col-2.col-sm-1,.col-3.col-sm-1,.col-4.col-sm-1,.col-5.col-sm-1,.col-6.col-sm-1,.col-7.col-sm-1,.col-8.col-sm-1,.col-9.col-sm-1,.col-10.col-sm-1,.col-11.col-sm-1,.col-12.col-sm-1,.col-sm-1{width:8.33333333333333% !important}.col-sm-2{width:16.6666666666667%}.col-sm-3{width:25%}.col-sm-4{width:33.3333333333333%}.col-sm-5{width:41.6666666666667%}.col-sm-6{width:50%}.col-sm-7{width:58.3333333333333%}.col-sm-8{width:66.6666666666667%}.col-sm-9{width:75%}.col-sm-10{width:83.3333333333333%}.col-1.col-sm-11,.col-2.col-sm-11,.col-3.col-sm-11,.col-4.col-sm-11,.col-5.col-sm-11,.col-6.col-sm-11,.col-7.col-sm-11,.col-8.col-sm-11,.col-9.col-sm-11,.col-10.col-sm-11,.col-11.col-sm-11,.col-12.col-sm-11,.col-sm-11{width:91.6666666666667%}.col-1.col-sm-12,.col-2.col-sm-12,.col-3.col-sm-12,.col-4.col-sm-12,.col-5.col-sm-12,.col-6.col-sm-12,.col-7.col-sm-12,.col-8.col-sm-12,.col-9.col-sm-12,.col-10.col-sm-12,.col-11.col-sm-12,.col-12.col-sm-12,.col-sm-12{width:100% !important}.hidden-mobile{display:none !important}.hidden-desktop{display:block}}.dt-election-pc-iframe{margin:0;padding:0;border:0;width:100%;height:140px;overflow-x:hidden;overflow-y:hidden;overflow-wrap:break-word}dt-text-center{text-align:center}.gbg{margin:1em\\nauto;display:block;-webkit-appearance:none;border:6px\\nsolid rgba(255, 255, 255, 0.45);border-radius:50px;padding:1em\\n3em;background-repeat:no-repeat;background-size:100%;background-clip:padding-box;position:relative;color:#FFF;text-decoration:none;font-weight:bold}.gbg:before{content:\"\";width:100%;height:100%;position:absolute;top:3px;left:3px;border-radius:50px;z-index:-1;filter:blur(14px)}.gbg:after{content:\"\";width:100%;height:100%;position:absolute;top:3px;right:3px;border-radius:50px;z-index:-1;filter:blur(14px)}.gbg1{background-image:linear-gradient(90deg, #7b81ec, #3bd1d3)}.gbg1:before{background-image:linear-gradient(90deg, #7b81ec, transparent)}.gbg1:after{background-image:linear-gradient(90deg, transparent, #3bd1d3)}.gbg2{background-image:linear-gradient(90deg, #fa80d2, #fed757)}.gbg2:before{background-image:linear-gradient(90deg, #fa80d2, transparent)}.gbg2:after{background-image:linear-gradient(90deg, transparent, #fed757)}.gbg3{background-image:linear-gradient(90deg, #18e37a, #06c12b)}.gbg3:before{background-image:linear-gradient(90deg, #95f4aa, transparent)}.gbg3:after{background-image:linear-gradient(90deg, transparent, #79f391)}.gbg4{background-image:linear-gradient(90deg, #525a69, #898dda)}.gbg4:before{background-image:linear-gradient(90deg, #525a69, transparent)}.gbg4:after{background-image:linear-gradient(90deg, transparent, #898dda)}\\n  &lt;/style&gt;\\n  &lt;script async=\"\" crossorigin=\"anonymous\" src=\"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7943548946436796&amp;host=ca-host-pub-2644536267352236\"&gt;\\n  &lt;/script&gt;\\n  &lt;meta content=\"article\" property=\"og:type\"/&gt;\\n  &lt;meta content=\"Last Saturday polls raised issues that require immediate solutions – INEC - Daily Trust\" property=\"og:title\"/&gt;\\n  &lt;meta content=\"https://dailytrust.com/last-saturday-polls-raised-issues-that-require-immediate-solutions-inec/\" property=\"og:url\"/&gt;\\n  &lt;meta content=\"The Independent National Electoral Commission (INEC) has admitted that the last Saturday presidential and National Assembly elections raises a number of issues that require immediate, medium and lo…\" property=\"og:description\"/&gt;\\n  &lt;meta content=\"2023-03-04T13:43:04+00:00\" property=\"article:published_time\"/&gt;\\n  &lt;meta content=\"2023-03-04T13:43:04+00:00\" property=\"article:modified_time\"/&gt;\\n  &lt;meta content=\"Daily Trust\" property=\"og:site_name\"/&gt;\\n  &lt;meta content=\"https://dailytrust.com/wp-content/uploads/2021/05/INEC-Chairman.jpg\" property=\"og:image\"/&gt;\\n  &lt;meta content=\"791\" property=\"og:image:width\"/&gt;\\n  &lt;meta content=\"445\" property=\"og:image:height\"/&gt;\\n  &lt;meta content=\"INEC Chairman, INEC Chairman, Prof. Mahmood Yakubu\" property=\"og:image:alt\"/&gt;\\n  &lt;meta content=\"en_US\" property=\"og:locale\"/&gt;\\n  &lt;meta content=\"@daily_trust\" name=\"twitter:site\"/&gt;\\n  &lt;meta content=\"Last Saturday polls raised issues that require immediate solutions – INEC\" name=\"twitter:text:title\"/&gt;\\n  &lt;meta content=\"https://dailytrust.com/wp-content/uploads/2021/05/INEC-Chairman.jpg?w=640\" name=\"twitter:image\"/&gt;\\n  &lt;meta content=\"INEC Chairman, INEC Chairman, Prof. Mahmood Yakubu\" name=\"twitter:image:alt\"/&gt;\\n  &lt;meta content=\"summary_large_image\" name=\"twitter:card\"/&gt;\\n  &lt;link href=\"https://dailytrust.com/wp-content/uploads/2022/06/cropped-Daily-Trust-Icon-03-32x32.png\" rel=\"icon\" sizes=\"32x32\"/&gt;\\n  &lt;link href=\"https://dailytrust.com/wp-content/uploads/2022/06/cropped-Daily-Trust-Icon-03-192x192.png\" rel=\"icon\" sizes=\"192x192\"/&gt;\\n  &lt;link href=\"https://dailytrust.com/wp-content/uploads/2022/06/cropped-Daily-Trust-Icon-03-180x180.png\" rel=\"apple-touch-icon\"/&gt;\\n  &lt;meta content=\"https://dailytrust.com/wp-content/uploads/2022/06/cropped-Daily-Trust-Icon-03-270x270.png\" name=\"msapplication-TileImage\"/&gt;\\n  &lt;script async=\"\" src=\"https://www.googletagmanager.com/gtag/js?id=UA-45673018-1\"&gt;\\n  &lt;/script&gt;\\n  &lt;script&gt;\\n   window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}\\ngtag(\\'js\\',new Date());gtag(\\'config\\',\\'UA-45673018-1\\');\\n  &lt;/script&gt;\\n  &lt;script async=\"\" src=\"https://www.googletagmanager.com/gtag/js?id=G-PH182QF6V1\"&gt;\\n  &lt;/script&gt;\\n  &lt;script&gt;\\n   window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}\\ngtag(\\'js\\',new Date());gtag(\\'config\\',\\'G-PH182QF6V1\\');\\n  &lt;/script&gt;\\n  &lt;script async=\"\" defer=\"\" src=\"https://securepubads.g.doubleclick.net/tag/js/gpt.js\"&gt;\\n  &lt;/script&gt;\\n  &lt;script&gt;\\n   window.googletag=window.googletag||{cmd:[]};googletag.cmd.push(function(){googletag.defineSlot(\\'/113471580/dailytrust.com_masthead_468_60\\',[[728,90],[468,60]],\\'div-gpt-ad-1668623512083-0\\').addService(googletag.pubads());googletag.pubads().enableSingleRequest();googletag.enableServices();});googletag.cmd.push(function(){googletag.defineSlot(\\'/113471580/dailytrust_home_hero_left\\',[[250,250],[300,250]],\\'div-gpt-ad-1668624668292-0\\').addService(googletag.pubads());googletag.pubads().enableSingleRequest();googletag.enableServices();});window.googletag=window.googletag||{cmd:[]};googletag.cmd.push(function(){googletag.defineSlot(\\'/113471580/dailytrust_home_hero_right\\',[[250,250],[300,250]],\\'div-gpt-ad-1668624841493-0\\').addService(googletag.pubads());googletag.pubads().enableSingleRequest();googletag.enableServices();});window.googletag=window.googletag||{cmd:[]};googletag.cmd.push(function(){googletag.defineSlot(\\'/113471580/dailytrust_homepage_leaderboard1\\',[[728,90],\\'fluid\\',[468,60]],\\'div-gpt-ad-1668625663864-0\\').addService(googletag.pubads());googletag.pubads().enableSingleRequest();googletag.enableServices();});googletag.cmd.push(function(){googletag.defineSlot(\\'/113471580/dailytrust_homepage_leaderboard2\\',[\\'fluid\\',[468,60],[728,90]],\\'div-gpt-ad-1668626014062-0\\').addService(googletag.pubads());googletag.pubads().enableSingleRequest();googletag.enableServices();});window.googletag=window.googletag||{cmd:[]};googletag.cmd.push(function(){googletag.defineSlot(\\'/113471580/dailytrust_homepage_leaderboard3\\',[[728,90],[468,60],\\'fluid\\'],\\'div-gpt-ad-1668626123585-0\\').addService(googletag.pubads());googletag.pubads().enableSingleRequest();googletag.enableServices();});window.googletag=window.googletag||{cmd:[]};googletag.cmd.push(function(){googletag.defineSlot(\\'/113471580/dailytrust.com_article_right_sidebar_top\\',[[300,250],\\'fluid\\'],\\'div-gpt-ad-1668847848292-0\\').addService(googletag.pubads());googletag.pubads().enableSingleRequest();googletag.enableServices();});window.googletag=window.googletag||{cmd:[]};googletag.cmd.push(function(){googletag.defineSlot(\\'/113471580/dailytrust.com_article_right_sidebar_bottom\\',[[250,250],[300,250],\\'fluid\\'],\\'div-gpt-ad-1668848350673-0\\').addService(googletag.pubads());googletag.pubads().enableSingleRequest();googletag.enableServices();});window.googletag=window.googletag||{cmd:[]};googletag.cmd.push(function(){googletag.defineSlot(\\'/113471580/dailytrust.com_300_600_article_right\\',[\\'fluid\\',[300,600]],\\'div-gpt-ad-1668848575614-0\\').addService(googletag.pubads());googletag.pubads().enableSingleRequest();googletag.enableServices();});\\n  &lt;/script&gt;\\n &lt;/head&gt;\\n &lt;body&gt;\\n  &lt;svg focusable=\"false\" height=\"0\" role=\"none\" style=\"visibility: hidden; position: absolute; left: -9999px; overflow: hidden;\" viewbox=\"0 0 0 0\" width=\"0\" xmlns=\"http://www.w3.org/2000/svg\"&gt;\\n   &lt;defs&gt;\\n    &lt;filter id=\"wp-duotone-dark-grayscale\"&gt;\\n     &lt;fecolormatrix color-interpolation-filters=\"sRGB\" type=\"matrix\" values=\" .299 .587 .114 0 0 .299 .587 .114 0 0 .299 .587 .114 0 0 .299 .587 .114 0 0 \"&gt;\\n     &lt;/fecolormatrix&gt;\\n     &lt;fecomponenttransfer color-interpolation-filters=\"sRGB\"&gt;\\n      &lt;fefuncr tablevalues=\"0 0.49803921568627\" type=\"table\"&gt;\\n      &lt;/fefuncr&gt;\\n      &lt;fefuncg tablevalues=\"0 0.49803921568627\" type=\"table\"&gt;\\n      &lt;/fefuncg&gt;\\n      &lt;fefuncb tablevalues=\"0 0.49803921568627\" type=\"table\"&gt;\\n      &lt;/fefuncb&gt;\\n      &lt;fefunca tablevalues=\"1 1\" type=\"table\"&gt;\\n      &lt;/fefunca&gt;\\n     &lt;/fecomponenttransfer&gt;\\n     &lt;fecomposite in2=\"SourceGraphic\" operator=\"in\"&gt;\\n     &lt;/fecomposite&gt;\\n    &lt;/filter&gt;\\n   &lt;/defs&gt;\\n  &lt;/svg&gt;\\n  &lt;svg focusable=\"false\" height=\"0\" role=\"none\" style=\"visibility: hidden; position: absolute; left: -9999px; overflow: hidden;\" viewbox=\"0 0 0 0\" width=\"0\" xmlns=\"http://www.w3.org/2000/svg\"&gt;\\n   &lt;defs&gt;\\n    &lt;filter id=\"wp-duotone-grayscale\"&gt;\\n     &lt;fecolormatrix color-interpolation-filters=\"sRGB\" type=\"matrix\" values=\" .299 .587 .114 0 0 .299 .587 .114 0 0 .299 .587 .114 0 0 .299 .587 .114 0 0 \"&gt;\\n     &lt;/fecolormatrix&gt;\\n     &lt;fecomponenttransfer color-interpolation-filters=\"sRGB\"&gt;\\n      &lt;fefuncr tablevalues=\"0 1\" type=\"table\"&gt;\\n      &lt;/fefuncr&gt;\\n      &lt;fefuncg tablevalues=\"0 1\" type=\"table\"&gt;\\n      &lt;/fefuncg&gt;\\n      &lt;fefuncb tablevalues=\"0 1\" type=\"table\"&gt;\\n      &lt;/fefuncb&gt;\\n      &lt;fefunca tablevalues=\"1 1\" type=\"table\"&gt;\\n      &lt;/fefunca&gt;\\n     &lt;/fecomponenttransfer&gt;\\n     &lt;fecomposite in2=\"SourceGraphic\" operator=\"in\"&gt;\\n     &lt;/fecomposite&gt;\\n    &lt;/filter&gt;\\n   &lt;/defs&gt;\\n  &lt;/svg&gt;\\n  &lt;svg focusable=\"false\" height=\"0\" role=\"none\" style=\"visibility: hidden; position: absolute; left: -9999px; overflow: hidden;\" viewbox=\"0 0 0 0\" width=\"0\" xmlns=\"http://www.w3.org/2000/svg\"&gt;\\n   &lt;defs&gt;\\n    &lt;filter id=\"wp-duotone-purple-yellow\"&gt;\\n     &lt;fecolormatrix color-interpolation-filters=\"sRGB\" type=\"matrix\" values=\" .299 .587 .114 0 0 .299 .587 .114 0 0 .299 .587 .114 0 0 .299 .587 .114 0 0 \"&gt;\\n     &lt;/fecolormatrix&gt;\\n     &lt;fecomponenttransfer color-interpolation-filters=\"sRGB\"&gt;\\n      &lt;fefuncr tablevalues=\"0.54901960784314 0.98823529411765\" type=\"table\"&gt;\\n      &lt;/fefuncr&gt;\\n      &lt;fefuncg tablevalues=\"0 1\" type=\"table\"&gt;\\n      &lt;/fefuncg&gt;\\n      &lt;fefuncb tablevalues=\"0.71764705882353 0.25490196078431\" type=\"table\"&gt;\\n      &lt;/fefuncb&gt;\\n      &lt;fefunca tablevalues=\"1 1\" type=\"table\"&gt;\\n      &lt;/fefunca&gt;\\n     &lt;/fecomponenttransfer&gt;\\n     &lt;fecomposite in2=\"SourceGraphic\" operator=\"in\"&gt;\\n     &lt;/fecomposite&gt;\\n    &lt;/filter&gt;\\n   &lt;/defs&gt;\\n  &lt;/svg&gt;\\n  &lt;svg focusable=\"false\" height=\"0\" role=\"none\" style=\"visibility: hidden; position: absolute; left: -9999px; overflow: hidden;\" viewbox=\"0 0 0 0\" width=\"0\" xmlns=\"http://www.w3.org/2000/svg\"&gt;\\n   &lt;defs&gt;\\n    &lt;filter id=\"wp-duotone-blue-red\"&gt;\\n     &lt;fecolormatrix color-interpolation-filters=\"sRGB\" type=\"matrix\" values=\" .299 .587 .114 0 0 .299 .587 .114 0 0 .299 .587 .114 0 0 .299 .587 .114 0 0 \"&gt;\\n     &lt;/fecolormatrix&gt;\\n     &lt;fecomponenttransfer color-interpolation-filters=\"sRGB\"&gt;\\n      &lt;fefuncr tablevalues=\"0 1\" type=\"table\"&gt;\\n      &lt;/fefuncr&gt;\\n      &lt;fefuncg tablevalues=\"0 0.27843137254902\" type=\"table\"&gt;\\n      &lt;/fefuncg&gt;\\n      &lt;fefuncb tablevalues=\"0.5921568627451 0.27843137254902\" type=\"table\"&gt;\\n      &lt;/fefuncb&gt;\\n      &lt;fefunca tablevalues=\"1 1\" type=\"table\"&gt;\\n      &lt;/fefunca&gt;\\n     &lt;/fecomponenttransfer&gt;\\n     &lt;fecomposite in2=\"SourceGraphic\" operator=\"in\"&gt;\\n     &lt;/fecomposite&gt;\\n    &lt;/filter&gt;\\n   &lt;/defs&gt;\\n  &lt;/svg&gt;\\n  &lt;svg focusable=\"false\" height=\"0\" role=\"none\" style=\"visibility: hidden; position: absolute; left: -9999px; overflow: hidden;\" viewbox=\"0 0 0 0\" width=\"0\" xmlns=\"http://www.w3.org/2000/svg\"&gt;\\n   &lt;defs&gt;\\n    &lt;filter id=\"wp-duotone-midnight\"&gt;\\n     &lt;fecolormatrix color-interpolation-filters=\"sRGB\" type=\"matrix\" values=\" .299 .587 .114 0 0 .299 .587 .114 0 0 .299 .587 .114 0 0 .299 .587 .114 0 0 \"&gt;\\n     &lt;/fecolormatrix&gt;\\n     &lt;fecomponenttransfer color-interpolation-filters=\"sRGB\"&gt;\\n      &lt;fefuncr tablevalues=\"0 0\" type=\"table\"&gt;\\n      &lt;/fefuncr&gt;\\n      &lt;fefuncg tablevalues=\"0 0.64705882352941\" type=\"table\"&gt;\\n      &lt;/fefuncg&gt;\\n      &lt;fefuncb tablevalues=\"0 1\" type=\"table\"&gt;\\n      &lt;/fefuncb&gt;\\n      &lt;fefunca tablevalues=\"1 1\" type=\"table\"&gt;\\n      &lt;/fefunca&gt;\\n     &lt;/fecomponenttransfer&gt;\\n     &lt;fecomposite in2=\"SourceGraphic\" operator=\"in\"&gt;\\n     &lt;/fecomposite&gt;\\n    &lt;/filter&gt;\\n   &lt;/defs&gt;\\n  &lt;/svg&gt;\\n  &lt;svg focusable=\"false\" height=\"0\" role=\"none\" style=\"visibility: hidden; position: absolute; left: -9999px; overflow: hidden;\" viewbox=\"0 0 0 0\" width=\"0\" xmlns=\"http://www.w3.org/2000/svg\"&gt;\\n   &lt;defs&gt;\\n    &lt;filter id=\"wp-duotone-magenta-yellow\"&gt;\\n     &lt;fecolormatrix color-interpolation-filters=\"sRGB\" type=\"matrix\" values=\" .299 .587 .114 0 0 .299 .587 .114 0 0 .299 .587 .114 0 0 .299 .587 .114 0 0 \"&gt;\\n     &lt;/fecolormatrix&gt;\\n     &lt;fecomponenttransfer color-interpolation-filters=\"sRGB\"&gt;\\n      &lt;fefuncr tablevalues=\"0.78039215686275 1\" type=\"table\"&gt;\\n      &lt;/fefuncr&gt;\\n      &lt;fefuncg tablevalues=\"0 0.94901960784314\" type=\"table\"&gt;\\n      &lt;/fefuncg&gt;\\n      &lt;fefuncb tablevalues=\"0.35294117647059 0.47058823529412\" type=\"table\"&gt;\\n      &lt;/fefuncb&gt;\\n      &lt;fefunca tablevalues=\"1 1\" type=\"table\"&gt;\\n      &lt;/fefunca&gt;\\n     &lt;/fecomponenttransfer&gt;\\n     &lt;fecomposite in2=\"SourceGraphic\" operator=\"in\"&gt;\\n     &lt;/fecomposite&gt;\\n    &lt;/filter&gt;\\n   &lt;/defs&gt;\\n  &lt;/svg&gt;\\n  &lt;svg focusable=\"false\" height=\"0\" role=\"none\" style=\"visibility: hidden; position: absolute; left: -9999px; overflow: hidden;\" viewbox=\"0 0 0 0\" width=\"0\" xmlns=\"http://www.w3.org/2000/svg\"&gt;\\n   &lt;defs&gt;\\n    &lt;filter id=\"wp-duotone-purple-green\"&gt;\\n     &lt;fecolormatrix color-interpolation-filters=\"sRGB\" type=\"matrix\" values=\" .299 .587 .114 0 0 .299 .587 .114 0 0 .299 .587 .114 0 0 .299 .587 .114 0 0 \"&gt;\\n     &lt;/fecolormatrix&gt;\\n     &lt;fecomponenttransfer color-interpolation-filters=\"sRGB\"&gt;\\n      &lt;fefuncr tablevalues=\"0.65098039215686 0.40392156862745\" type=\"table\"&gt;\\n      &lt;/fefuncr&gt;\\n      &lt;fefuncg tablevalues=\"0 1\" type=\"table\"&gt;\\n      &lt;/fefuncg&gt;\\n      &lt;fefuncb tablevalues=\"0.44705882352941 0.4\" type=\"table\"&gt;\\n      &lt;/fefuncb&gt;\\n      &lt;fefunca tablevalues=\"1 1\" type=\"table\"&gt;\\n      &lt;/fefunca&gt;\\n     &lt;/fecomponenttransfer&gt;\\n     &lt;fecomposite in2=\"SourceGraphic\" operator=\"in\"&gt;\\n     &lt;/fecomposite&gt;\\n    &lt;/filter&gt;\\n   &lt;/defs&gt;\\n  &lt;/svg&gt;\\n  &lt;svg focusable=\"false\" height=\"0\" role=\"none\" style=\"visibility: hidden; position: absolute; left: -9999px; overflow: hidden;\" viewbox=\"0 0 0 0\" width=\"0\" xmlns=\"http://www.w3.org/2000/svg\"&gt;\\n   &lt;defs&gt;\\n    &lt;filter id=\"wp-duotone-blue-orange\"&gt;\\n     &lt;fecolormatrix color-interpolation-filters=\"sRGB\" type=\"matrix\" values=\" .299 .587 .114 0 0 .299 .587 .114 0 0 .299 .587 .114 0 0 .299 .587 .114 0 0 \"&gt;\\n     &lt;/fecolormatrix&gt;\\n     &lt;fecomponenttransfer color-interpolation-filters=\"sRGB\"&gt;\\n      &lt;fefuncr tablevalues=\"0.098039215686275 1\" type=\"table\"&gt;\\n      &lt;/fefuncr&gt;\\n      &lt;fefuncg tablevalues=\"0 0.66274509803922\" type=\"table\"&gt;\\n      &lt;/fefuncg&gt;\\n      &lt;fefuncb tablevalues=\"0.84705882352941 0.41960784313725\" type=\"table\"&gt;\\n      &lt;/fefuncb&gt;\\n      &lt;fefunca tablevalues=\"1 1\" type=\"table\"&gt;\\n      &lt;/fefunca&gt;\\n     &lt;/fecomponenttransfer&gt;\\n     &lt;fecomposite in2=\"SourceGraphic\" operator=\"in\"&gt;\\n     &lt;/fecomposite&gt;\\n    &lt;/filter&gt;\\n   &lt;/defs&gt;\\n  &lt;/svg&gt;\\n  &lt;div class=\"sidenav\" id=\"mySidenav\"&gt;\\n   &lt;a class=\"menu_menu_item__1uvRI\"&gt;\\n    &lt;span onclick=\"closeNav()\" style=\"color:#fff;cursor:pointer; font-weight:bold\"&gt;\\n     ✕ CLOSE\\n    &lt;/span&gt;\\n   &lt;/a&gt;\\n   &lt;a class=\"menu_menu_item__1uvRI\" href=\"/topics/online-special\"&gt;\\n    Online Special\\n   &lt;/a&gt;\\n   &lt;a class=\"menu_menu_item__1uvRI\" href=\"/topics/city-news\" id=\"1\"&gt;\\n    City News\\n   &lt;/a&gt;\\n   &lt;a class=\"menu_menu_item__1uvRI\" href=\"/topics/entrepreneurship\" id=\"3\"&gt;\\n    Entrepreneurship\\n   &lt;/a&gt;\\n   &lt;a class=\"menu_menu_item__1uvRI\" href=\"/topics/environment\" id=\"4\"&gt;\\n    Environment\\n   &lt;/a&gt;\\n   &lt;a class=\"menu_menu_item__1uvRI\" href=\"/topics/factcheck\" id=\"5\"&gt;\\n    Factcheck\\n   &lt;/a&gt;\\n   &lt;a class=\"menu_menu_item__1uvRI\" href=\"/topics/everything-woman\" id=\"6\"&gt;\\n    Everything Woman\\n   &lt;/a&gt;\\n   &lt;a class=\"menu_menu_item__1uvRI\" href=\"/topics/home-front\" id=\"7\"&gt;\\n    Home Front\\n   &lt;/a&gt;\\n   &lt;a class=\"menu_menu_item__1uvRI\" href=\"/topics/islamic-forum\" id=\"8\"&gt;\\n    Islamic Forum\\n   &lt;/a&gt;\\n   &lt;a class=\"menu_menu_item__1uvRI\" href=\"/topics/life-xtra\" id=\"9\"&gt;\\n    Life Xtra\\n   &lt;/a&gt;\\n   &lt;a class=\"menu_menu_item__1uvRI\" href=\"/topics/property\" id=\"10\"&gt;\\n    Property\\n   &lt;/a&gt;\\n   &lt;a class=\"menu_menu_item__1uvRI\" href=\"/topics/travel-leisure\" id=\"11\"&gt;\\n    Travel &amp;amp; Leisure\\n   &lt;/a&gt;\\n   &lt;a class=\"menu_menu_item__1uvRI\" href=\"/topics/viewpoint\" id=\"12\"&gt;\\n    Viewpoint\\n   &lt;/a&gt;\\n   &lt;a class=\"menu_menu_item__1uvRI\" href=\"/topics/vox-pop\" id=\"13\"&gt;\\n    Vox Pop\\n   &lt;/a&gt;\\n   &lt;a class=\"menu_menu_item__1uvRI\" href=\"/topics/women-in-business\" id=\"14\"&gt;\\n    Women In Business\\n   &lt;/a&gt;\\n   &lt;a class=\"menu_menu_item__1uvRI\" href=\"/topics/art-and-ideas\" id=\"15\"&gt;\\n    Art and Ideas\\n   &lt;/a&gt;\\n   &lt;a class=\"menu_menu_item__1uvRI\" href=\"/topics/bookshelf\" id=\"16\"&gt;\\n    Bookshelf\\n   &lt;/a&gt;\\n   &lt;a class=\"menu_menu_item__1uvRI\" href=\"/topics/labour\" id=\"17\"&gt;\\n    Labour\\n   &lt;/a&gt;\\n   &lt;a class=\"menu_menu_item__1uvRI\" href=\"/topics/law\" id=\"18\"&gt;\\n    Law\\n   &lt;/a&gt;\\n   &lt;a class=\"menu_menu_item__1uvRI\" href=\"/topics/letters\" id=\"19\"&gt;\\n    Letters\\n   &lt;/a&gt;\\n  &lt;/div&gt;\\n  &lt;main id=\"main\" style=\"background: white; position: relative; z-index: 10; display: flex; flex-direction: column; min-height: 100vh;\"&gt;\\n   &lt;style&gt;\\n    @media screen and (max-width:620px){.top__menu__wrap{text-align:center}.top__menu__item{border-right:1px solid #c0c0c0;text-align:center;padding-right:5px;align-content:center;justify-content:space-between}.top__menu__item&gt;a{font-size:9px;text-align:center;color:navy}#blink-link-image{width:80px !important;height:35px !important}}@media screen and (max-width:350px){.top__menu__item{padding-right:1px}.top__menu__item&gt;a{font-size:7px}#blink-link-image{width:60px !important;height:30px !important}}@keyframes\\nblink{0%{opacity:1}49%{opacity:1}50%{opacity:0}100%{opacity:0}}#blink-link-image{width:150px;height:80px;vertical-align:bottom;animation:blink 1s;animation-iteration-count:infinite}\\n   &lt;/style&gt;\\n   &lt;header class=\"header\" id=\"masthead\"&gt;\\n    &lt;section class=\"top__menu\"&gt;\\n     &lt;div class=\"container\"&gt;\\n      &lt;ul class=\"top__menu__wrap\"&gt;\\n       &lt;li class=\"top__menu__item\"&gt;\\n        &lt;a href=\"https://trustcheck.dailytrust.com/\" rel=\"noopener noreferrer\" target=\"_trustcheck\" title=\"Fact Check\"&gt;\\n         Fact Check\\n        &lt;/a&gt;\\n       &lt;/li&gt;\\n       &lt;li class=\"top__menu__item\"&gt;\\n        &lt;a href=\"https://dailytrust.com/change-of-name/\" rel=\"noopener noreferrer\" target=\"_changeofname\" title=\"Change Of Name\"&gt;\\n         Change Of Name\\n        &lt;/a&gt;\\n       &lt;/li&gt;\\n       &lt;li class=\"top__menu__item\"&gt;\\n        &lt;a href=\"https://membership.dailytrust.com/\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Membership\"&gt;\\n         Trust+\\n        &lt;/a&gt;\\n       &lt;/li&gt;\\n       &lt;li class=\"top__menu__item\"&gt;\\n        &lt;a href=\"https://aminiya.dailytrust.com/\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Aminiya\"&gt;\\n         Aminiya\\n        &lt;/a&gt;\\n       &lt;/li&gt;\\n       &lt;li class=\"top__menu__item\"&gt;\\n        &lt;a href=\"https://epaper.dailytrust.com/\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"e-Paper\"&gt;\\n         E-Paper\\n        &lt;/a&gt;\\n        &lt;li class=\"top__menu__item\"&gt;\\n         &lt;li class=\"top__menu__item\"&gt;\\n          &lt;a href=\"https://trusttv.com/\" rel=\"noopener noreferrer\" target=\"trusttv\" title=\"Trust-Tv\"&gt;\\n           TV-Live\\n          &lt;/a&gt;\\n         &lt;/li&gt;\\n        &lt;/li&gt;\\n       &lt;/li&gt;\\n      &lt;/ul&gt;\\n     &lt;/div&gt;\\n    &lt;/section&gt;\\n    &lt;div class=\"header__advert\"&gt;\\n     &lt;div class=\"ad_container\"&gt;\\n      &lt;div id=\"div-gpt-ad-1668623512083-0\" style=\"min-width: 468px; min-height: 60px;\"&gt;\\n       &lt;script&gt;\\n        googletag.cmd.push(function(){googletag.display(\\'div-gpt-ad-1668623512083-0\\');});\\n       &lt;/script&gt;\\n      &lt;/div&gt;\\n     &lt;/div&gt;\\n    &lt;/div&gt;\\n    &lt;section class=\"main\"&gt;\\n     &lt;div class=\"container inner\"&gt;\\n      &lt;div class=\"left\"&gt;\\n       &lt;a aria-label=\"Search dailytrust\" href=\"/search\" title=\"Search\"&gt;\\n        &lt;div class=\"search\"&gt;\\n         &lt;svg fill=\"#000000\" height=\"20px\" viewbox=\"0 0 50 50\" width=\"20px\" xmlns=\"http://www.w3.org/2000/svg\"&gt;\\n          &lt;path d=\"M 21 3 C 11.621094 3 4 10.621094 4 20 C 4 29.378906 11.621094 37 21 37 C 24.710938 37 28.140625 35.804688 30.9375 33.78125 L 44.09375 46.90625 L 46.90625 44.09375 L 33.90625 31.0625 C 36.460938 28.085938 38 24.222656 38 20 C 38 10.621094 30.378906 3 21 3 Z M 21 5 C 29.296875 5 36 11.703125 36 20 C 36 28.296875 29.296875 35 21 35 C 12.703125 35 6 28.296875 6 20 C 6 11.703125 12.703125 5 21 5 Z\"&gt;\\n          &lt;/path&gt;\\n         &lt;/svg&gt;\\n        &lt;/div&gt;\\n        &lt;span&gt;\\n         Search\\n        &lt;/span&gt;\\n       &lt;/a&gt;\\n      &lt;/div&gt;\\n      &lt;div class=\"logo\"&gt;\\n       &lt;a aria-label=\"Dailytrust\" href=\"/\" title=\"Dailytrust\"&gt;\\n        &lt;img alt=\"Dailytrust\" aria-label=\"Dailytrust\" class=\"lazy\" data-src=\"https://dailytrust.com/dailytrust.svg\" src=\"data:image/svg+xml,%3Csvg%20xmlns=\\'http://www.w3.org/2000/svg\\'%20viewBox=\\'0%200%201%201\\'%3E%3C/svg%3E\"/&gt;\\n       &lt;/a&gt;\\n      &lt;/div&gt;\\n      &lt;div class=\"right\"&gt;\\n       &lt;a class=\"subscribe\" href=\"https://epaper.dailytrust.com/\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"E-Paper Subscription\"&gt;\\n        Subscribe\\n       &lt;/a&gt;\\n      &lt;/div&gt;\\n     &lt;/div&gt;\\n    &lt;/section&gt;\\n    &lt;nav class=\"menu\" id=\"site-navigation\"&gt;\\n     &lt;ul class=\"container\"&gt;\\n      &lt;li&gt;\\n       &lt;a class=\"active\" href=\"/\"&gt;\\n        Home\\n       &lt;/a&gt;\\n      &lt;/li&gt;\\n      &lt;li&gt;\\n       &lt;a href=\"/topics/news\"&gt;\\n        News\\n       &lt;/a&gt;\\n      &lt;/li&gt;\\n      &lt;li&gt;\\n       &lt;a href=\"/topics/2023-elections\" title=\"Nigeria 2023 General Elections\"&gt;\\n        2023 Elections\\n       &lt;/a&gt;\\n      &lt;/li&gt;\\n      &lt;li&gt;\\n       &lt;a href=\"/topics/business\"&gt;\\n        Business\\n       &lt;/a&gt;\\n      &lt;/li&gt;\\n      &lt;li&gt;\\n       &lt;a href=\"/topics/politics\"&gt;\\n        Politics\\n       &lt;/a&gt;\\n      &lt;/li&gt;\\n      &lt;li&gt;\\n       &lt;a href=\"/topics/podcast\"&gt;\\n        Podcast\\n       &lt;/a&gt;\\n      &lt;/li&gt;\\n      &lt;li&gt;\\n       &lt;a href=\"/topics/videos\"&gt;\\n        Videos\\n       &lt;/a&gt;\\n      &lt;/li&gt;\\n      &lt;li&gt;\\n       &lt;a href=\"/topics/agriculture\"&gt;\\n        Agriculture\\n       &lt;/a&gt;\\n      &lt;/li&gt;\\n      &lt;li&gt;\\n       &lt;a href=\"/topics/sports\"&gt;\\n        Sports\\n       &lt;/a&gt;\\n      &lt;/li&gt;\\n      &lt;li&gt;\\n       &lt;a href=\"/topics/opinion\"&gt;\\n        Opinion\\n       &lt;/a&gt;\\n      &lt;/li&gt;\\n      &lt;li&gt;\\n       &lt;a href=\"/topics/education\"&gt;\\n        Education\\n       &lt;/a&gt;\\n      &lt;/li&gt;\\n      &lt;li&gt;\\n       &lt;a href=\"/topics/entertainment\"&gt;\\n        Entertainment\\n       &lt;/a&gt;\\n      &lt;/li&gt;\\n      &lt;li&gt;\\n       &lt;a href=\"/topics/international\"&gt;\\n        International\\n       &lt;/a&gt;\\n      &lt;/li&gt;\\n      &lt;li&gt;\\n       &lt;span onclick=\"openNav()\" style=\"color:#ffffff;cursor:pointer;font-size:25px\"&gt;\\n        ☰\\n       &lt;/span&gt;\\n      &lt;/li&gt;\\n     &lt;/ul&gt;\\n    &lt;/nav&gt;\\n   &lt;/header&gt;\\n   &lt;div class=\"mytuner-widget\" data-autoplay=\"false\" data-hidehistory=\"true\" data-requires_initialization=\"true\" data-target=\"494909\" id=\"wq/DusOzPWIxwrzDtHhjU3g7wrVDRzdHIsOybA==\" style=\"width: 100%; max-width: 100%; overflow: hidden; max-height: 500px; border: 1px solid rgb(249, 240, 107); border-radius: 6px;\"&gt;\\n    &lt;style&gt;\\n     .mytuner-widget{all:initial;display:block;color:#fff}.mytuner-widget, .mytuner-widget\\n*{box-sizing:border-box;font-family:sans-serif}.main-play-button{padding:5px;border-radius:20px;width:40px;height:40px;float:left;margin-left:10px;margin-right:15px;margin-top:12.5px;cursor:pointer;background-color:#FFF;box-shadow:1px 2px 6px -3px black;display:inline-block}.main-play-button:hover{background-color:#EEE}.main-play-button.disabled{filter:grayscale(1);cursor:not-allowed}.main-play-button\\ndiv{background:url(\"https://mytuner-radio.com/static/icons/widgets/BT_Play/BT_Play@2x.png\") no-repeat center;background-size:16px;width:28px;height:28px;margin-left:3px}.main-play-button.loading\\ndiv{background:url(\"https://static2.mytuner.mobi/static/images/sprite-loading.gif\") no-repeat center;filter:grayscale(1);background-size:28px;margin-left:2px}.main-play-button.playing\\ndiv{background:url(\"https://mytuner-radio.com/static/icons/widgets/BT_Pause/BT_Pause@2x.png\") no-repeat center;background-size:16px;margin-left:2px}.main-play-button.error\\ndiv{background:url(\"https://mytuner-radio.com/static/icons/widgets/BT_Error/erro@2x.png\") no-repeat center;background-size:16px;margin-left:0}.play-button{background:url(\"https://mytuner-radio.com/static/icons/widgets/BT_Play/BT_Play.png\") no-repeat center;width:40px;height:40px;cursor:pointer;display:inline-flex;align-items:center;margin:auto 4px auto 19px}.play-button.loading{background:url(\"https://static2.mytuner.mobi/static/images/sprite-loading.gif\") no-repeat center;filter:grayscale(1);background-size:28px}.play-button.playing{background:url(\"https://mytuner-radio.com/static/icons/widgets/BT_Pause/BT_Pause.png\") no-repeat center}.play-button.error{background:url(\"https://mytuner-radio.com/static/icons/widgets/BT_Error/erro.png\") no-repeat center;background-size:15px}.play-button.disabled{opacity:0.3}.play-button.disabled:hover{cursor:not-allowed}input[type=range][orient=vertical]{writing-mode:bt-lr;-webkit-appearance:slider-vertical;width:8px;padding:0\\n5px}.volume-controls{width:35px;height:35px;display:inline-block;position:absolute;margin-left:5px;margin-top:14px;padding-top:0;border-radius:20px;box-sizing:content-box !important;z-index:10;border:1px\\nsolid transparent;transition:background 0.5s, padding 0.5s, margin 0.5s, border 0.5s;overflow:hidden}.volume-controls:hover{padding-top:140px;margin-top:-126px;background-color:#F2F2F2;border:1px\\nsolid grey;transition:background 0.5s, padding 0.5s, margin 0.5s}.volume-controls:hover&gt;.volume-control{display:block}.volume-controls .volume-control{opacity:0;margin-top:-126px;margin-left:13px;position:absolute;transition:0.5s all}.volume-controls:hover .volume-control{opacity:1}.volume-controls .volume-indicator{cursor:pointer;display:block}.player-radio-link{width:calc(100% - 65px - 84px - 37px - 12px)}.player-radio-name{width:calc(100% - 74px - 10px)}.player-mytuner-logo{margin-left:47px}@media (max-width: 480px){.player-radio-link{width:calc(100% - 65px - 84px - 12px)}.player-mytuner-logo{margin-left:10px}.volume-controls{display:none}}\\n    &lt;/style&gt;\\n    &lt;div id=\"wq/DusOzPWIxwrzDtHhjU3g7wrVDRzdHIsOybA==top-bar\" style=\"background: rgb(153, 193, 241); height: 75px; width: 100%; display: block; padding: 5px; line-height: 65px;\"&gt;\\n     &lt;div class=\"main-play-button disabled\" data-id=\"wq/DusOzPWIxwrzDtHhjU3g7wrVDRzdHIsOybA==\" id=\"wq/DusOzPWIxwrzDtHhjU3g7wrVDRzdHIsOybA==play-button\"&gt;\\n      &lt;div class=\"play-image\"&gt;\\n      &lt;/div&gt;\\n     &lt;/div&gt;\\n     &lt;a class=\"player-radio-link\" href=\"http://mytuner-radio.com/radio/trust-radio-494909/?utm_source=widget&amp;utm_medium=player\" rel=\"noopener\" style=\"height: 100%; display: inline-block; line-height: 65px; cursor: pointer;\" target=\"_blank\"&gt;\\n      &lt;img alt=\"Trust Radio Live\" class=\"lazy\" data-src=\"https://static2.mytuner.mobi/media/tvos_radios/u24fKacN9K.png\" src=\"data:image/svg+xml,%3Csvg%20xmlns=\\'http://www.w3.org/2000/svg\\'%20viewBox=\\'0%200%201%201\\'%3E%3C/svg%3E\" style=\"float: left; height: 74px; margin-top: -5px; box-shadow: black 0px 0px 3px -1px;\"/&gt;\\n      &lt;span class=\"player-radio-name\" style=\"margin-left: 10px; color: rgb(255, 255, 255); font-weight: bold; font-size: 20px; float: left; overflow: hidden; text-overflow: ellipsis; white-space: nowrap;\"&gt;\\n       Trust Radio Live\\n      &lt;/span&gt;\\n     &lt;/a&gt;\\n     &lt;div class=\"volume-controls\"&gt;\\n      &lt;input class=\"volume-control slider\" id=\"wq/DusOzPWIxwrzDtHhjU3g7wrVDRzdHIsOybA==volume-control\" max=\"100\" min=\"1\" orient=\"vertical\" type=\"range\" value=\"100\"/&gt;\\n      &lt;svg class=\"volume-indicator\" height=\"30\" id=\"wq/DusOzPWIxwrzDtHhjU3g7wrVDRzdHIsOybA==volume-indicator\" style=\"fill: grey; margin: 2px;\" viewbox=\"0 0 24 24\" width=\"30\" xmlns=\"http://www.w3.org/2000/svg\"&gt;\\n       &lt;path d=\"M3 10v4c0 .55.45 1 1 1h3l3.29 3.29c.63.63 1.71.18 1.71-.71V6.41c0-.89-1.08-1.34-1.71-.71L7 9H4c-.55 0-1 .45-1 1zm13.5 2c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 4.45v.2c0 .38.25.71.6.85C17.18 6.53 19 9.06 19 12s-1.82 5.47-4.4 6.5c-.36.14-.6.47-.6.85v.2c0 .63.63 1.07 1.21.85C18.6 19.11 21 15.84 21 12s-2.4-7.11-5.79-8.4c-.58-.23-1.21.22-1.21.85z\"&gt;\\n       &lt;/path&gt;\\n      &lt;/svg&gt;\\n     &lt;/div&gt;\\n     &lt;a class=\"player-mytuner-logo\" href=\"https://mytuner-radio.com?utm_source=widget&amp;utm_medium=player\" rel=\"noopener\" style=\"display: inline-block; vertical-align: top;\" target=\"_blank\"&gt;\\n      &lt;img alt=\"Listen on myTuner radio!\" class=\"lazy\" data-src=\"https://mytuner-radio.com/static/icons/widgets/MyTuner_Logo/MyTunerLogo_White.png\" src=\"data:image/svg+xml,%3Csvg%20xmlns=\\'http://www.w3.org/2000/svg\\'%20viewBox=\\'0%200%201%201\\'%3E%3C/svg%3E\" style=\"height: 36px; width: 84px; vertical-align: middle;\"/&gt;\\n     &lt;/a&gt;\\n    &lt;/div&gt;\\n    &lt;ul data-border=\"1\" data-bordercolor=\"#817f80\" id=\"wq/DusOzPWIxwrzDtHhjU3g7wrVDRzdHIsOybA==song-history\" style=\"width: 100%; background-color: rgb(238, 238, 238); max-height: calc(415px); padding: 0px; margin: 0px; overflow-y: scroll;\"&gt;\\n    &lt;/ul&gt;\\n    &lt;script&gt;\\n     var mytuner_scripts=mytuner_scripts||{};mytuner_scripts[\"player-v1.js_queue\"]=mytuner_scripts[\"player-v1.js_queue\"]||[];if(mytuner_scripts[\"player-v1.js-imported\"]==undefined){mytuner_scripts[\"player-v1.js-imported\"]=false;mytuner_scripts[\"player-v1.js\"]=function(){};var s=document.createElement(\"script\");s.type=\"text/javascript\";s.src=\"https://mytuner-radio.com/static/js/widgets/player-v1.js\";s.defer=true;if(s.readyState){s.onreadystatechange=function(){if(s.readyState==\"loaded\"||s.readyState==\"complete\"){s.onreadystatechange=null;runQueue();}};}else{s.onload=function(){runQueue();};}\\ndocument.getElementsByTagName(\\'head\\')[0].appendChild(s);function runQueue(){mytuner_scripts[\"player-v1.js_queue\"].forEach(function(func){func();});}\\nmytuner_scripts[\"player-v1.js_queue\"].push(function(){mytuner_scripts[\"player-v1.js\"](\"wq/DusOzPWIxwrzDtHhjU3g7wrVDRzdHIsOybA==\")});}else{let widget=document.getElementById(\"wq/DusOzPWIxwrzDtHhjU3g7wrVDRzdHIsOybA==\");if(widget&&widget.dataset.requires_initialization===\"true\"){if(mytuner_scripts[\"player-v1.js-imported\"]){mytuner_scripts[\"player-v1.js\"](\"wq/DusOzPWIxwrzDtHhjU3g7wrVDRzdHIsOybA==\");widget.dataset.requires_initialization=\"false\";}else{mytuner_scripts[\"player-v1.js_queue\"].push(function(){mytuner_scripts[\"player-v1.js\"](\"wq/DusOzPWIxwrzDtHhjU3g7wrVDRzdHIsOybA==\");widget.dataset.requires_initialization=\"false\";});}}}\\n    &lt;/script&gt;\\n    &lt;script&gt;\\n     var mytuner_scripts=mytuner_scripts||{};if(mytuner_scripts[\"widget-player-v1.js-imported\"]==undefined){mytuner_scripts[\"widget-player-v1.js-imported\"]=false;var s=document.createElement(\"script\");s.type=\"text/javascript\";s.src=\"https://mytuner-radio.com/static/js/widgets/widget-player-v1.js\";s.defer=true;document.getElementsByTagName(\\'head\\')[0].appendChild(s);}\\n    &lt;/script&gt;\\n   &lt;/div&gt;\\n   &lt;main class=\"post-container row\"&gt;\\n    &lt;div class=\"post-middle article-container post\"&gt;\\n     &lt;script defer=\"\"&gt;\\n      window.googletag=window.googletag||{cmd:[]};googletag.cmd.push(function(){googletag.defineSlot(\\'/113471580/dailytrust.com_300_600_article_right\\',[\\'fluid\\',[300,600]],\\'div-gpt-ad-1669708905704-0\\').addService(googletag.pubads());googletag.pubads().enableSingleRequest();googletag.enableServices();});\\n     &lt;/script&gt;\\n     &lt;script&gt;\\n      window.googletag=window.googletag||{cmd:[]};googletag.cmd.push(function(){googletag.defineSlot(\\'/113471580/dailytrust.com_inarticle-1\\',[[250,250],[300,250],[728,90],[168,42],[960,90],[120,30],[168,28],[120,20],[120,60],[200,200],[950,90],[750,200]],\\'div-gpt-ad-1669272607411-0\\').addService(googletag.pubads());googletag.pubads().enableSingleRequest();googletag.pubads().collapseEmptyDivs();googletag.enableServices();});\\n     &lt;/script&gt;\\n     &lt;script defer=\"\"&gt;\\n      window.googletag=window.googletag||{cmd:[]};googletag.cmd.push(function(){googletag.defineSlot(\\'/113471580/dailytrust.com_inarticle-2\\',[[120,30],[120,20],[960,90],[234,60],[970,90],[250,250],[300,250],[750,100],[320,50],[200,200],[980,90],[300,100],\\'fluid\\',[120,90],[750,300],[728,90],[300,75],[750,200],[950,90],[300,50],[300,31],[120,60],[970,66]],\\'div-gpt-ad-1669291431753-0\\').addService(googletag.pubads());googletag.pubads().enableSingleRequest();googletag.pubads().collapseEmptyDivs();googletag.enableServices();});\\n     &lt;/script&gt;\\n     &lt;script defer=\"\"&gt;\\n      window.googletag=window.googletag||{cmd:[]};googletag.cmd.push(function(){googletag.defineSlot(\\'/113471580/dailytrust.com_inarticle-3\\',[[950,90],[200,200],[320,100],[300,50],[292,30],[300,31],[120,30],\\'fluid\\',[234,60],[300,75],[750,200],[320,50],[750,100],[320,480],[300,100],[980,90],[120,20],[960,90],[970,90],[250,250],[300,250],[728,90]],\\'div-gpt-ad-1669291045016-0\\').addService(googletag.pubads());googletag.pubads().enableSingleRequest();googletag.pubads().collapseEmptyDivs();googletag.enableServices();});\\n     &lt;/script&gt;\\n     &lt;script defer=\"\"&gt;\\n      window.googletag=window.googletag||{cmd:[]};googletag.cmd.push(function(){googletag.defineSlot(\\'/113471580/dailytrust_article_bottom\\',[[640,360],[468,60],[300,250],[400,300],[728,90]],\\'div-gpt-ad-1670511529037-0\\').addService(googletag.pubads());googletag.pubads().enableSingleRequest();googletag.enableServices();});\\n     &lt;/script&gt;\\n     &lt;script defer=\"\"&gt;\\n      window.googletag=window.googletag||{cmd:[]};googletag.cmd.push(function(){googletag.defineSlot(\\'/113471580/dailytrust_article_bottom\\',[[640,360],[468,60],[400,300],[728,90],[300,250]],\\'div-gpt-ad-1670932839695-0\\').addService(googletag.pubads());googletag.pubads().enableSingleRequest();googletag.enableServices();});\\n     &lt;/script&gt;\\n     &lt;script async=\"\" src=\"https://securepubads.g.doubleclick.net/tag/js/gpt.js\"&gt;\\n     &lt;/script&gt;\\n     &lt;script&gt;\\n      window.googletag=window.googletag||{cmd:[]};googletag.cmd.push(function(){googletag.defineSlot(\\'/113471580/dailytrust.com_article_right_sidebar_top\\',[[300,250],\\'fluid\\'],\\'div-gpt-ad-1685451176264-0\\').addService(googletag.pubads());googletag.pubads().enableSingleRequest();googletag.enableServices();});\\n     &lt;/script&gt;\\n     &lt;script async=\"\" src=\"https://securepubads.g.doubleclick.net/tag/js/gpt.js\"&gt;\\n     &lt;/script&gt;\\n     &lt;script&gt;\\n      window.googletag=window.googletag||{cmd:[]};googletag.cmd.push(function(){googletag.defineSlot(\\'/113471580/dailytrust.com_300_600_article_right\\',[[300,600],\\'fluid\\'],\\'div-gpt-ad-1685451004084-0\\').addService(googletag.pubads());googletag.pubads().enableSingleRequest();googletag.enableServices();});\\n     &lt;/script&gt;\\n     &lt;section class=\"title-header\"&gt;\\n      &lt;div style=\"margin-bottom:10\"&gt;\\n       &lt;div class=\"categoty__title\"&gt;\\n        &lt;a aria-label=\"News\" href=\"https://dailytrust.com/topics/news/\" title=\"News\"&gt;\\n         News\\n        &lt;/a&gt;\\n       &lt;/div&gt;\\n      &lt;/div&gt;\\n      &lt;h1 class=\"h1v1\"&gt;\\n       Last Saturday polls raised issues that require immediate solutions – INEC\\n      &lt;/h1&gt;\\n      &lt;p class=\"excerpt\"&gt;\\n       The Independent National Electoral Commission (INEC) has admitted that the last Saturday presidential and National Assembly elections raises a number of issues that require immediate,…\\n      &lt;/p&gt;\\n     &lt;/section&gt;\\n     &lt;section class=\"featured-image\"&gt;\\n      &lt;div class=\"post-thumbnail\"&gt;\\n       &lt;img alt=\"INEC Chairman, INEC Chairman, Prof. Mahmood Yakubu\" class=\"attachment-post-thumbnail size-post-thumbnail wp-post-image lazy\" data-sizes=\"(max-width: 791px) 100vw, 791px\" data-src=\"https://dailytrust.com/wp-content/uploads/2021/05/INEC-Chairman.jpg\" data-srcset=\"https://dailytrust.com/wp-content/uploads/2021/05/INEC-Chairman.jpg 791w, https://dailytrust.com/wp-content/uploads/2021/05/INEC-Chairman-600x338.jpg 600w, https://dailytrust.com/wp-content/uploads/2021/05/INEC-Chairman-711x400.jpg 711w, https://dailytrust.com/wp-content/uploads/2021/05/INEC-Chairman-768x432.jpg 768w\" decoding=\"async\" height=\"445\" src=\"data:image/svg+xml,%3Csvg%20xmlns=\\'http://www.w3.org/2000/svg\\'%20viewBox=\\'0%200%20791%20445\\'%3E%3C/svg%3E\" width=\"791\"/&gt;\\n      &lt;/div&gt;\\n      &lt;figcaption class=\"caption\"&gt;\\n       &lt;span&gt;\\n        INEC Chairman, INEC Chairman, Prof. Mahmood Yakubu\\n       &lt;/span&gt;\\n      &lt;/figcaption&gt;\\n     &lt;/section&gt;\\n     &lt;section class=\"post-author\"&gt;\\n      &lt;div class=\"wraper flow\"&gt;\\n       &lt;ul class=\"names\"&gt;\\n        &lt;span style=\"margin-right:6px\"&gt;\\n         By\\n        &lt;/span&gt;\\n        &lt;a href=\"https://dailytrust.com/author/Abbas Jimoh\"&gt;\\n         Abbas Jimoh\\n        &lt;/a&gt;\\n       &lt;/ul&gt;\\n      &lt;/div&gt;\\n     &lt;/section&gt;\\n     &lt;section class=\"author__share\"&gt;\\n      &lt;div class=\"post-time\"&gt;\\n       Sat, 4 Mar 2023 14:43:04 WAT\\n      &lt;/div&gt;\\n      &lt;ul class=\"share small dark\"&gt;\\n       &lt;li&gt;\\n        &lt;a href=\"https://wa.me/?text=https://dailytrust.com/last-saturday-polls-raised-issues-that-require-immediate-solutions-inec/\" rel=\"noopener noreferrer\" target=\"_blank\"&gt;\\n         &lt;svg height=\"24\" viewbox=\"0 0 24 24\" width=\"24\" xmlns=\"http://www.w3.org/2000/svg\"&gt;\\n          &lt;path d=\"M12.031 6.172c-3.181 0-5.767 2.586-5.768 5.766-.001 1.298.38 2.27 1.019 3.287l-.582 2.128 2.182-.573c.978.58 1.911.928 3.145.929 3.178 0 5.767-2.587 5.768-5.766.001-3.187-2.575-5.77-5.764-5.771zm3.392 8.244c-.144.405-.837.774-1.17.824-.299.045-.677.063-1.092-.069-.252-.08-.575-.187-.988-.365-1.739-.751-2.874-2.502-2.961-2.617-.087-.116-.708-.94-.708-1.793s.448-1.273.607-1.446c.159-.173.346-.217.462-.217l.332.006c.106.005.249-.04.39.298.144.347.491 1.2.534 1.287.043.087.072.188.014.304-.058.116-.087.188-.173.289l-.26.304c-.087.086-.177.18-.076.354.101.174.449.741.964 1.201.662.591 1.221.774 1.394.86s.274.072.376-.043c.101-.116.433-.506.549-.68.116-.173.231-.145.39-.087s1.011.477 1.184.564.289.13.332.202c.045.072.045.419-.1.824zm-3.423-14.416c-6.627 0-12 5.373-12 12s5.373 12 12 12 12-5.373 12-12-5.373-12-12-12zm.029 18.88c-1.161 0-2.305-.292-3.318-.844l-3.677.964.984-3.595c-.607-1.052-.927-2.246-.926-3.468.001-3.825 3.113-6.937 6.937-6.937 1.856.001 3.598.723 4.907 2.034 1.31 1.311 2.031 3.054 2.03 4.908-.001 3.825-3.113 6.938-6.937 6.938z\"&gt;\\n          &lt;/path&gt;\\n         &lt;/svg&gt;\\n        &lt;/a&gt;\\n        &lt;li&gt;\\n         &lt;a href=\"https://www.facebook.com/sharer/sharer.php?u=https://dailytrust.com/last-saturday-polls-raised-issues-that-require-immediate-solutions-inec/\" rel=\"noopener noreferrer\" target=\"_blank\"&gt;\\n          &lt;svg height=\"24\" viewbox=\"0 0 24 24\" width=\"24\" xmlns=\"http://www.w3.org/2000/svg\"&gt;\\n           &lt;path d=\"M12 0c-6.627 0-12 5.373-12 12s5.373 12 12 12 12-5.373 12-12-5.373-12-12-12zm3 8h-1.35c-.538 0-.65.221-.65.778v1.222h2l-.209 2h-1.791v7h-3v-7h-2v-2h2v-2.308c0-1.769.931-2.692 3.029-2.692h1.971v3z\"&gt;\\n           &lt;/path&gt;\\n          &lt;/svg&gt;\\n         &lt;/a&gt;\\n        &lt;/li&gt;\\n        &lt;li&gt;\\n         &lt;a href=\"https://twitter.com/intent/tweet?text=https://dailytrust.com/last-saturday-polls-raised-issues-that-require-immediate-solutions-inec/\" rel=\"noopener noreferrer\" target=\"_blank\"&gt;\\n          &lt;svg height=\"24\" viewbox=\"0 0 24 24\" width=\"24\" xmlns=\"http://www.w3.org/2000/svg\"&gt;\\n           &lt;path d=\"M12 0c-6.627 0-12 5.373-12 12s5.373 12 12 12 12-5.373 12-12-5.373-12-12-12zm6.066 9.645c.183 4.04-2.83 8.544-8.164 8.544-1.622 0-3.131-.476-4.402-1.291 1.524.18 3.045-.244 4.252-1.189-1.256-.023-2.317-.854-2.684-1.995.451.086.895.061 1.298-.049-1.381-.278-2.335-1.522-2.304-2.853.388.215.83.344 1.301.359-1.279-.855-1.641-2.544-.889-3.835 1.416 1.738 3.533 2.881 5.92 3.001-.419-1.796.944-3.527 2.799-3.527.825 0 1.572.349 2.096.907.654-.128 1.27-.368 1.824-.697-.215.671-.67 1.233-1.263 1.589.581-.07 1.135-.224 1.649-.453-.384.578-.87 1.084-1.433 1.489z\"&gt;\\n           &lt;/path&gt;\\n          &lt;/svg&gt;\\n         &lt;/a&gt;\\n        &lt;/li&gt;\\n       &lt;/li&gt;\\n      &lt;/ul&gt;\\n     &lt;/section&gt;\\n     &lt;article class=\"body article__body\"&gt;\\n      &lt;p&gt;\\n       &lt;strong&gt;\\n        The Independent National Electoral Commission (INEC) has admitted that the last Saturday presidential and National Assembly elections raises a number of issues that require immediate, medium and long-term solutions.\\n       &lt;/strong&gt;\\n      &lt;/p&gt;\\n      &lt;p&gt;\\n       INEC Chairman, Prof. Mahmood Yakubu, said this on Saturday in Abuja at a meeting with the Resident Electoral commissioners (RECs) held at the INEC headquarters, Abuja.\\n      &lt;/p&gt;\\n      &lt;div id=\"v-dailytrust\"&gt;\\n      &lt;/div&gt;\\n      &lt;script&gt;\\n       (function(v,d,o,ai){ai=d.createElement(\\'script\\');ai.defer=true;ai.async=true;ai.src=v.location.protocol+o;d.head.appendChild(ai);})(window,document,\\'//a.vdo.ai/core/v-dailytrust/vdo.ai.js\\');\\n      &lt;/script&gt;\\n      &lt;p&gt;\\n       He said, “It is imperative to review performance and assess preparations. No doubt, last week’s national elections raised a number of issues that require immediate, medium, and long-term solutions.\\n      &lt;/p&gt;\\n      &lt;p&gt;\\n       “The planning for the election was painstakingly done. However, its implementation came with challenges, some of them unforeseen. The issues of logistics, election technology, behaviour of some election personnel at different levels, attitude of some party agents and supporters added to the extremely challenging environment in which elections are usually held in Nigeria.”\\n      &lt;/p&gt;\\n      &lt;div id=\"div-gpt-ad-1669272607411-0\" style=\"min-width: 120px; min-height: 20px;\"&gt;\\n       &lt;script&gt;\\n        googletag.cmd.push(function(){googletag.display(\\'div-gpt-ad-1669272607411-0\\');});\\n       &lt;/script&gt;\\n      &lt;/div&gt;\\n      &lt;p&gt;\\n       &lt;a href=\"https://dailytrust.com/breaking-inec-bars-negligent-staff-from-governorship-polls-issues-stern-warning-to-recs/\"&gt;\\n        BREAKING: INEC bars negligent staff from governorship polls, issues stern warning to RECs\\n       &lt;/a&gt;\\n      &lt;/p&gt;\\n      &lt;p&gt;\\n       &lt;a href=\"https://dailytrust.com/court-grants-atiku-obis-request-to-inspect-election-materials/\"&gt;\\n        Court grants Atiku, Obi&amp;#8217;s request to inspect election materials\\n       &lt;/a&gt;\\n      &lt;/p&gt;\\n      &lt;p&gt;\\n       The INEC boss said he appreciated the sacrifice and doggedness of Nigerians and the dignity and maturity displayed by political leaders even in the context of divergent views about the election.\\n      &lt;/p&gt;\\n      &lt;p&gt;\\n       Yakubu said that a lot of lessons had been learnt and of immediate concern to the commission is how the identified challenges can be addressed ahead of the next Saturday’s governorship and state houses of aassembly elections in 28 states.\\n      &lt;/p&gt;\\n      &lt;script async=\"\" crossorigin=\"anonymous\" src=\"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7943548946436796\"&gt;\\n      &lt;/script&gt;\\n      &lt;ins class=\"adsbygoogle\" data-ad-client=\"ca-pub-7943548946436796\" data-ad-format=\"auto\" data-ad-slot=\"4569210792\" data-full-width-responsive=\"true\" style=\"display:block\"&gt;\\n      &lt;/ins&gt;\\n      &lt;script&gt;\\n       (adsbygoogle=window.adsbygoogle||[]).push({});\\n      &lt;/script&gt;\\n      &lt;p&gt;\\n       The chairman also said that for the last Saturday’s elections, winners had been declared for 423 national legislative seats while supplementary elections would be held in 46 constituencies.\\n      &lt;/p&gt;\\n      &lt;p&gt;\\n       According to him, in the Senate, 98 out of 109 seats had been declared, with seven political parties winning senatorial seats while in the House of Representatives, 325 out of 360 seats had been won by eight political parties.\\n      &lt;/p&gt;\\n      &lt;p&gt;\\n       He noted that in terms of party representation, this is the most diverse National Assembly since 1999.\\n      &lt;/p&gt;\\n      &lt;p&gt;\\n       He said that in the Senate APC has\\xa0 57 seats, APGA 1, LP 6, NNPP 2, PDP 29, SDP 2, YPP 1; while in the House of Representatives, ADC 2, APC 162, APGA 4, LP 34, NNPP 18, PDP 102, SDP 2, YPP 1.\\n      &lt;/p&gt;\\n      &lt;p&gt;\\n       He said, “Certificates of Return will be presented to Senators-elect on Tuesday 7th March 2023 at 11.00am at the National Collation Centre (the International Conference Centre), Abuja, while Members of the House of Representatives-elect will receive theirs the following day, Wednesday 8th March 2023, at 11.00am at the same venue.\\n      &lt;/p&gt;\\n      &lt;script async=\"\" crossorigin=\"anonymous\" src=\"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7005578050454879\"&gt;\\n      &lt;/script&gt;\\n      &lt;ins class=\"adsbygoogle\" data-ad-client=\"ca-pub-7005578050454879\" data-ad-format=\"auto\" data-ad-slot=\"3639709870\" data-full-width-responsive=\"true\" style=\"display:block\"&gt;\\n      &lt;/ins&gt;\\n      &lt;script&gt;\\n       (adsbygoogle=window.adsbygoogle||[]).push({});\\n      &lt;/script&gt;\\n      &lt;p&gt;\\n       “However, for effective crowd management, each Senator/Member-elect should be accompanied by a maximum of two guests.”\\n      &lt;/p&gt;\\n      &lt;div id=\"div-gpt-ad-1669291045016-0\" style=\"min-width: 120px; min-height: 20px;\"&gt;\\n       &lt;script&gt;\\n        googletag.cmd.push(function(){googletag.display(\\'div-gpt-ad-1669291045016-0\\');});\\n       &lt;/script&gt;\\n      &lt;/div&gt;\\n      &lt;a href=\"https://dailytrust.com/nigerians-can-now-get-paid-monthly-in-us-dollars/\" rel=\"noopener\" target=\"_blank\"&gt;\\n       &lt;div style=\"border: 1px dashed red; padding: 2rem\"&gt;\\n        &lt;p&gt;\\n         Dollar payments are now available for ALL Nigerians. Our clients earn about $5,000 - $10,000 acquiring premium domains. Read testimonials from others who have benefited. Click here to start\\n        &lt;/p&gt;\\n       &lt;/div&gt;\\n      &lt;/a&gt;\\n      &lt;a href=\"https://app.expertnaire.com/product/6592959075/7169283889/\" rel=\"noopener\" target=\"_blank\"&gt;\\n       &lt;div style=\"border: 1px dashed red; padding: 2rem\"&gt;\\n        &lt;p&gt;\\n         Learn How to Relocate to Canada &amp; Become a Permanent Resident With Your Family. No Need for IELTS &amp; Agent Wahala. Click to Learn More\\n        &lt;/p&gt;\\n       &lt;/div&gt;\\n      &lt;/a&gt;\\n      &lt;a href=\"https://app.expertnaire.com/product/6592959075/7669096392/\" rel=\"noopener\" target=\"_blank\"&gt;\\n       &lt;div style=\"border: 1px dashed red; padding: 2rem\"&gt;\\n        &lt;p&gt;\\n         I Made N1 Million Naira Monthly Using this WhatsApp Marketing Strategy. Follow My Steps &amp; Make More Sales. Click to Learn More\\n        &lt;/p&gt;\\n       &lt;/div&gt;\\n      &lt;/a&gt;\\n      &lt;div class=\"sharedaddy sd-sharing-enabled\"&gt;\\n       &lt;div class=\"robots-nocontent sd-block sd-social sd-social-icon-text sd-sharing\"&gt;\\n        &lt;h3 class=\"sd-title\"&gt;\\n         Share this:\\n        &lt;/h3&gt;\\n        &lt;div class=\"sd-content\"&gt;\\n         &lt;ul&gt;\\n          &lt;li class=\"share-twitter\"&gt;\\n           &lt;a class=\"share-twitter sd-button share-icon\" data-shared=\"sharing-twitter-1208528\" href=\"https://dailytrust.com/last-saturday-polls-raised-issues-that-require-immediate-solutions-inec/?share=twitter\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Click to share on Twitter\"&gt;\\n            &lt;span&gt;\\n             Twitter\\n            &lt;/span&gt;\\n           &lt;/a&gt;\\n          &lt;/li&gt;\\n          &lt;li class=\"share-facebook\"&gt;\\n           &lt;a class=\"share-facebook sd-button share-icon\" data-shared=\"sharing-facebook-1208528\" href=\"https://dailytrust.com/last-saturday-polls-raised-issues-that-require-immediate-solutions-inec/?share=facebook\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Click to share on Facebook\"&gt;\\n            &lt;span&gt;\\n             Facebook\\n            &lt;/span&gt;\\n           &lt;/a&gt;\\n          &lt;/li&gt;\\n          &lt;li class=\"share-jetpack-whatsapp\"&gt;\\n           &lt;a class=\"share-jetpack-whatsapp sd-button share-icon\" data-shared=\"\" href=\"https://dailytrust.com/last-saturday-polls-raised-issues-that-require-immediate-solutions-inec/?share=jetpack-whatsapp\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Click to share on WhatsApp\"&gt;\\n            &lt;span&gt;\\n             WhatsApp\\n            &lt;/span&gt;\\n           &lt;/a&gt;\\n          &lt;/li&gt;\\n          &lt;li class=\"share-linkedin\"&gt;\\n           &lt;a class=\"share-linkedin sd-button share-icon\" data-shared=\"sharing-linkedin-1208528\" href=\"https://dailytrust.com/last-saturday-polls-raised-issues-that-require-immediate-solutions-inec/?share=linkedin\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Click to share on LinkedIn\"&gt;\\n            &lt;span&gt;\\n             LinkedIn\\n            &lt;/span&gt;\\n           &lt;/a&gt;\\n          &lt;/li&gt;\\n          &lt;li class=\"share-pinterest\"&gt;\\n           &lt;a class=\"share-pinterest sd-button share-icon\" data-shared=\"sharing-pinterest-1208528\" href=\"https://dailytrust.com/last-saturday-polls-raised-issues-that-require-immediate-solutions-inec/?share=pinterest\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Click to share on Pinterest\"&gt;\\n            &lt;span&gt;\\n             Pinterest\\n            &lt;/span&gt;\\n           &lt;/a&gt;\\n          &lt;/li&gt;\\n          &lt;li class=\"share-telegram\"&gt;\\n           &lt;a class=\"share-telegram sd-button share-icon\" data-shared=\"\" href=\"https://dailytrust.com/last-saturday-polls-raised-issues-that-require-immediate-solutions-inec/?share=telegram\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Click to share on Telegram\"&gt;\\n            &lt;span&gt;\\n             Telegram\\n            &lt;/span&gt;\\n           &lt;/a&gt;\\n          &lt;/li&gt;\\n          &lt;li&gt;\\n           &lt;a class=\"sharing-anchor sd-button share-more\" href=\"#\"&gt;\\n            &lt;span&gt;\\n             More\\n            &lt;/span&gt;\\n           &lt;/a&gt;\\n          &lt;/li&gt;\\n          &lt;li class=\"share-end\"&gt;\\n          &lt;/li&gt;\\n         &lt;/ul&gt;\\n         &lt;div class=\"sharing-hidden\"&gt;\\n          &lt;div class=\"inner\" style=\"display: none;\"&gt;\\n           &lt;ul&gt;\\n            &lt;li class=\"share-mastodon\"&gt;\\n             &lt;a class=\"share-mastodon sd-button share-icon\" data-shared=\"sharing-mastodon-1208528\" href=\"https://dailytrust.com/last-saturday-polls-raised-issues-that-require-immediate-solutions-inec/?share=mastodon\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Click to share on Mastodon\"&gt;\\n              &lt;span&gt;\\n               Mastodon\\n              &lt;/span&gt;\\n             &lt;/a&gt;\\n            &lt;/li&gt;\\n            &lt;li class=\"share-email\"&gt;\\n             &lt;a class=\"share-email sd-button share-icon\" data-email-share-error-text=\"If you\\'re having problems sharing via email, you might not have email set up for your browser. You may need to create a new email yourself.\" data-email-share-error-title=\"Do you have email set up?\" data-email-share-nonce=\"15664b4253\" data-email-share-track-url=\"https://dailytrust.com/last-saturday-polls-raised-issues-that-require-immediate-solutions-inec/?share=email\" data-shared=\"\" href=\"/cdn-cgi/l/email-protection#447b3731262e21273079617106172c25362120617674142b373061710061767408253730617674172530313620253d617674342b28283761767436252d3721206176742d3737312137617674302c2530617674362135312d36216176742d292921202d253021617674372b2831302d2b2a37617674696176740d0a010762262b203d792c3030343761770561760261760220252d283d30363137306a272b296176022825373069372530313620253d69342b2828376936252d372120692d373731213769302c253069362135312d3621692d292921202d25302169372b2831302d2b2a37692d2a212761760262372c253621792129252d28\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Click to email a link to a friend\"&gt;\\n              &lt;span&gt;\\n               Email\\n              &lt;/span&gt;\\n             &lt;/a&gt;\\n            &lt;/li&gt;\\n            &lt;li class=\"share-reddit\"&gt;\\n             &lt;a class=\"share-reddit sd-button share-icon\" data-shared=\"\" href=\"https://dailytrust.com/last-saturday-polls-raised-issues-that-require-immediate-solutions-inec/?share=reddit\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Click to share on Reddit\"&gt;\\n              &lt;span&gt;\\n               Reddit\\n              &lt;/span&gt;\\n             &lt;/a&gt;\\n            &lt;/li&gt;\\n            &lt;li class=\"share-pocket\"&gt;\\n             &lt;a class=\"share-pocket sd-button share-icon\" data-shared=\"\" href=\"https://dailytrust.com/last-saturday-polls-raised-issues-that-require-immediate-solutions-inec/?share=pocket\" rel=\"nofollow noopener noreferrer\" target=\"_blank\" title=\"Click to share on Pocket\"&gt;\\n              &lt;span&gt;\\n               Pocket\\n              &lt;/span&gt;\\n             &lt;/a&gt;\\n            &lt;/li&gt;\\n            &lt;li class=\"share-end\"&gt;\\n            &lt;/li&gt;\\n           &lt;/ul&gt;\\n          &lt;/div&gt;\\n         &lt;/div&gt;\\n        &lt;/div&gt;\\n       &lt;/div&gt;\\n      &lt;/div&gt;\\n      &lt;div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-212980935-1208528-64be182e2d24c\" data-src=\"https://widgets.wp.com/likes/#blog_id=212980935&amp;post_id=1208528&amp;origin=dailytrust.com&amp;obj_id=212980935-1208528-64be182e2d24c\" data-title=\"Like or Reblog\" id=\"like-post-wrapper-212980935-1208528-64be182e2d24c\"&gt;\\n       &lt;h3 class=\"sd-title\"&gt;\\n        Like this:\\n       &lt;/h3&gt;\\n       &lt;div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height: 55px;\"&gt;\\n        &lt;span class=\"button\"&gt;\\n         &lt;span&gt;\\n          Like\\n         &lt;/span&gt;\\n        &lt;/span&gt;\\n        &lt;span class=\"loading\"&gt;\\n         Loading...\\n        &lt;/span&gt;\\n       &lt;/div&gt;\\n       &lt;span class=\"sd-text-color\"&gt;\\n       &lt;/span&gt;\\n       &lt;a class=\"sd-link-color\"&gt;\\n       &lt;/a&gt;\\n      &lt;/div&gt;\\n      &lt;div class=\"jp-relatedposts\" id=\"jp-relatedposts\"&gt;\\n       &lt;h3 class=\"jp-relatedposts-headline\"&gt;\\n        &lt;em&gt;\\n         Related\\n        &lt;/em&gt;\\n       &lt;/h3&gt;\\n      &lt;/div&gt;\\n     &lt;/article&gt;\\n     &lt;div class=\"article-footer ad-container\"&gt;\\n      &lt;section class=\"widget_text widget widget_custom_html\" id=\"custom_html-2\"&gt;\\n       &lt;div class=\"textwidget custom-html-widget\"&gt;\\n        &lt;div class=\"JC-WIDGET-DMROOT\" data-widget-id=\"c3de7bbf2df2eb66be45312bc808b557\"&gt;\\n        &lt;/div&gt;\\n        &lt;script data-cfasync=\"false\" src=\"/cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min.js\"&gt;\\n        &lt;/script&gt;\\n        &lt;script async=\"async\" src=\"https://static.jubnaadserve.com/api/widget.js\"&gt;\\n        &lt;/script&gt;\\n       &lt;/div&gt;\\n      &lt;/section&gt;\\n     &lt;/div&gt;\\n     &lt;div class=\"post-tags\"&gt;\\n      &lt;ul&gt;\\n       &lt;li&gt;\\n        &lt;a href=\"https://dailytrust.com/tag/immediate-solutions/\" rel=\"tag\"&gt;\\n         immediate solutions\\n        &lt;/a&gt;\\n       &lt;/li&gt;\\n       &lt;li&gt;\\n        &lt;a href=\"https://dailytrust.com/tag/inec/\" rel=\"tag\"&gt;\\n         INEC\\n        &lt;/a&gt;\\n       &lt;/li&gt;\\n       &lt;li&gt;\\n        &lt;a href=\"https://dailytrust.com/tag/issues/\" rel=\"tag\"&gt;\\n         issues\\n        &lt;/a&gt;\\n       &lt;/li&gt;\\n       &lt;li&gt;\\n        &lt;a href=\"https://dailytrust.com/tag/polls/\" rel=\"tag\"&gt;\\n         polls\\n        &lt;/a&gt;\\n       &lt;/li&gt;\\n      &lt;/ul&gt;\\n     &lt;/div&gt;\\n     &lt;div class=\"related\"&gt;\\n      &lt;div class=\"post-tags\"&gt;\\n       &lt;div class=\"section__title\"&gt;\\n        More Stories\\n       &lt;/div&gt;\\n       &lt;div&gt;\\n        &lt;li class=\"list_list\"&gt;\\n         &lt;div class=\"list_card\"&gt;\\n          &lt;div class=\"list_body list_compact\"&gt;\\n           &lt;div class=\"list_category\"&gt;\\n            &lt;span class=\"list_time\"&gt;\\n             &lt;svg aria-label=\"Clock24 icon\" height=\"14\" viewbox=\"0 0 24 24\" width=\"14\" xmlns=\"http://www.w3.org/2000/svg\"&gt;\\n              &lt;path d=\"M11.5 2a10.5 10.5 0 1 1 0 21 10.5 10.5 0 0 1 0-21zm0 1a9.5 9.5 0 1 0 0 19 9.5 9.5 0 0 0 0-19zm0 4c.28 0 .5.22.5.5v5.3l5.34-1.77a.5.5 0 1 1 .32.94l-6 2a.5.5 0 0 1-.66-.47v-6c0-.28.22-.5.5-.5z\" fillrule=\"evenodd\"&gt;\\n              &lt;/path&gt;\\n             &lt;/svg&gt;\\n             2 days ago\\n            &lt;/span&gt;\\n           &lt;/div&gt;\\n           &lt;a href=\"/2023-polls-inec-expresses-worry-over-result-management-crisis\"&gt;\\n            2023 polls: INEC expresses worry over result management crisis\\n           &lt;/a&gt;\\n          &lt;/div&gt;\\n          &lt;a href=\"/2023-polls-inec-expresses-worry-over-result-management-crisis\"&gt;\\n           &lt;div class=\"desktop__image\" style=\"min-width: 80px; height: 80px; background: #ccc; display: block; position:relative\"&gt;\\n            &lt;img alt=\"\" class=\"attachment-80x80x10vwximg size-80x80x10vwximg wp-post-image lazy\" data-src=\"https://dailytrust.com/wp-content/uploads/2023/03/INEC-150x150.jpg\" decoding=\"async\" height=\"80\" src=\"data:image/svg+xml,%3Csvg%20xmlns=\\'http://www.w3.org/2000/svg\\'%20viewBox=\\'0%200%2080%2080\\'%3E%3C/svg%3E\" width=\"80\"/&gt;\\n           &lt;/div&gt;\\n          &lt;/a&gt;\\n         &lt;/div&gt;\\n        &lt;/li&gt;\\n        &lt;li class=\"list_list\"&gt;\\n         &lt;div class=\"list_card\"&gt;\\n          &lt;div class=\"list_body list_compact\"&gt;\\n           &lt;div class=\"list_category\"&gt;\\n            &lt;span class=\"list_time\"&gt;\\n             &lt;svg aria-label=\"Clock24 icon\" height=\"14\" viewbox=\"0 0 24 24\" width=\"14\" xmlns=\"http://www.w3.org/2000/svg\"&gt;\\n              &lt;path d=\"M11.5 2a10.5 10.5 0 1 1 0 21 10.5 10.5 0 0 1 0-21zm0 1a9.5 9.5 0 1 0 0 19 9.5 9.5 0 0 0 0-19zm0 4c.28 0 .5.22.5.5v5.3l5.34-1.77a.5.5 0 1 1 .32.94l-6 2a.5.5 0 0 1-.66-.47v-6c0-.28.22-.5.5-.5z\" fillrule=\"evenodd\"&gt;\\n              &lt;/path&gt;\\n             &lt;/svg&gt;\\n             3 days ago\\n            &lt;/span&gt;\\n           &lt;/div&gt;\\n           &lt;a href=\"/kano-govship-tribunal-inec-closes-case-presents-no-witness\"&gt;\\n            Kano gov’ship tribunal: INEC closes case, presents no witness\\n           &lt;/a&gt;\\n          &lt;/div&gt;\\n          &lt;a href=\"/kano-govship-tribunal-inec-closes-case-presents-no-witness\"&gt;\\n           &lt;div class=\"desktop__image\" style=\"min-width: 80px; height: 80px; background: #ccc; display: block; position:relative\"&gt;\\n            &lt;img alt=\"\" class=\"attachment-80x80x10vwximg size-80x80x10vwximg wp-post-image lazy\" data-src=\"https://dailytrust.com/wp-content/uploads/2023/02/INEC-150x150.webp\" decoding=\"async\" height=\"80\" src=\"data:image/svg+xml,%3Csvg%20xmlns=\\'http://www.w3.org/2000/svg\\'%20viewBox=\\'0%200%2080%2080\\'%3E%3C/svg%3E\" width=\"80\"/&gt;\\n           &lt;/div&gt;\\n          &lt;/a&gt;\\n         &lt;/div&gt;\\n        &lt;/li&gt;\\n        &lt;li class=\"list_list\"&gt;\\n         &lt;div class=\"list_card\"&gt;\\n          &lt;div class=\"list_body list_compact\"&gt;\\n           &lt;div class=\"list_category\"&gt;\\n            &lt;span class=\"list_time\"&gt;\\n             &lt;svg aria-label=\"Clock24 icon\" height=\"14\" viewbox=\"0 0 24 24\" width=\"14\" xmlns=\"http://www.w3.org/2000/svg\"&gt;\\n              &lt;path d=\"M11.5 2a10.5 10.5 0 1 1 0 21 10.5 10.5 0 0 1 0-21zm0 1a9.5 9.5 0 1 0 0 19 9.5 9.5 0 0 0 0-19zm0 4c.28 0 .5.22.5.5v5.3l5.34-1.77a.5.5 0 1 1 .32.94l-6 2a.5.5 0 0 1-.66-.47v-6c0-.28.22-.5.5-.5z\" fillrule=\"evenodd\"&gt;\\n              &lt;/path&gt;\\n             &lt;/svg&gt;\\n             4 days ago\\n            &lt;/span&gt;\\n           &lt;/div&gt;\\n           &lt;a href=\"/just-in-inec-registers-new-party\"&gt;\\n            INEC registers new party\\n           &lt;/a&gt;\\n          &lt;/div&gt;\\n          &lt;a href=\"/just-in-inec-registers-new-party\"&gt;\\n           &lt;div class=\"desktop__image\" style=\"min-width: 80px; height: 80px; background: #ccc; display: block; position:relative\"&gt;\\n            &lt;img alt=\"\" class=\"attachment-80x80x10vwximg size-80x80x10vwximg wp-post-image lazy\" data-src=\"https://dailytrust.com/wp-content/uploads/2023/03/INEC-150x150.jpg\" decoding=\"async\" height=\"80\" src=\"data:image/svg+xml,%3Csvg%20xmlns=\\'http://www.w3.org/2000/svg\\'%20viewBox=\\'0%200%2080%2080\\'%3E%3C/svg%3E\" width=\"80\"/&gt;\\n           &lt;/div&gt;\\n          &lt;/a&gt;\\n         &lt;/div&gt;\\n        &lt;/li&gt;\\n        &lt;li class=\"list_list\"&gt;\\n         &lt;div class=\"list_card\"&gt;\\n          &lt;div class=\"list_body list_compact\"&gt;\\n           &lt;div class=\"list_category\"&gt;\\n            &lt;span class=\"list_time\"&gt;\\n             &lt;svg aria-label=\"Clock24 icon\" height=\"14\" viewbox=\"0 0 24 24\" width=\"14\" xmlns=\"http://www.w3.org/2000/svg\"&gt;\\n              &lt;path d=\"M11.5 2a10.5 10.5 0 1 1 0 21 10.5 10.5 0 0 1 0-21zm0 1a9.5 9.5 0 1 0 0 19 9.5 9.5 0 0 0 0-19zm0 4c.28 0 .5.22.5.5v5.3l5.34-1.77a.5.5 0 1 1 .32.94l-6 2a.5.5 0 0 1-.66-.47v-6c0-.28.22-.5.5-.5z\" fillrule=\"evenodd\"&gt;\\n              &lt;/path&gt;\\n             &lt;/svg&gt;\\n             5 days ago\\n            &lt;/span&gt;\\n           &lt;/div&gt;\\n           &lt;a href=\"/photos-inec-continues-review-of-2023-elections\"&gt;\\n            PHOTOS: INEC continues review of 2023 elections\\n           &lt;/a&gt;\\n          &lt;/div&gt;\\n          &lt;a href=\"/photos-inec-continues-review-of-2023-elections\"&gt;\\n           &lt;div class=\"desktop__image\" style=\"min-width: 80px; height: 80px; background: #ccc; display: block; position:relative\"&gt;\\n            &lt;img alt=\"\" class=\"attachment-80x80x10vwximg size-80x80x10vwximg wp-post-image lazy\" data-src=\"https://dailytrust.com/wp-content/uploads/2023/07/IMG-20230719-WA0007-150x150.jpg\" decoding=\"async\" height=\"80\" src=\"data:image/svg+xml,%3Csvg%20xmlns=\\'http://www.w3.org/2000/svg\\'%20viewBox=\\'0%200%2080%2080\\'%3E%3C/svg%3E\" width=\"80\"/&gt;\\n           &lt;/div&gt;\\n          &lt;/a&gt;\\n         &lt;/div&gt;\\n        &lt;/li&gt;\\n       &lt;/div&gt;\\n      &lt;/div&gt;\\n     &lt;/div&gt;\\n     &lt;div id=\"disqus_thread\"&gt;\\n     &lt;/div&gt;\\n    &lt;/div&gt;\\n    &lt;aside class=\"widget-area post-right\" id=\"primary\"&gt;\\n     &lt;section class=\"widget_text widget widget_custom_html\" id=\"custom_html-6\"&gt;\\n      &lt;div class=\"textwidget custom-html-widget\"&gt;\\n       &lt;div id=\"div-gpt-ad-1685451176264-0\" style=\"min-width: 300px; min-height: 250px;\"&gt;\\n        &lt;script&gt;\\n         googletag.cmd.push(function(){googletag.display(\\'div-gpt-ad-1685451176264-0\\');});\\n        &lt;/script&gt;\\n       &lt;/div&gt;\\n      &lt;/div&gt;\\n     &lt;/section&gt;\\n     &lt;section class=\"widget widget_block\" id=\"block-3\"&gt;\\n      &lt;script async=\"\" src=\"https://securepubads.g.doubleclick.net/tag/js/gpt.js\"&gt;\\n      &lt;/script&gt;\\n      &lt;script&gt;\\n       window.googletag=window.googletag||{cmd:[]};googletag.cmd.push(function(){googletag.defineSlot(\\'/113471580/dailytrust.com_article_right_sidebar_bottom\\',[\\'fluid\\',[250,250],[300,250]],\\'div-gpt-ad-1687524185525-0\\').addService(googletag.pubads());googletag.pubads().enableSingleRequest();googletag.enableServices();});\\n      &lt;/script&gt;\\n      &lt;div id=\"div-gpt-ad-1687524185525-0\" style=\"min-width: 250px; min-height: 250px;\"&gt;\\n       &lt;script&gt;\\n        googletag.cmd.push(function(){googletag.display(\\'div-gpt-ad-1687524185525-0\\');});\\n       &lt;/script&gt;\\n      &lt;/div&gt;\\n     &lt;/section&gt;\\n     &lt;section class=\"widget widget_block\" id=\"block-4\"&gt;\\n      &lt;script async=\"\" src=\"https://securepubads.g.doubleclick.net/tag/js/gpt.js\"&gt;\\n      &lt;/script&gt;\\n      &lt;script&gt;\\n       window.googletag=window.googletag||{cmd:[]};googletag.cmd.push(function(){googletag.defineSlot(\\'/113471580/dailytrust.com_300_600_article_right\\',[[300,600],\\'fluid\\'],\\'div-gpt-ad-1689177188515-0\\').addService(googletag.pubads());googletag.pubads().enableSingleRequest();googletag.enableServices();});\\n      &lt;/script&gt;\\n      &lt;div id=\"div-gpt-ad-1689177188515-0\" style=\"min-width: 300px; min-height: 600px;\"&gt;\\n       &lt;script&gt;\\n        googletag.cmd.push(function(){googletag.display(\\'div-gpt-ad-1689177188515-0\\');});\\n       &lt;/script&gt;\\n      &lt;/div&gt;\\n     &lt;/section&gt;\\n     &lt;section class=\"widget_text widget widget_custom_html\" id=\"custom_html-4\"&gt;\\n      &lt;div class=\"textwidget custom-html-widget\"&gt;\\n       &lt;div id=\"div-gpt-ad-1685451004084-0\" style=\"min-width: 300px; min-height: 600px;\"&gt;\\n        &lt;script&gt;\\n         googletag.cmd.push(function(){googletag.display(\\'div-gpt-ad-1685451004084-0\\');});\\n        &lt;/script&gt;\\n       &lt;/div&gt;\\n      &lt;/div&gt;\\n     &lt;/section&gt;\\n     &lt;section class=\"widget widget_twitter_timeline\" id=\"twitter_timeline-2\"&gt;\\n      &lt;h2 class=\"widget-title\"&gt;\\n       Follow us on Twitter\\n      &lt;/h2&gt;\\n      &lt;a class=\"twitter-timeline\" data-border-color=\"#f0f0f1\" data-chrome=\"noscrollbar\" data-lang=\"EN\" data-partner=\"jetpack\" data-theme=\"light\" data-tweet-limit=\"3\" data-width=\"300\" href=\"https://twitter.com/@daily_trust\"&gt;\\n       My Tweets\\n      &lt;/a&gt;\\n     &lt;/section&gt;\\n    &lt;/aside&gt;\\n   &lt;/main&gt;\\n   &lt;footer class=\"footer\"&gt;\\n    &lt;div class=\"footer__top footer__wrapper container\"&gt;\\n     &lt;div class=\"footer__social\"&gt;\\n      &lt;h4 class=\"f_title\"&gt;\\n       Follow\\n       &lt;span&gt;\\n        Dailytrust\\n       &lt;/span&gt;\\n      &lt;/h4&gt;\\n      &lt;div class=\"f_menu\"&gt;\\n      &lt;/div&gt;\\n     &lt;/div&gt;\\n     &lt;div class=\"footer__menu\"&gt;\\n      &lt;ul class=\"f_menu\"&gt;\\n       &lt;li&gt;\\n        &lt;a href=\"https://dailytrust.com\" target=\"_home\"&gt;\\n         Home\\n        &lt;/a&gt;\\n       &lt;/li&gt;\\n       &lt;li&gt;\\n        &lt;a href=\"https://dailytrust.com/about-us\" target=\"_about_us\"&gt;\\n         About Us\\n        &lt;/a&gt;\\n       &lt;/li&gt;\\n       &lt;li&gt;\\n        &lt;a href=\"https://dailytrust.com/contact-us\" target=\"_contact\"&gt;\\n         Contact Us\\n        &lt;/a&gt;\\n       &lt;/li&gt;\\n       &lt;li&gt;\\n        &lt;a href=\"https://career.dailytrust.com\" target=\"_career\"&gt;\\n         Careers\\n        &lt;/a&gt;\\n       &lt;/li&gt;\\n       &lt;li&gt;\\n        &lt;a href=\"https://membership.dailytrust.com\" target=\"_membership\"&gt;\\n         Trust +\\n        &lt;/a&gt;\\n       &lt;/li&gt;\\n      &lt;/ul&gt;\\n      &lt;ul class=\"f_menu\"&gt;\\n       &lt;li&gt;\\n        &lt;a href=\"https://dailytrust.com/advertise\" target=\"_advert\"&gt;\\n         Advertise With Us\\n        &lt;/a&gt;\\n       &lt;/li&gt;\\n       &lt;li&gt;\\n        &lt;a href=\"https://dailytrust.com/privacy-policy\" target=\"_advert\"&gt;\\n         Privacy Policy\\n        &lt;/a&gt;\\n       &lt;/li&gt;\\n       &lt;li&gt;\\n        &lt;a href=\"https://dailytrust.com/terms-of-use\" target=\"_advert\"&gt;\\n         Terms of Use\\n        &lt;/a&gt;\\n       &lt;/li&gt;\\n       &lt;li&gt;\\n        &lt;a href=\"https://businessdirectory.dailytrust.com\" target=\"_businessdirectory\"&gt;\\n         Business Directory\\n        &lt;/a&gt;\\n       &lt;/li&gt;\\n       &lt;li&gt;\\n        &lt;a href=\"https://dailytrust.com/change-of-name/\" target=\"_changeofname\"&gt;\\n         Change Of Name\\n        &lt;/a&gt;\\n       &lt;/li&gt;\\n       &lt;li&gt;\\n        &lt;a href=\"https://dailytrust.com/loss-of-document/\" target=\"_changeofname\"&gt;\\n         Lost Of Document\\n        &lt;/a&gt;\\n       &lt;/li&gt;\\n       &lt;li&gt;\\n        &lt;a href=\"https://dailytrust.com/registration-of-trustees/\" target=\"_registrationoftrustees\"&gt;\\n         Registration Of Trustee\\n        &lt;/a&gt;\\n       &lt;/li&gt;\\n      &lt;/ul&gt;\\n      &lt;ul class=\"menu__addr f_menu\"&gt;\\n       &lt;li&gt;\\n        &lt;a href=\"https://goo.gl/maps/28L1JUVF4setF9mi9\" rel=\"noopener noreferrer\" target=\"_blank\"&gt;\\n         20 P.O.W Mafemi Crescent, Utako District, Abuja\\n        &lt;/a&gt;\\n       &lt;/li&gt;\\n       &lt;li&gt;\\n        &lt;a href=\"tel:+2347001777577\" rel=\"noopener\" target=\"_blank\"&gt;\\n         700-177-7577\\n        &lt;/a&gt;\\n       &lt;/li&gt;\\n       &lt;li&gt;\\n        &lt;a href=\"/cdn-cgi/l/email-protection#7d14131b123d191c141104090f080e09531e1210\" rel=\"noopener\" target=\"_blank\"&gt;\\n         &lt;span class=\"__cf_email__\" data-cfemail=\"ee87808881ae8a8f8782979a9c\"&gt;\\n          [email\\xa0protected]\\n         &lt;/span&gt;\\n         ust.com\\n        &lt;/a&gt;\\n       &lt;/li&gt;\\n      &lt;/ul&gt;\\n     &lt;/div&gt;\\n    &lt;/div&gt;\\n    &lt;section class=\"footer__bottom\"&gt;\\n     &lt;div class=\"footer__wrapper container\" style=\"justify-content: space-between;\"&gt;\\n      &lt;div style=\"opacity: 0.6;\"&gt;\\n       1998 - 2021 Media Trust Limited. All rights reserved.\\n      &lt;/div&gt;\\n     &lt;/div&gt;\\n    &lt;/section&gt;\\n   &lt;/footer&gt;\\n  &lt;/main&gt;\\n  &lt;script data-cfasync=\"false\" src=\"/cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min.js\"&gt;\\n  &lt;/script&gt;\\n  &lt;script async=\"\" src=\"//lib.wtg-ads.com/publisher/dailytrust.com/bf209a17d532927284fd.js\"&gt;\\n  &lt;/script&gt;\\n  &lt;script&gt;\\n   window.WPCOM_sharing_counts={\"https:\\\\/\\\\/dailytrust.com\\\\/last-saturday-polls-raised-issues-that-require-immediate-solutions-inec\\\\/\":1208528};\\n  &lt;/script&gt;\\n  &lt;script src=\"https://dailytrust.com/wp-content/cache/minify/1c044.js\"&gt;\\n  &lt;/script&gt;\\n  &lt;script defer=\"\" id=\"jetpack-stats-js\" src=\"https://stats.wp.com/e-202330.js\"&gt;\\n  &lt;/script&gt;\\n  &lt;script id=\"jetpack-stats-js-after\"&gt;\\n   _stq=window._stq||[];_stq.push([\"view\",{v:\\'ext\\',blog:\\'212980935\\',post:\\'1208528\\',tz:\\'1\\',srv:\\'dailytrust.com\\',j:\\'1:12.3\\'}]);_stq.push([\"clickTrackerInit\",\"212980935\",\"1208528\"]);\\n  &lt;/script&gt;\\n  &lt;script src=\"https://dailytrust.com/wp-content/cache/minify/4347a.js\"&gt;\\n  &lt;/script&gt;\\n  &lt;script id=\"sharing-js-js-extra\"&gt;\\n   var sharing_js_options={\"lang\":\"en\",\"counts\":\"1\",\"is_stats_active\":\"1\"};\\n  &lt;/script&gt;\\n  &lt;script src=\"https://dailytrust.com/wp-content/cache/minify/d6089.js\"&gt;\\n  &lt;/script&gt;\\n  &lt;script id=\"sharing-js-js-after\"&gt;\\n   var windowOpen;(function(){function matches(el,sel){return!!(el.matches&&el.matches(sel)||el.msMatchesSelector&&el.msMatchesSelector(sel));}\\ndocument.body.addEventListener(\\'click\\',function(event){if(!event.target){return;}\\nvar el;if(matches(event.target,\\'a.share-twitter\\')){el=event.target;}else if(event.target.parentNode&&matches(event.target.parentNode,\\'a.share-twitter\\')){el=event.target.parentNode;}\\nif(el){event.preventDefault();if(typeof windowOpen!==\\'undefined\\'){windowOpen.close();}\\nwindowOpen=window.open(el.getAttribute(\\'href\\'),\\'wpcomtwitter\\',\\'menubar=1,resizable=1,width=600,height=350\\');return false;}});})();var windowOpen;(function(){function matches(el,sel){return!!(el.matches&&el.matches(sel)||el.msMatchesSelector&&el.msMatchesSelector(sel));}\\ndocument.body.addEventListener(\\'click\\',function(event){if(!event.target){return;}\\nvar el;if(matches(event.target,\\'a.share-facebook\\')){el=event.target;}else if(event.target.parentNode&&matches(event.target.parentNode,\\'a.share-facebook\\')){el=event.target.parentNode;}\\nif(el){event.preventDefault();if(typeof windowOpen!==\\'undefined\\'){windowOpen.close();}\\nwindowOpen=window.open(el.getAttribute(\\'href\\'),\\'wpcomfacebook\\',\\'menubar=1,resizable=1,width=600,height=400\\');return false;}});})();var windowOpen;(function(){function matches(el,sel){return!!(el.matches&&el.matches(sel)||el.msMatchesSelector&&el.msMatchesSelector(sel));}\\ndocument.body.addEventListener(\\'click\\',function(event){if(!event.target){return;}\\nvar el;if(matches(event.target,\\'a.share-linkedin\\')){el=event.target;}else if(event.target.parentNode&&matches(event.target.parentNode,\\'a.share-linkedin\\')){el=event.target.parentNode;}\\nif(el){event.preventDefault();if(typeof windowOpen!==\\'undefined\\'){windowOpen.close();}\\nwindowOpen=window.open(el.getAttribute(\\'href\\'),\\'wpcomlinkedin\\',\\'menubar=1,resizable=1,width=580,height=450\\');return false;}});})();var windowOpen;(function(){function matches(el,sel){return!!(el.matches&&el.matches(sel)||el.msMatchesSelector&&el.msMatchesSelector(sel));}\\ndocument.body.addEventListener(\\'click\\',function(event){if(!event.target){return;}\\nvar el;if(matches(event.target,\\'a.share-telegram\\')){el=event.target;}else if(event.target.parentNode&&matches(event.target.parentNode,\\'a.share-telegram\\')){el=event.target.parentNode;}\\nif(el){event.preventDefault();if(typeof windowOpen!==\\'undefined\\'){windowOpen.close();}\\nwindowOpen=window.open(el.getAttribute(\\'href\\'),\\'wpcomtelegram\\',\\'menubar=1,resizable=1,width=450,height=450\\');return false;}});})();var windowOpen;(function(){function matches(el,sel){return!!(el.matches&&el.matches(sel)||el.msMatchesSelector&&el.msMatchesSelector(sel));}\\ndocument.body.addEventListener(\\'click\\',function(event){if(!event.target){return;}\\nvar el;if(matches(event.target,\\'a.share-mastodon\\')){el=event.target;}else if(event.target.parentNode&&matches(event.target.parentNode,\\'a.share-mastodon\\')){el=event.target.parentNode;}\\nif(el){event.preventDefault();if(typeof windowOpen!==\\'undefined\\'){windowOpen.close();}\\nwindowOpen=window.open(el.getAttribute(\\'href\\'),\\'wpcommastodon\\',\\'menubar=1,resizable=1,width=460,height=400\\');return false;}});})();var windowOpen;(function(){function matches(el,sel){return!!(el.matches&&el.matches(sel)||el.msMatchesSelector&&el.msMatchesSelector(sel));}\\ndocument.body.addEventListener(\\'click\\',function(event){if(!event.target){return;}\\nvar el;if(matches(event.target,\\'a.share-pocket\\')){el=event.target;}else if(event.target.parentNode&&matches(event.target.parentNode,\\'a.share-pocket\\')){el=event.target.parentNode;}\\nif(el){event.preventDefault();if(typeof windowOpen!==\\'undefined\\'){windowOpen.close();}\\nwindowOpen=window.open(el.getAttribute(\\'href\\'),\\'wpcompocket\\',\\'menubar=1,resizable=1,width=450,height=450\\');return false;}});})();\\n  &lt;/script&gt;\\n  &lt;iframe id=\"likes-master\" name=\"likes-master\" scrolling=\"no\" src=\"https://widgets.wp.com/likes/master.html?ver=202330#ver=202330\" style=\"display:none;\"&gt;\\n  &lt;/iframe&gt;\\n  &lt;div id=\"likes-other-gravatars\"&gt;\\n   &lt;div class=\"likes-text\"&gt;\\n    &lt;span&gt;\\n     %d\\n    &lt;/span&gt;\\n    bloggers like this:\\n   &lt;/div&gt;\\n   &lt;ul class=\"wpl-avatars sd-like-gravatars\"&gt;\\n   &lt;/ul&gt;\\n  &lt;/div&gt;\\n  &lt;script&gt;\\n   function openNav(){const wid=document.getElementById(\"mySidenav\").style.width;if(wid==\\'220px\\'){document.getElementById(\"mySidenav\").style.width=\"0px\";document.getElementById(\"main\").style.marginRight=\"0px\";}else{document.getElementById(\"mySidenav\").style.width=\"220px\";document.getElementById(\"main\").style.marginRight=\"220px\";}}\\nfunction closeNav(){const wid=document.getElementById(\"mySidenav\").style.width;document.getElementById(\"mySidenav\").style.width=\"0px\";document.getElementById(\"main\").style.marginRight=\"0px\";}\\n  &lt;/script&gt;\\n  &lt;script&gt;\\n   window.w3tc_lazyload=1,window.lazyLoadOptions={elements_selector:\".lazy\",callback_loaded:function(t){var e;try{e=new CustomEvent(\"w3tc_lazyload_loaded\",{detail:{e:t}})}catch(a){(e=document.createEvent(\"CustomEvent\")).initCustomEvent(\"w3tc_lazyload_loaded\",!1,!1,{e:t})}window.dispatchEvent(e)}}\\n  &lt;/script&gt;\\n  &lt;script async=\"\" src=\"https://dailytrust.com/wp-content/cache/minify/1615d.js\"&gt;\\n  &lt;/script&gt;\\n  &lt;script crossorigin=\"anonymous\" data-cf-beacon=\\'{\"rayId\":\"7eba0ec02b66b88e\",\"version\":\"2023.4.0\",\"b\":1,\"token\":\"61e9828cd12c41eebd5ea34170988570\",\"si\":100}\\' defer=\"\" integrity=\"sha512-DI3rPuZDcpH/mSGyN22erN5QFnhl760f50/te7FTIYxodEF8jJnSFnfnmG/c+osmIQemvUrnBtxnMpNdzvx1/g==\" src=\"https://static.cloudflareinsights.com/beacon.min.js/v2cb3a2ab87c5498db5ce7e6608cf55231689030342039\"&gt;\\n  &lt;/script&gt;\\n &lt;/body&gt;\\n&lt;/html&gt;'\n\n\nThe url of the page is first declared, then the response object is created using the get method of requests. Afterwards, the content attribute of the response object is passed to BeautifulSoup to create a soup object that we can use to navigate through the content of the web page. The prettify method is used to show the basic structure of the web page as this can help in pinpointing the exact content needed, in our case, the paragraphs. \nTo keep things simple for now, this is the approach I followed:\n\nparagraphs =  soup.find_all('p')\nprint(soup.title)\nfor paragraph in paragraphs:\n  if not paragraph.find('a'):\n    print(paragraph.get_text())\n\n&lt;title&gt;Last Saturday polls raised issues that require immediate solutions – INEC - Daily Trust&lt;/title&gt;\nThe Independent National Electoral Commission (INEC) has admitted that the last Saturday presidential and National Assembly elections raises a number of issues that require immediate,…\nThe Independent National Electoral Commission (INEC) has admitted that the last Saturday presidential and National Assembly elections raises a number of issues that require immediate, medium and long-term solutions.\nINEC Chairman, Prof. Mahmood Yakubu, said this on Saturday in Abuja at a meeting with the Resident Electoral commissioners (RECs) held at the INEC headquarters, Abuja.\nHe said, “It is imperative to review performance and assess preparations. No doubt, last week’s national elections raised a number of issues that require immediate, medium, and long-term solutions.\n“The planning for the election was painstakingly done. However, its implementation came with challenges, some of them unforeseen. The issues of logistics, election technology, behaviour of some election personnel at different levels, attitude of some party agents and supporters added to the extremely challenging environment in which elections are usually held in Nigeria.”\nThe INEC boss said he appreciated the sacrifice and doggedness of Nigerians and the dignity and maturity displayed by political leaders even in the context of divergent views about the election.\nYakubu said that a lot of lessons had been learnt and of immediate concern to the commission is how the identified challenges can be addressed ahead of the next Saturday’s governorship and state houses of aassembly elections in 28 states.\nThe chairman also said that for the last Saturday’s elections, winners had been declared for 423 national legislative seats while supplementary elections would be held in 46 constituencies.\nAccording to him, in the Senate, 98 out of 109 seats had been declared, with seven political parties winning senatorial seats while in the House of Representatives, 325 out of 360 seats had been won by eight political parties.\nHe noted that in terms of party representation, this is the most diverse National Assembly since 1999.\nHe said that in the Senate APC has  57 seats, APGA 1, LP 6, NNPP 2, PDP 29, SDP 2, YPP 1; while in the House of Representatives, ADC 2, APC 162, APGA 4, LP 34, NNPP 18, PDP 102, SDP 2, YPP 1.\nHe said, “Certificates of Return will be presented to Senators-elect on Tuesday 7th March 2023 at 11.00am at the National Collation Centre (the International Conference Centre), Abuja, while Members of the House of Representatives-elect will receive theirs the following day, Wednesday 8th March 2023, at 11.00am at the same venue.\n“However, for effective crowd management, each Senator/Member-elect should be accompanied by a maximum of two guests.”\nDollar payments are now available for ALL Nigerians. Our clients earn about $5,000 - $10,000 acquiring premium domains. Read testimonials from others who have benefited. Click here to start\nLearn How to Relocate to Canada & Become a Permanent Resident With Your Family. No Need for IELTS & Agent Wahala. Click to Learn More\nI Made N1 Million Naira Monthly Using this WhatsApp Marketing Strategy. Follow My Steps & Make More Sales. Click to Learn More\n\n\nFirst, the find_all() method is used to get all the p tags as a list and this is assigned to the paragraphs variable. Then for the main target of this article, I print the title attribute of the soup object and then loop through each item in the paragraphs variable and print, this gives the entire content of the article. I discovered that the hyperlinked titles of related articles where included and I removed them by excluding all p tags that contained a tags. \nIn conclusion, web scraping is highly useful to a data scientist as it automates getting content from the internet. One does not have to cut and paste long pieces of text or other contents."
  },
  {
    "objectID": "posts/writearticles/index.html",
    "href": "posts/writearticles/index.html",
    "title": "Writing a Scientific Paper",
    "section": "",
    "text": "In this post, I aim to summarize the key points from the journal article “How to Write a Scientific Article” by Barbara J. Hoogenboom and Robert C. Manske, which offers practical advice for successfully writing and submitting scientific manuscripts to peer-reviewed journals. The authors stress the significance of clear communication and effective writing skills in advancing evidence-based practice and scientific thinking. They cover various aspects of manuscript preparation, general writing tips, and important content sections.\nThe introduction emphasizes the importance of publishing research findings in peer-reviewed journals to positively impact the scientific community and clinical decision-making. The authors recognize that writing a scientific paper can be challenging, particularly for inexperienced authors due to various barriers. Nevertheless, they assert that effective writing skills can be honed with practice.\nThe article provides valuable tips on manuscript preparation and advises authors to identify a specific target journal and adhere to its submission criteria. The recommended IMRaD format (Introduction, Methods, Results, and Discussion) helps structure scientific manuscripts.\nThe authors emphasize the need for clear and accurate expression of ideas and research information. They caution against excessive use of jargon and recommend avoiding first-person language, which might contradict certain viewpoints from other sources.\nSeeking a reading mentor and obtaining feedback on the manuscript before submission are encouraged, particularly for novice writers.\nThe article highlights the significance of using figures, graphics, and videos to enhance research presentations. Proper citation and avoiding plagiarism are deemed essential in scientific writing.\nThe content section discusses key elements like the abstract, introduction, review of literature, methods, results, discussion, and conclusions. The abstract is deemed critical as it often determines whether readers will continue reading the paper. The introduction should clearly state the research question and justify the study’s significance.\nThe methods section should provide sufficient detail for replication of the study, including mentioning ethical approval. The results section should present findings without interpretation, while the discussion section should contextualize results by comparing them with existing evidence.\nIn conclusion, the article encourages scholarly writing as a rewarding endeavor that contributes to scientific knowledge and evidence-based practice. It provides valuable insights and practical tips for authors, especially those new to scientific writing, to successfully navigate the publication process and produce high-quality manuscripts.\nIt’s worth noting that while most of the article aligns with what I learned in a “Writing in the Sciences” MOOC on Coursera, there may be some differences in opinions regarding the use of first-person language, as it can vary depending on the journal’s preferences and the author’s writing style.\n\nReference\nHoogenboom BJ, Manske RC. How to write a scientific article. Int J Sports Phys Ther. 2012 Oct;7(5):512-7. PMID: 23091783; PMCID: PMC3474301."
  },
  {
    "objectID": "posts/data-analytics/index.html",
    "href": "posts/data-analytics/index.html",
    "title": "Behind the Scenes: Testing the DeepLearning.AI Data Analytics Professional Certificate",
    "section": "",
    "text": "I’m excited to share that I’ve been recognized as a Top Tester for the third course, Python for Data Analytics, in the new DeepLearning.AI Data Analytics Professional Certificate!\nThis marks my third Top Tester award in the specialization — having previously contributed to the development of the first two courses as well. With two more courses still ahead, the journey so far has been incredibly rewarding, both in contributing to the learner experience and strengthening my own skills along the way.\nOne aspect I deeply appreciate about this specialization is how thoughtfully it’s structured:\n\nCourses 1 and 2 build a strong foundation in analytics fundamentals, starting with spreadsheets and data thinking.\nCourse 3 makes a natural transition into Python, giving learners practical, hands-on experience with real-world analytics projects — covering topics like retail sales trends, environmental data, and more.\n\nWhat makes the third course particularly special is its emphasis on realistic coding challenges. It doesn’t just walk you through polished examples; it encourages troubleshooting and critical thinking — skills that are essential for real-world analytics work.\nLooking ahead, Courses 4 and 5 are set to dive even deeper into advanced analytics topics and tools, making this specialization an excellent pathway for anyone serious about becoming a data professional.\nI’m also honored to have been recognized as a mentor for the course, thanks to the deep understanding I’ve developed through testing and feedback.\nA huge thank you to the entire DeepLearning.AI team for the opportunity to contribute! If you’re looking to start or strengthen your career in data analytics, I highly recommend checking out this specialization. It’s been a fantastic learning experience, and I can’t wait to see what’s next!"
  }
]