<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>ailearningloop</title>
<link>https://lukmanaj.github.io/ailearningloop/</link>
<atom:link href="https://lukmanaj.github.io/ailearningloop/index.xml" rel="self" type="application/rss+xml"/>
<description>Venturing Beyond My Roots: An AI Odyssey Unfolds for This Non-Tech Trailblazer!</description>
<generator>quarto-1.5.56</generator>
<lastBuildDate>Sun, 25 Aug 2024 00:00:00 GMT</lastBuildDate>
<item>
  <title>Understanding Responsible AI: A Deep Dive into Ethical AI Development</title>
  <dc:creator>Lukman Aliyu Jibril</dc:creator>
  <link>https://lukmanaj.github.io/ailearningloop/posts/responsible-ai/</link>
  <description><![CDATA[ 





<p>Artificial intelligence (AI) is rapidly advancing, transforming industries from healthcare to transportation. With its growing influence, there is an increasing need to ensure that AI is developed and deployed responsibly. This is where Responsible AI (RAI) comes in—a framework designed to guide the ethical use of AI, ensuring fairness, transparency, accountability, and respect for human rights. In this article, I explore the key concepts of Responsible AI, its importance, and how organizations can implement strategies that align AI development with ethical principles.</p>
<p>This article is primarily a summary of the content I learnt in the course <a href="https://app.datacamp.com/learn/courses/responsible-ai-practices">Responsible AI Practices</a> on DataCamp. I enrolled in this course after I had the privilege of reviewing the notebooks for the Responsible AI practicals for the upcoming Deep Learning Indaba 2024 in Dakar, Senegal. This combination of learning and practical experience has given me a well-rounded perspective on the principles of Responsible AI, which I am excited to share. I must also thank Arewa Data Science Academy for giving me the opportunity to enjoy a Data Camp scholarship, which allowed me to learn several concepts and earn relevant certifications for free.</p>
<section id="defining-responsible-ai" class="level2">
<h2 class="anchored" data-anchor-id="defining-responsible-ai">Defining Responsible AI</h2>
<p>Responsible AI refers to the practice of designing, developing, and deploying AI systems with an understanding of their ethical implications and societal impacts. It is about ensuring that AI systems are fair, transparent, accountable, and respectful of privacy and human rights. The goal is to create AI that benefits society without causing harm, focusing not only on the technology itself but also on the values guiding its use.</p>
</section>
<section id="why-responsible-ai-is-crucial" class="level2">
<h2 class="anchored" data-anchor-id="why-responsible-ai-is-crucial">Why Responsible AI is Crucial</h2>
<p>AI’s ability to process vast amounts of data and learn from it is revolutionizing fields like healthcare, education, and finance. However, these advancements come with risks. AI can perpetuate biases, invade privacy, or be used unethically if not developed responsibly. For example, facial recognition systems have been found to have higher error rates for people of color, leading to discriminatory outcomes. Similarly, automated hiring systems can reflect and amplify existing biases in the data they are trained on.</p>
<p>These risks highlight the need for Responsible AI practices that mitigate negative impacts and promote fairness and inclusivity. The stakes are high, making Responsible AI a critical aspect of any AI development strategy.</p>
</section>
<section id="key-principles-of-responsible-ai" class="level2">
<h2 class="anchored" data-anchor-id="key-principles-of-responsible-ai">Key Principles of Responsible AI</h2>
<p>The principles of Responsible AI are based on globally recognized guidelines, such as those from the Organisation for Economic Co-operation and Development (OECD). Key principles include:</p>
<ol type="1">
<li>Transparency and Explainability: AI systems should be designed so that their decisions can be understood and explained.</li>
<li>Fairness and Non-Discrimination: AI should be free from biases and discrimination, promoting inclusivity and equity.</li>
<li>Robustness and Safety: AI systems must be reliable, secure, and capable of operating safely in various conditions.</li>
<li>Privacy and Data Governance: AI should respect privacy and ensure secure data handling.</li>
<li>Accountability: Organizations should be accountable for the outcomes of their AI systems.</li>
<li>Inclusive Growth and Sustainability: AI should bridge digital divides and contribute to sustainable development.</li>
</ol>
</section>
<section id="responsible-ai-vs.-ai-ethics" class="level2">
<h2 class="anchored" data-anchor-id="responsible-ai-vs.-ai-ethics">Responsible AI vs.&nbsp;AI Ethics</h2>
<p>While Responsible AI and AI Ethics are closely related, they are not identical. AI Ethics focuses broadly on the philosophical and moral questions surrounding AI, while Responsible AI is more practical, emphasizing the implementation of ethical considerations in measurable ways. It involves using clear metrics and frameworks to ensure that AI systems align with ethical principles.</p>
</section>
<section id="the-global-regulatory-landscape" class="level2">
<h2 class="anchored" data-anchor-id="the-global-regulatory-landscape">The Global Regulatory Landscape</h2>
<p>Regulation of AI is still evolving, but several regions are leading the way. The European Union’s AI Act classifies AI systems based on risk levels and imposes stringent requirements on high-risk applications. In the United States, initiatives like the AI Bill of Rights and various state-level regulations are shaping AI governance. Countries like Canada, the UK, and China are also developing frameworks tailored to their societal and economic needs.</p>
<p>These regulatory efforts share a common goal: to protect human rights while fostering innovation in AI. As AI technology continues to advance, these regulations will play a critical role in ensuring that AI systems are used responsibly.</p>
</section>
<section id="implementing-responsible-ai-an-8-step-approach" class="level2">
<h2 class="anchored" data-anchor-id="implementing-responsible-ai-an-8-step-approach">Implementing Responsible AI: An 8-Step Approach</h2>
<p>For organizations looking to implement Responsible AI, the following eight-step approach is recommended:</p>
<ol type="1">
<li>Embrace AI Governance: Secure organizational commitment to align AI development with ethical standards and business goals.</li>
<li>Build an AI Playbook: Develop a strategic plan that documents AI governance practices, keeping it flexible and up to date.</li>
<li>Identify Key Stakeholders: Engage both internal and external stakeholders to ensure inclusivity and address diverse perspectives.</li>
<li>Leverage Internal Support Mechanisms: Use ethics boards, training programs, and AI squads to guide ethical AI development.</li>
<li>Embrace a Multi-Stakeholder Approach: Include community members, industry experts, and policymakers in AI governance.</li>
<li>Explore Additional Responsible Behavior Indicators: Demonstrate a commitment to Responsible AI through certifications, CSR standards, and transparent ESG reporting.</li>
<li>Implement Governance Tools: Develop guidelines and procedures that uphold Responsible AI principles.</li>
<li>Monitor, Audit, and Evaluate AI Systems: Continuously monitor and audit AI systems to ensure they remain aligned with ethical standards.</li>
</ol>
</section>
<section id="the-importance-of-diversity-in-ai-development" class="level2">
<h2 class="anchored" data-anchor-id="the-importance-of-diversity-in-ai-development">The Importance of Diversity in AI Development</h2>
<p>Diversity, equity, and inclusion (DE&amp;I) are essential for creating fair and unbiased AI systems. A lack of diversity in AI development teams can lead to systems that reflect and reinforce societal biases. For instance, facial recognition systems and hiring algorithms have been shown to produce biased results when developed without diverse perspectives.</p>
<p>Research shows that diverse teams are more innovative and perform better. Prioritizing DE&amp;I in AI development is not just ethically sound but also beneficial for business, leading to more robust and inclusive AI solutions.</p>
</section>
<section id="conclusion-the-path-forward-for-responsible-ai" class="level2">
<h2 class="anchored" data-anchor-id="conclusion-the-path-forward-for-responsible-ai">Conclusion: The Path Forward for Responsible AI</h2>
<p>Responsible AI is a continuous journey that requires ongoing evaluation, monitoring, and adaptation. My recent experience reviewing content for the upcoming Deep Learning Indaba 2024 reinforced the importance of this topic. The principles I learned in the DataCamp course on Responsible AI practices, coupled with my practical experience, have deepened my understanding of the need for ethical AI governance.</p>
<p>As AI continues to evolve, it is our collective responsibility to guide its development in ways that reflect our highest ethical standards. By embracing Responsible AI principles, we can ensure that AI serves humanity in a way that is fair, transparent, and accountable, ultimately contributing to a more equitable and sustainable future.</p>


</section>

 ]]></description>
  <category>deeplearning indaba</category>
  <category>responsible ai</category>
  <category>datacamp</category>
  <guid>https://lukmanaj.github.io/ailearningloop/posts/responsible-ai/</guid>
  <pubDate>Sun, 25 Aug 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>The Power of Persistence: How 365 Days of Consistency Transformed My Learning Journey</title>
  <dc:creator>Lukman Aliyu Jibril</dc:creator>
  <link>https://lukmanaj.github.io/ailearningloop/posts/forum-devotee/</link>
  <description><![CDATA[ 





<p>After 365 consecutive days of active participation, I’m thrilled to announce that I’ve achieved Devotee status on the DeepLearning.AI forum. This milestone is more than just a badge—it shows the power of unwavering commitment and the compound effect of daily habits.</p>
<p><img src="https://lukmanaj.github.io/ailearningloop/posts/forum-devotee/devotee.png" class="img-fluid"></p>
<p>The journey to Devotee status is as unforgiving as it is rewarding. Missing even a single day means starting over from scratch. Imagine the dedication required to maintain this streak for an entire year, knowing that a momentary lapse on day 364 would reset your progress to zero. It’s this level of consistency that separates casual participants from true devotees.</p>
<p>My path to this achievement began with smaller milestones: the 10-day Enthusiast badge, followed by the 100-day Aficionado badge. Each step reinforced my commitment and built momentum towards the ultimate goal. Now, as only the fifth person in over four years to reach Devotee status, I find myself in the company of the forum’s most dedicated members.</p>
<p>But this streak is just one facet of a larger commitment to personal growth through consistent effort. Parallel to my DeepLearning.AI journey, I’m maintaining a 196-day streak on DataCamp and a 261 day streak on GitHub. These aren’t just numbers; they represent a fundamental shift in how I approach learning and self-improvement.</p>
<p>The concept of “atomic habits,” popularized by James Clear, has been instrumental in my approach. The idea is simple yet powerful: tiny, consistent changes accumulate over time, leading to significant personal transformation. By showing up every day, even when motivation wanes, I’m not just learning new skills—I’m reshaping my identity into someone who pursues growth relentlessly.</p>
<p>This journey has taught me several valuable lessons:</p>
<ol type="1">
<li><p>Consistency trumps intensity: Small, daily efforts are more impactful than sporadic bursts of activity.</p></li>
<li><p>Progress compounds: What seems like slow progress early on accelerates as habits become ingrained.</p></li>
<li><p>Identity-based habits stick: By viewing myself as a “lifelong learner,” maintaining these streaks becomes natural.</p></li>
<li><p>Community matters: The DeepLearning.AI forum and other platforms provide accountability and support.</p></li>
</ol>
<p>Moving forward, I’m committed to channeling my energy into endeavors that contribute to my professional and personal growth. This means setting ambitious targets, minimizing distractions like excessive social media use, and continuously reassessing my habits to ensure they align with my long-term goals.</p>
<p>As I reflect on this achievement, I’m filled with gratitude for the guidance and opportunities that have brought me here. I’m also excited about the future, knowing that the habits I’ve built will serve as a foundation for even greater accomplishments.</p>
<p>To those embarking on their own journey of consistent self-improvement, remember: every day is an opportunity to take a step forward. The path may be challenging, but the rewards—in knowledge gained, skills developed, and personal growth achieved—are immeasurable.</p>
<p>Alhamdulillah for this milestone, and may Allah continue to guide and bless our efforts towards meaningful growth and contribution.</p>



 ]]></description>
  <category>deeplearning-ai</category>
  <guid>https://lukmanaj.github.io/ailearningloop/posts/forum-devotee/</guid>
  <pubDate>Mon, 19 Aug 2024 00:00:00 GMT</pubDate>
  <media:content url="https://lukmanaj.github.io/ailearningloop/posts/forum-devotee/devotee.png" medium="image" type="image/png" height="99" width="144"/>
</item>
<item>
  <title>A Journey from Learner to Leader: My Deep Learning.AI and Coursera Adventure</title>
  <dc:creator>Lukman Aliyu Jibril</dc:creator>
  <link>https://lukmanaj.github.io/ailearningloop/posts/tensorflow-mentor/</link>
  <description><![CDATA[ 





<p>On August 14, 2024, I reached a significant milestone in my AI career: I was officially designated as a mentor for the TensorFlow Developer Certificate program on Deep Learning.AI and Coursera. This comprehensive certification, consisting of four courses, introduces learners to the powerful TensorFlow framework. As I reflect on this achievement, I’m filled with pride and gratitude for how far I’ve come in such a short time.</p>
<section id="the-tensorflow-developer-professional-certificate" class="level2">
<h2 class="anchored" data-anchor-id="the-tensorflow-developer-professional-certificate">The TensorFlow Developer Professional Certificate</h2>
<p>The certificate program I now mentor comprises four essential courses:</p>
<ol type="1">
<li>Introduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning</li>
<li>Convolutional Neural Networks in TensorFlow</li>
<li>Natural Language Processing in TensorFlow</li>
<li>Sequences, Time Series and Prediction</li>
</ol>
<p>These courses cover a broad spectrum of AI applications, from basic neural networks to advanced techniques in computer vision, natural language processing, and time series analysis.</p>
</section>
<section id="a-serendipitous-beginning" class="level2">
<h2 class="anchored" data-anchor-id="a-serendipitous-beginning">A Serendipitous Beginning</h2>
<p>My AI learning journey began with the Arewa Data Science Academy fellowship, a program that holds a special place in my heart. Simultaneously, I embarked on Andrew Ng’s Machine Learning Specialization on Coursera. This coincidence set the stage for my rapid progression in the AI community.</p>
</section>
<section id="rising-through-the-ranks" class="level2">
<h2 class="anchored" data-anchor-id="rising-through-the-ranks">Rising Through the Ranks</h2>
<p>My involvement with Deep Learning.AI and Coursera deepened quickly:</p>
<ul>
<li><p><strong>Forum Moderator:</strong> In November 2023, just a few months into my journey, I became a moderator at the DeepLearning.AI forum, helping to guide discussions and support learners tackling challenges in various courses offered in the community.</p></li>
<li><p><strong>Course Tester:</strong> I was invited to be part of the testing group for a major course upgrade, partly due to my active contribution as a moderator, where we transitioned from an older version of TensorFlow to the cutting-edge TensorFlow 2.16. My insightful feedback contributed to refining the course content and user experience across all four courses.</p></li>
</ul>
<p><img src="https://lukmanaj.github.io/ailearningloop/posts/tensorflow-mentor/tester.png" class="img-fluid"></p>
<ul>
<li><strong>Mentor:</strong> The crowning achievement came in August 2024, when I was offered the opportunity to become a mentor for the TensorFlow Developer Certificate courses, in addition to my moderator role. This position allows me to guide learners through the intricacies of TensorFlow, from basic concepts to advanced applications in CNN, NLP, and time series prediction.</li>
</ul>
<p>This rapid transition from learner to contributor to mentor and moderator has been both humbling and exhilarating.</p>
</section>
<section id="celebrating-milestones-and-looking-ahead" class="level2">
<h2 class="anchored" data-anchor-id="celebrating-milestones-and-looking-ahead">Celebrating Milestones and Looking Ahead</h2>
<p>As I continue to contribute to the AI community, I’m committed to making a lasting impact in all the places I volunteer, especially at Arewa Data Science Academy, where my journey began. Each step forward reminds me of the importance of celebrating small victories while keeping an eye on future goals.</p>
<p>My new role as a TensorFlow Developer Certificate mentor, alongside my moderator responsibilities, is not just a personal achievement; it’s an opportunity to give back to the community that has nurtured my growth. It’s a chance to inspire and guide the next generation of AI enthusiasts and developers across various domains of machine learning and deep learning.</p>
</section>
<section id="a-message-to-aspiring-ai-developers" class="level2">
<h2 class="anchored" data-anchor-id="a-message-to-aspiring-ai-developers">A Message to Aspiring AI Developers</h2>
<p>As I forge ahead in pursuing my dreams in the AI field, I’m reminded of the power of persistence, continuous learning, and community support. To all aspiring AI developers out there: keep pushing, keep learning, and remember that today’s learner can be tomorrow’s leader.</p>
<p>The path from novice to mentor can be shorter than you think. With dedication and the right opportunities, you can quickly find yourself in a position to guide others. Embrace every chance to learn, contribute, and grow within the AI community.</p>
<p>Here’s to the journey ahead, filled with neural networks, convolutional layers, natural language models, time series predictions, and the endless possibilities of artificial intelligence!</p>


</section>

 ]]></description>
  <category>tensorflow</category>
  <category>arewads</category>
  <category>deeplearning-ai</category>
  <guid>https://lukmanaj.github.io/ailearningloop/posts/tensorflow-mentor/</guid>
  <pubDate>Sun, 18 Aug 2024 00:00:00 GMT</pubDate>
  <media:content url="https://lukmanaj.github.io/ailearningloop/posts/tensorflow-mentor/mentor-mail.png" medium="image" type="image/png" height="91" width="144"/>
</item>
<item>
  <title>My Contribution to Quarto-web: A Small Edit with a Big Impact</title>
  <dc:creator>Lukman Aliyu Jibril</dc:creator>
  <link>https://lukmanaj.github.io/ailearningloop/posts/contributing-to-quarto/</link>
  <description><![CDATA[ 





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>I’m thrilled to share my recent experience contributing to the Quarto-web open-source project. Although it might seem like a small change, my contribution aimed to improve the clarity and readability of the documentation, ensuring users have a smoother experience when learning about Quarto. I’ve always appreciated Quarto since I first learned about it from <a href="https://shmuhammadd.github.io/">Dr.&nbsp;Shamsuddeen Muhammad</a> during the first cohort of the Arewa Data Science Fellowship. I continued practicing, and now I am proudly a contributor. I look forward to making more impactful contributions in the future.</p>
</section>
<section id="spotting-the-issue-a-simple-typo-and-punctuation-fix" class="level2">
<h2 class="anchored" data-anchor-id="spotting-the-issue-a-simple-typo-and-punctuation-fix">Spotting the Issue: A Simple Typo and Punctuation Fix</h2>
<p>While exploring the Quarto Dashboards documentation, specifically the <code>R Graphics</code> section tip on this <a href="https://quarto.org/docs/dashboards/data-display.html">page</a>, I noticed a minor typo—‘than’ was mistakenly written as ‘that’. Additionally, there were a few missing commas, which could potentially confuse readers.</p>
<p>These might seem like minor issues, but clear and accurate documentation is crucial in helping users understand and utilize software tools effectively. I decided to take action and correct these mistakes to enhance the user experience.</p>
</section>
<section id="the-pull-request-a-quick-fix" class="level2">
<h2 class="anchored" data-anchor-id="the-pull-request-a-quick-fix">The Pull Request: A Quick Fix</h2>
<p>I submitted a pull request (#1214) to update the _plots-interactive.md file, which contained the typo and punctuation errors. The changes were straightforward:</p>
<ul>
<li>Corrected the typo from ‘that’ to ‘than’.</li>
<li>Added a few missing commas for better clarity and readability.</li>
</ul>
<p>These edits were minor but important, as they helped ensure the documentation was accurate and easy to follow.</p>
</section>
<section id="merged-and-celebrated-the-joy-of-contributing" class="level2">
<h2 class="anchored" data-anchor-id="merged-and-celebrated-the-joy-of-contributing">Merged and Celebrated: The Joy of Contributing</h2>
<p>To my delight, my pull request was reviewed and merged by the maintainers. You can view the merged changes in the pull request <a href="https://github.com/quarto-dev/quarto-web/pull/1214">here</a>. It was a small contribution, but seeing my changes go live was incredibly rewarding. It was a reminder that even small contributions can make a big difference in the open-source community.</p>
</section>
<section id="reflections-on-my-contribution" class="level2">
<h2 class="anchored" data-anchor-id="reflections-on-my-contribution">Reflections on My Contribution</h2>
<p>This experience has taught me a few valuable lessons:</p>
<ul>
<li>Attention to Detail Matters: Even small errors in documentation can lead to misunderstandings. Taking the time to correct them helps improve the overall quality of the project.</li>
<li>Every Contribution Counts: No matter the size, every contribution to an open-source project helps make it better. It’s about being part of a community effort.</li>
<li>Learning and Growing: This was my first contribution to Quarto-web, and it has encouraged me to continue participating in open-source projects. There’s always something new to learn and areas to improve.</li>
</ul>
</section>
<section id="looking-forward" class="level2">
<h2 class="anchored" data-anchor-id="looking-forward">Looking Forward</h2>
<p>I’m excited to continue exploring opportunities in the open-source world. Whether it’s fixing typos, adding features, or writing documentation, every bit helps the community grow and thrive. If you’re interested in getting involved, I encourage you to look for ways to contribute, no matter how small they may seem.</p>
<p>Thank you for reading, and feel free to follow me on the social media links in my about page. Happy coding!</p>


</section>

 ]]></description>
  <category>quarto</category>
  <category>quarto-web</category>
  <guid>https://lukmanaj.github.io/ailearningloop/posts/contributing-to-quarto/</guid>
  <pubDate>Sat, 27 Jul 2024 00:00:00 GMT</pubDate>
  <media:content url="https://lukmanaj.github.io/ailearningloop/posts/contributing-to-quarto/contribution.png" medium="image" type="image/png" height="92" width="144"/>
</item>
<item>
  <title>Metrics for Medical Diagnosis Evaluation</title>
  <dc:creator>Lukman Aliyu Jibril</dc:creator>
  <link>https://lukmanaj.github.io/ailearningloop/posts/medical-eval/</link>
  <description><![CDATA[ 





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Medical diagnosis is a multifaceted endeavour that relies on a comprehensive assessment of various factors, including symptoms, tests, and medical history. Conditional probability, which calculates the likelihood of an event occurring given another event, plays a pivotal role in evaluating the effectiveness of medical diagnoses. In the context of healthcare, this involves estimating the probability of a patient having a disease, given that they have tested positive for it.</p>
</section>
<section id="understanding-accuracy" class="level2">
<h2 class="anchored" data-anchor-id="understanding-accuracy">Understanding Accuracy</h2>
<p>One of the most commonly used metrics in medical diagnosis is accuracy. It gauges the overall proportion of accurate diagnoses, derived from correctly identifying patients with or without a specific ailment. Accuracy can be mathematically expressed as a combination of sensitivity and specificity, both of which are vital metrics.</p>
<p><img src="https://latex.codecogs.com/png.latex?Accuracy%20=%20Sensitivity%20%5Ctimes%20Prevalence%20+%20Specificity%20%5Ctimes%20(1%20-%20Prevalence)"></p>
</section>
<section id="sensitivity-and-specificity" class="level2">
<h2 class="anchored" data-anchor-id="sensitivity-and-specificity">Sensitivity and Specificity</h2>
<p>Sensitivity measures the ability of a diagnostic method or model to correctly identify patients who have the disease, while specificity measures its ability to accurately identify those without the disease. These metrics can also be understood in terms of conditional probabilities, simplifying them to the probability of a positive or negative test result given a patient’s disease status.</p>
<p><img src="https://latex.codecogs.com/png.latex?Sensitivity%20=%20%5Cdfrac%7B%5C,number%5C,of%5C,positive%5C,and%5C,disease%7D%7B%5C,number%5C,of%5C,disease%7D"></p>
<p><img src="https://latex.codecogs.com/png.latex?Specificity%20=%20%5Cdfrac%7B%5C,number%5C,of%5C,negative%5C,and%5C,normal%7D%7B%5C,number%5C,of%5C,normal%7D"></p>
<p>In terms of conditional probabilities:</p>
<p><img src="https://latex.codecogs.com/png.latex?Sensitivity%20=%20P(positive%7Cdisease)"> <img src="https://latex.codecogs.com/png.latex?Specificity%20=%20P(negative%7Cnormal)"></p>
</section>
<section id="positive-predictive-value-and-negative-predictive-value" class="level2">
<h2 class="anchored" data-anchor-id="positive-predictive-value-and-negative-predictive-value">Positive Predictive Value and Negative Predictive Value</h2>
<p>To assess the likelihood of a patient having or not having the disease based on their test results, we employ positive predictive value (PPV) and negative predictive value (NPV). These metrics provide insight into the probabilities of disease presence or absence given a positive or negative test result, respectively. This is more useful and practical than just sensitivity and specificity</p>
<p><img src="https://latex.codecogs.com/png.latex?PPV%20=%20%5Cdfrac%7B%5C,number%5C,of%5C,positive%5C,and%5C,disease%7D%7B%5C,number%5C,of%5C,positive%7D"></p>
<p><img src="https://latex.codecogs.com/png.latex?NPV%20=%20%5Cdfrac%7B%5C,number%5C,of%5C,negative%5C,and%5C,normal%7D%7B%5C,number%5C,of%5C,negative%7D"></p>
<section id="leveraging-bayes-rule" class="level3">
<h3 class="anchored" data-anchor-id="leveraging-bayes-rule">Leveraging Bayes’ Rule:</h3>
<p>To calculate PPV and NPV, Bayes’ rule, a formula relating conditional probabilities, is employed. This rule offers a nuanced perspective by taking into account sensitivity, specificity, and prevalence—the proportion of patients with the disease in the population.</p>
<p>Using conditional probabilities, we write:</p>
<p><img src="https://latex.codecogs.com/png.latex?PPV%20=%20P(disease%7Cpositive)"></p>
<p><img src="https://latex.codecogs.com/png.latex?NPV%20=%20P(normal%7Cnegative)"></p>
</section>
</section>
<section id="confusion-matrix" class="level2">
<h2 class="anchored" data-anchor-id="confusion-matrix">Confusion Matrix</h2>
<p>A confusion matrix serves as a valuable tool to visualize the performance of a diagnostic method or model. It displays true positives, true negatives, false positives, and false negatives for different thresholds or cut-off points, which can be adjusted to modify the diagnostic performance metrics.</p>
<p>A confusion matrix can be represented as:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th>Positive</th>
<th>Negative</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Disease</td>
<td>TP</td>
<td>FN</td>
</tr>
<tr class="even">
<td>Normal</td>
<td>FP</td>
<td>TN</td>
</tr>
</tbody>
</table>
<p>where: TP = True Positives (number of)</p>
<p>FP = False Positives</p>
<p>FN = False Negatives</p>
<p>TN = True Negatives</p>
</section>
<section id="precision-and-recall" class="level2">
<h2 class="anchored" data-anchor-id="precision-and-recall">Precision and Recall</h2>
<p>Precision measures the proportion of correctly identified positive test results, while recall assesses the proportion of patients with the disease that are accurately identified. These metrics are also synonymous with positive predictive value (PPV) and sensitivity, respectively.</p>
<p><img src="https://latex.codecogs.com/png.latex?Precision%20=%20%5Cdfrac%7BTP%7D%7BTP+FP%7D"></p>
<p><img src="https://latex.codecogs.com/png.latex?Recall%20=%20%5Cdfrac%7BTP%7D%7BTP+FN%7D"></p>
<p>Precision and recall are also known as positive predictive value (PPV) and sensitivity, respectively. Therefore, they are equivalent to:</p>
<p><img src="https://latex.codecogs.com/png.latex?Precision%20=%20PPR"></p>
<p><img src="https://latex.codecogs.com/png.latex?Recall%20=%20Sensitivity"></p>
</section>
<section id="f1-score" class="level2">
<h2 class="anchored" data-anchor-id="f1-score">F1-Score</h2>
<p>The F1-score combines precision and recall into a single metric, which is the harmonic mean of both values. This metric is also known as the Dice coefficient score, reflecting the similarity between two sets.</p>
<p><img src="https://latex.codecogs.com/png.latex?F1%5C,Score%20%20=%202%20%5Ctimes%20%5Cdfrac%7BPR%7D%7BP+R%7D"></p>
</section>
<section id="evaluating-diagnostic-methods-with-roc-curves" class="level2">
<h2 class="anchored" data-anchor-id="evaluating-diagnostic-methods-with-roc-curves">Evaluating Diagnostic Methods with ROC Curves</h2>
<p>To compare different diagnostic methods or models, the receiver operating characteristic (ROC) curve is a valuable tool. It illustrates the trade-off between sensitivity and specificity at varying thresholds. A high-performing diagnostic method or model should yield a ROC curve closely resembling the top-left corner of the plot, with the area under the ROC curve (AUC) indicating its overall performance.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://lukmanaj.github.io/ailearningloop/posts/medical-eval/roc_curve.png" class="img-fluid figure-img"></p>
<figcaption>ROC Curve</figcaption>
</figure>
</div>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>The multifaceted landscape of medical diagnosis evaluation encompasses a spectrum of metrics and tools. Understanding these metrics and their mathematical underpinnings is crucial for accurately assessing the performance of diagnostic methods and models in healthcare.</p>


</section>

 ]]></description>
  <category>evaluation metrics</category>
  <category>medical diagnosis</category>
  <guid>https://lukmanaj.github.io/ailearningloop/posts/medical-eval/</guid>
  <pubDate>Sat, 06 Jul 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Mastering Data Structures and Algorithms on Coursera</title>
  <dc:creator>Lukman Aliyu Jibril</dc:creator>
  <link>https://lukmanaj.github.io/ailearningloop/posts/dsa-algorithms-cert/</link>
  <description><![CDATA[ 





<p>I am thrilled to announce that I have successfully completed the <strong>Foundations of Algorithms and Data Structures Specialization</strong> offered by the University of Colorado Boulder on Coursera. The specialization was excellently taught by Professor Sriram Sankaranarayanan and spanned five comprehensive courses. I embarked on this journey last year, and it has been an enriching experience.</p>
<section id="course-overview-and-learnings" class="level2">
<h2 class="anchored" data-anchor-id="course-overview-and-learnings">Course Overview and Learnings</h2>
<section id="algorithms-for-searching-sorting-and-indexing" class="level3">
<h3 class="anchored" data-anchor-id="algorithms-for-searching-sorting-and-indexing">1. Algorithms for Searching, Sorting, and Indexing</h3>
<p>This course laid the foundation for algorithm design and analysis. It covered:</p>
<ul>
<li>Basics of algorithm design and analysis</li>
<li>Sorting arrays</li>
<li>Data structures such as priority queues and hash functions</li>
<li>Applications like Bloom filters</li>
</ul>
</section>
<section id="trees-and-graphs-basics" class="level3">
<h3 class="anchored" data-anchor-id="trees-and-graphs-basics">2. Trees and Graphs: Basics</h3>
<p>This course delved into tree and graph data structures. Key topics included:</p>
<ul>
<li>Basic algorithms on tree data structures</li>
<li>Binary search trees and self-balancing trees</li>
<li>Graph data structures and traversal algorithms</li>
<li>Advanced topics like kd-trees for spatial data and algorithms for spatial data</li>
</ul>
</section>
<section id="dynamic-programming-and-greedy-algorithms" class="level3">
<h3 class="anchored" data-anchor-id="dynamic-programming-and-greedy-algorithms">3. Dynamic Programming and Greedy Algorithms</h3>
<p>In this course, I learned essential algorithm design techniques, including:</p>
<ul>
<li>Divide and conquer</li>
<li>Dynamic programming</li>
<li>Greedy algorithms</li>
<li>Introduction to intractability (NP-completeness)</li>
<li>Using linear/integer programming solvers for optimization problems</li>
<li>Advanced topics in data structures</li>
</ul>
</section>
<section id="approximate-algorithms-and-linear-programming" class="level3">
<h3 class="anchored" data-anchor-id="approximate-algorithms-and-linear-programming">4. Approximate Algorithms and Linear Programming</h3>
<p>This course focused on:</p>
<ul>
<li>Linear and integer programming formulations for solving algorithmic problems</li>
<li>Applications in resource allocation, scheduling, task assignment, and the traveling salesperson problem</li>
<li>Algorithms for NP-hard problems with guaranteed approximation factors</li>
<li>Efficient algorithms providing useful bounds on optimal solutions</li>
</ul>
</section>
<section id="advanced-data-structures-rsa-and-quantum-algorithms" class="level3">
<h3 class="anchored" data-anchor-id="advanced-data-structures-rsa-and-quantum-algorithms">5. Advanced Data Structures, RSA, and Quantum Algorithms</h3>
<p>The final course introduced:</p>
<ul>
<li>Number-theory based cryptography</li>
<li>Basics of quantum algorithms</li>
<li>Advanced data structures like B-Trees and Suffix Tries</li>
</ul>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Completing this specialization has been a significant milestone in my Data Science and Machine Learning/AI journey. I am excited to apply the knowledge and skills I have gained to future projects and challenges.</p>
<p>I highly recommend this specialization to anyone looking to deepen their understanding of algorithms and data structures. The courses are well-structured, and Professor Sriram Sankaranarayanan’s teaching is both clear and engaging.</p>
<p>I look forward to what the future holds and I am eager to continue my learning journey in the ever-evolving field of Data Science and AI.</p>


</section>

 ]]></description>
  <category>data structures</category>
  <category>algorithms</category>
  <category>data science</category>
  <category>programming</category>
  <guid>https://lukmanaj.github.io/ailearningloop/posts/dsa-algorithms-cert/</guid>
  <pubDate>Mon, 10 Jun 2024 00:00:00 GMT</pubDate>
  <media:content url="https://lukmanaj.github.io/ailearningloop/posts/dsa-algorithms-cert/dls-cert.png" medium="image" type="image/png" height="113" width="144"/>
</item>
<item>
  <title>My Journey in Deep Learning: From TensorFlow to PyTorch and Back</title>
  <dc:creator>Lukman Aliyu Jibril</dc:creator>
  <link>https://lukmanaj.github.io/ailearningloop/posts/tensorflow-dev/</link>
  <description><![CDATA[ 





<p>I recently completed the <code>DeepLearning.AI</code> TensorFlow Developer Professional Certificate, a milestone that marked a significant step in my deep learning journey. My exploration of TensorFlow began last year while taking the second course of the DeepLearning.AI Deep Learning Specialization, excellently taught by Andrew Ng.</p>
<p>Following my initial foray into TensorFlow, I delved extensively into PyTorch during the Arewa Data Science Academy’s Deep Learning with PyTorch fellowship. The allure of PyTorch, with its state-of-the-art (SOTA) models, pythonic nature and high customizability, made me somewhat hesitant to further pursue TensorFlow. However, an unexpected opportunity arose when, as a moderator in the DeepLearning.AI Forum, I was invited to test a TensorFlow course. This experience reignited my interest and ultimately led me to pursue the Professional Certificate on Coursera.</p>
<p>Contrary to my initial reservations, I found TensorFlow to be quite user-friendly. While it may offer less customization compared to PyTorch, TensorFlow’s learning curve is significantly gentler, making it accessible to a wider audience.</p>
<p>The TensorFlow Developer Professional Certificate comprises four courses, each designed to quickly get learners up to speed with TensorFlow:</p>
<ol type="1">
<li>Introduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning</li>
<li>Convolutional Neural Networks in TensorFlow</li>
<li>Natural Language Processing in TensorFlow</li>
<li>Sequences, Time Series, and Prediction</li>
</ol>
<p>The certificate is excellently taught by Laurence Moroney, who leads AI Advocacy at Google. His vision is to make AI easy for developers and widen access to machine learning careers for everyone. Laurence is a prolific author, with dozens of programming books to his name, including ‘AI and ML for Coders’ at O’Reilly. He is also an active member of the Science Fiction Writers of America, having authored several sci-fi novels, comic books, and a produced screenplay.</p>
<p>Throughout the series of courses, I built on my existing TensorFlow skills, learning about regular dense layers, convolutional neural networks (CNNs), recurrent neural networks (RNNs), long short-term memory networks (LSTMs), gated recurrent units (GRUs), and more. I even explored lambda layers in TensorFlow. The culmination of the course involved applying deep learning and sequence models to time series data.</p>
<p>Completing this course has been an immensely rewarding experience. I am grateful to Coursera for providing a 90% discount, making it possible for me to embark on and complete this learning journey. The structured learning path and the expertise of the instructors have significantly enhanced my understanding and proficiency in using TensorFlow.</p>
<p>In conclusion, while my journey began with PyTorch, the experience with TensorFlow has been equally enriching. TensorFlow’s ease of use and comprehensive learning resources have made it a valuable tool in my deep learning toolkit. I am excited to apply these skills in future projects and continue exploring the evolving landscape of deep learning technologies.</p>



 ]]></description>
  <category>tensorflow</category>
  <category>ai</category>
  <category>deep learning</category>
  <guid>https://lukmanaj.github.io/ailearningloop/posts/tensorflow-dev/</guid>
  <pubDate>Mon, 27 May 2024 00:00:00 GMT</pubDate>
  <media:content url="https://lukmanaj.github.io/ailearningloop/posts/tensorflow-dev/tensorflow-cert.png" medium="image" type="image/png" height="118" width="144"/>
</item>
<item>
  <title>PyTorch for Natural Language Processing</title>
  <dc:creator>Lukman Aliyu Jibril</dc:creator>
  <link>https://lukmanaj.github.io/ailearningloop/posts/pytorch-nlp/</link>
  <description><![CDATA[ 





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>In the dynamic field of Natural Language Processing (NLP), the quest for more robust and scalable models has led researchers and developers towards powerful, flexible tools that can handle the complexity and size of modern datasets. One such tool that has risen to prominence is PyTorch. Developed by Facebook’s AI Research lab, PyTorch offers a compelling blend of flexibility, speed, and ease of use, making it an ideal choice for NLP tasks. Having recently completed a track on DataCamp on NLP with PyTorch, I find it useful to pen down how PyTorch changes the game.</p>
</section>
<section id="what-is-pytorch" class="level2">
<h2 class="anchored" data-anchor-id="what-is-pytorch">What is PyTorch?</h2>
<p>PyTorch is an open-source machine learning library based on the Torch library, widely recognized for its simplicity and interface clarity. It excels in areas requiring automatic differentiation and dynamic neural networks, particularly in complex, evolving projects where versatility is as critical as performance. This is largely attributed to its use of dynamic computation graphs (called “define-by-run” schema), which allow changes to be made on-the-fly and graphs to be built during runtime.</p>
</section>
<section id="pytorchs-advantages-for-nlp" class="level2">
<h2 class="anchored" data-anchor-id="pytorchs-advantages-for-nlp">PyTorch’s Advantages for NLP</h2>
<p>The characteristics of PyTorch particularly beneficial for NLP include its intuitive design, ease of debugging, and seamless integration with the Python programming environment. Unlike static graphs, which need to define and optimize the entire model architecture before running, PyTorch’s dynamic graphs enable developers to alter their models as inputs change, which is especially useful for the varying sequence lengths in text data.</p>
<section id="flexibility-and-ease-of-use" class="level3">
<h3 class="anchored" data-anchor-id="flexibility-and-ease-of-use">Flexibility and Ease of Use</h3>
<p>For NLP, models often need to experiment with novel ideas or hybrid architectures, and PyTorch’s flexibility ensures that researchers can implement changes almost as quickly as they can think of them. It integrates seamlessly with the Python data science stack, making it easier to turn research prototypes into production-ready code.</p>
</section>
<section id="rich-prebuilt-libraries" class="level3">
<h3 class="anchored" data-anchor-id="rich-prebuilt-libraries">Rich Prebuilt Libraries</h3>
<p>PyTorch is supported by a rich ecosystem of libraries and extensions. Libraries such as torchtext simplify text processing and provide utilities for common tasks like tokenization, vocabulary creation, and sequence padding, allowing researchers to handle preprocessing efficiently and focus more on model development.</p>
</section>
<section id="torchtext-for-streamlined-text-preprocessing" class="level3">
<h3 class="anchored" data-anchor-id="torchtext-for-streamlined-text-preprocessing">torchtext for Streamlined Text Preprocessing</h3>
<p>A key component in PyTorch’s NLP capabilities is torchtext, a library designed to streamline preprocessing pipelines. torchtext offers utilities for batch processing of text, making it easier to load and handle large datasets efficiently. With functionalities such as built-in vocabularies, pre-trained word vectors, and support for common datasets, torchtext significantly reduces the boilerplate code required in text preprocessing. This allows developers to rapidly experiment with different NLP models and techniques, enhancing productivity and innovation.</p>
</section>
<section id="community-and-support" class="level3">
<h3 class="anchored" data-anchor-id="community-and-support">Community and Support</h3>
<p>The PyTorch community is a vibrant and growing ecosystem. With extensive documentation, tutorials, and forums, developers and researchers can easily find help and resources. The community not only contributes to the core library but also continuously adds to a growing repository of models and tools, which accelerates development and fosters innovation in NLP.</p>
</section>
</section>
<section id="pytorch-in-action-nlp-applications" class="level2">
<h2 class="anchored" data-anchor-id="pytorch-in-action-nlp-applications">PyTorch in Action: NLP Applications</h2>
<p>In NLP, PyTorch has been used to achieve state-of-the-art results in several areas:</p>
<ol type="1">
<li>Text Classification: PyTorch provides a straightforward approach for developing models for sentiment analysis, topic classification, and more.</li>
<li>Machine Translation: The flexibility in sequence-to-sequence models, attention mechanisms, and memory networks has made PyTorch a popular choice for researchers working on language translation applications.</li>
<li>Language Modeling and Generation: PyTorch supports advanced models like Transformers and GPT-n series, which are crucial for tasks that require understanding context and generating text.</li>
</ol>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>PyTorch’s impact on NLP is undeniable. Its design inherently supports the rapid prototyping and iterative refinement that NLP models often require. Whether you’re a seasoned data scientist or a novice in machine learning, PyTorch provides the tools to innovate and scale NLP applications effectively. As NLP continues to evolve, PyTorch’s role in driving forward the boundaries of what machines understand about human language is likely only to grow, making it an invaluable asset in any NLP developer’s toolkit.</p>


</section>

 ]]></description>
  <category>arewads</category>
  <category>pytorch</category>
  <category>nlp</category>
  <guid>https://lukmanaj.github.io/ailearningloop/posts/pytorch-nlp/</guid>
  <pubDate>Sun, 12 May 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>My Journey to Becoming a Professional Data Scientist: A DataCamp Certification Odyssey</title>
  <dc:creator>Lukman Aliyu Jibril</dc:creator>
  <link>https://lukmanaj.github.io/ailearningloop/posts/datacamp-cert/</link>
  <description><![CDATA[ 





<p>Embarking on the path to deepen my data science expertise, I was presented with an invaluable opportunity—a free premium subscription to DataCamp, courtesy of Arewa Data Science Academy. This opportunity was not just a stepping stone but a gateway to a broader horizon in data science that I eagerly embraced.</p>
<p>The journey commenced with the Associate Data Scientist with Python track. Given my foundational knowledge and previous encounter with DataCamp’s Introduction to Python course, I was not venturing into uncharted territory. This familiarity served me well, enabling me to swiftly navigate through the coursework and complete the entire track within a week.</p>
<p>But the quest for knowledge is insatiable. I ventured further into the realm of data manipulation and retrieval by tackling the revamped SQL Fundamentals track. This was followed by successfully achieving the Associate Data Scientist with SQL certification, marking another milestone in my journey.</p>
<p>Eager to delve deeper, I enrolled in the Data Scientist with Python track. This comprehensive curriculum was not just about Python; it integrated SQL courses and introduced vital skills in package writing and Git fundamentals. This holistic approach equipped me with a robust set of tools and knowledge, preparing me for the pinnacle of my journey—the Professional Data Scientist Certification.</p>
<p>The certification process was rigorous, encompassing two timed written exams that tested my theoretical understanding through multiple-choice and fill-in-the-blank questions. However, the real challenge lay in the practical data science exam, which required me to code machine learning models and present my findings via webcam. This phase tested not just my technical skills but also my ability to communicate complex results effectively.</p>
<p>Despite the pressure and the initial setback in the practical exam—stemming from difficulties in data cleaning—I remained undeterred. After a period of diligent study and review, I reattempted the practical exam. With renewed confidence and enhanced skills, I passed on my second attempt.</p>
<p>This journey was demanding, yet immensely rewarding. It culminated in earning a certificate that not only represents a testament to my skills and dedication but also empowers me with credentials recognized for two years.</p>
<p>Reflecting on this experience, I am reminded of the importance of resilience, the value of continuous learning, and the doors that can open when one is willing to push through challenges. My journey to becoming a professional data scientist was not just about acquiring a certificate; it was about growth, perseverance, and the relentless pursuit of excellence in the ever-evolving field of data science.</p>



 ]]></description>
  <category>arewads</category>
  <category>datacamp</category>
  <category>dcdonates</category>
  <guid>https://lukmanaj.github.io/ailearningloop/posts/datacamp-cert/</guid>
  <pubDate>Sun, 17 Mar 2024 00:00:00 GMT</pubDate>
  <media:content url="https://lukmanaj.github.io/ailearningloop/posts/datacamp-cert/datacamp.png" medium="image" type="image/png" height="83" width="144"/>
</item>
<item>
  <title>Harnessing the Power of R Shiny for Interactive Data Visualization</title>
  <dc:creator>Lukman Aliyu Jibril</dc:creator>
  <link>https://lukmanaj.github.io/ailearningloop/posts/first-shiny/</link>
  <description><![CDATA[ 





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction:</h2>
<p>In the realm of data science and statistical computing, R has long been a staple for analysts and researchers. However, the R ecosystem extends far beyond static analyses and plots, entering the dynamic world of web applications with R Shiny. This powerful framework allows users to build interactive web applications directly from R, enabling end-users to interact with their data analyses and visualizations in real-time.</p>
</section>
<section id="what-is-r-shiny" class="level2">
<h2 class="anchored" data-anchor-id="what-is-r-shiny">What is R Shiny?</h2>
<p>R Shiny is an R package that makes it straightforward to build interactive web apps straight from R. Without needing to know HTML, CSS, or JavaScript, data scientists can create applications that allow users to interact with their data, change parameters, and visualize the results instantly.</p>
</section>
<section id="key-features-of-r-shiny" class="level2">
<h2 class="anchored" data-anchor-id="key-features-of-r-shiny">Key Features of R Shiny:</h2>
<ol type="1">
<li><p>Interactivity: Users can interact with the application, altering inputs and immediately seeing the output change.</p></li>
<li><p>Accessibility: Shiny apps can be hosted on a webpage, making your R analysis accessible to anyone with internet access.</p></li>
<li><p>Customization: While Shiny makes it easy to get started with default settings, it also allows for extensive customization for more advanced users.</p></li>
</ol>
</section>
<section id="getting-started-with-r-shiny" class="level2">
<h2 class="anchored" data-anchor-id="getting-started-with-r-shiny">Getting Started with R Shiny:</h2>
<p>To begin, you need to install the Shiny package from CRAN:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">install.packages</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"shiny"</span>)</span></code></pre></div>
<p>Then, you can create a simple Shiny app with just a few lines of code:</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">library</span>(shiny)</span>
<span id="cb2-2"></span>
<span id="cb2-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Define UI for application</span></span>
<span id="cb2-4">ui <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">fluidPage</span>(</span>
<span id="cb2-5">   <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">titlePanel</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"My First Shiny App"</span>),</span>
<span id="cb2-6">   <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sidebarLayout</span>(</span>
<span id="cb2-7">      <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sidebarPanel</span>(</span>
<span id="cb2-8">         <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sliderInput</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"num"</span>, </span>
<span id="cb2-9">                     <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Choose a number:"</span>, </span>
<span id="cb2-10">                     <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">min =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, </span>
<span id="cb2-11">                     <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">max =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, </span>
<span id="cb2-12">                     <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">value =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>)</span>
<span id="cb2-13">      ),</span>
<span id="cb2-14">      <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mainPanel</span>(</span>
<span id="cb2-15">         <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">textOutput</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"selectedNum"</span>)</span>
<span id="cb2-16">      )</span>
<span id="cb2-17">   )</span>
<span id="cb2-18">)</span>
<span id="cb2-19"></span>
<span id="cb2-20"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Define server logic</span></span>
<span id="cb2-21">server <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">function</span>(input, output) {</span>
<span id="cb2-22">   output<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>selectedNum <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">renderText</span>({ </span>
<span id="cb2-23">       <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">paste</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"You selected"</span>, input<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>num)</span>
<span id="cb2-24">   })</span>
<span id="cb2-25">}</span>
<span id="cb2-26"></span>
<span id="cb2-27"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Run the application </span></span>
<span id="cb2-28"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">shinyApp</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ui =</span> ui, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">server =</span> server)</span></code></pre></div>
<p>This app creates a slider for the user to select a number and immediately displays the selected number on the screen.</p>
</section>
<section id="challenges-and-limitations" class="level2">
<h2 class="anchored" data-anchor-id="challenges-and-limitations">Challenges and Limitations:</h2>
<p>While R Shiny is an incredibly powerful tool, it does come with its challenges and limitations:</p>
<ol type="1">
<li><p>Performance: Heavy computational tasks can slow down your app, affecting user experience.</p></li>
<li><p>Scalability: Shiny apps can require significant resources for multiple users, which can be a challenge for large-scale applications.</p></li>
<li><p>Learning Curve: For those unfamiliar with web development concepts, there might be a learning curve.</p></li>
</ol>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion:</h2>
<p>R Shiny offers a bridge between data analysis in R and interactive web applications, empowering data scientists to share their insights in a more dynamic and accessible way. Despite its challenges, the benefits of creating interactive and user-friendly applications make R Shiny a valuable tool in the data scientist’s toolkit.</p>


</section>

 ]]></description>
  <category>arewads</category>
  <category>shiny</category>
  <category>rprogramming</category>
  <guid>https://lukmanaj.github.io/ailearningloop/posts/first-shiny/</guid>
  <pubDate>Sun, 10 Mar 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Object Oriented Programming: Python vs R</title>
  <dc:creator>Lukman Aliyu Jibril</dc:creator>
  <link>https://lukmanaj.github.io/ailearningloop/posts/oop/</link>
  <description><![CDATA[ 





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Object-Oriented Programming (OOP) is a programming paradigm that uses “objects” to design applications and computer programs. It utilizes several techniques from previously established paradigms, including modularity, polymorphism, and encapsulation. Today, we’ll explore how OOP concepts manifest in two popular programming languages: Python and R, particularly focusing on inheritance, using the context of a microwave oven as an example.</p>
</section>
<section id="oop-in-python" class="level2">
<h2 class="anchored" data-anchor-id="oop-in-python">OOP in Python</h2>
<p>In Python, OOP is central to the language. This can be seen in the way classes are defined and used. Python supports inheritance, allowing new classes to inherit attributes and methods from existing classes. This feature facilitates code reusability and the hierarchical organization of classes. For example, if you were modeling microwave ovens, you could start with a basic microwave class and then create a subclass for a fancier microwave with additional features:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> MicrowaveOven:</span>
<span id="cb1-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, power_rating):</span>
<span id="cb1-3">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.power_rating <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> power_rating</span>
<span id="cb1-4"></span>
<span id="cb1-5">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> cook(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, time_seconds):</span>
<span id="cb1-6">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Your food is cooked!"</span>)</span>
<span id="cb1-7"></span>
<span id="cb1-8"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> FancyMicrowaveOven(MicrowaveOven):</span>
<span id="cb1-9">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> cook_baked_potato(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb1-10">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.cook(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)</span>
<span id="cb1-11">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Enjoy your baked potato!"</span>)</span>
<span id="cb1-12"></span>
<span id="cb1-13">a_fancy_microwave <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> FancyMicrowaveOven(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)</span>
<span id="cb1-14">a_fancy_microwave.cook_baked_potato()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Your food is cooked!
Enjoy your baked potato!</code></pre>
</div>
</div>
<p>In this Python example, FancyMicrowaveOven inherits from MicrowaveOven, meaning it can use the cook method defined in the parent class and add its methods like cook_baked_potato.</p>
</section>
<section id="oop-in-r" class="level2">
<h2 class="anchored" data-anchor-id="oop-in-r">OOP in R</h2>
<p>In contrast, R, traditionally seen as a statistical programming language, has incorporated OOP features more gradually. The R6 package in R allows for a more classical approach to OOP, supporting encapsulation and inheritance but in a somewhat different manner than Python. The R6 framework allows R developers to create classes with reference semantics, which can be somewhat akin to how Python’s classes operate.</p>
<p>Here’s how you might define a similar set of microwave classes in R using the R6 package:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">library</span>(R6)</span>
<span id="cb3-2">microwave_oven_factory <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">R6Class</span>(</span>
<span id="cb3-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MicrowaveOven"</span>,</span>
<span id="cb3-4">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">private =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">list</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">power_rating_watts =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">800</span>),</span>
<span id="cb3-5">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">public =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">list</span>(</span>
<span id="cb3-6">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">cook =</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">function</span>(time_seconds) {</span>
<span id="cb3-7">            <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">Sys.sleep</span>(time_seconds)</span>
<span id="cb3-8">            <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Your food is cooked!"</span>)</span>
<span id="cb3-9">        }</span>
<span id="cb3-10">    )</span>
<span id="cb3-11">)</span>
<span id="cb3-12"></span>
<span id="cb3-13">fancy_microwave_oven_factory <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">R6Class</span>(</span>
<span id="cb3-14">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"FancyMicrowaveOven"</span>,</span>
<span id="cb3-15">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">inherit =</span> microwave_oven_factory,</span>
<span id="cb3-16">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">public =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">list</span>(</span>
<span id="cb3-17">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">cook_baked_potato =</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">function</span>() {</span>
<span id="cb3-18">            self<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">cook</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)</span>
<span id="cb3-19">            <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Enjoy your baked potato!"</span>)</span>
<span id="cb3-20">        }</span>
<span id="cb3-21">    )</span>
<span id="cb3-22">)</span>
<span id="cb3-23"></span>
<span id="cb3-24">a_fancy_microwave <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> fancy_microwave_oven_factory<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">new</span>()</span>
<span id="cb3-25">a_fancy_microwave<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">cook_baked_potato</span>()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Your food is cooked!"
[1] "Enjoy your baked potato!"</code></pre>
</div>
</div>
<p>In the R example, FancyMicrowaveOven is defined with inherit = microwave_oven_factory, which establishes an inheritance relationship with MicrowaveOven. This setup allows the fancy microwave to use the cook method from its parent class while adding a new method cook_baked_potato.</p>
</section>
<section id="differences" class="level2">
<h2 class="anchored" data-anchor-id="differences">Differences</h2>
<p>A key difference in the OOP implementation between Python and R is the syntax and the explicit use of self and super. In Python, self refers to the instance itself and is used to access class attributes and methods from within. super(), on the other hand, is used to call methods from a superclass in the context of inheritance.</p>
<p>In R’s R6, self serves a similar purpose as in Python, referring to the current object. However, R6 does not have a direct counterpart to Python’s super; instead, method overriding involves calling the superclass method directly through super$method_name(). Inheritance in R6 is established through the inherit parameter in the class definition, allowing the new class to access the public methods and properties of the parent class.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>In summary, while Python’s OOP features have been integral to the language from its conception, R has adopted OOP paradigms over time, with packages like R6 introducing class-based programming that includes inheritance. Both languages offer robust capabilities for OOP, facilitating complex and modular program design. In practice, choosing between Python and R for OOP depends on the specific requirements of your project and your personal or team’s familiarity with each language.</p>


</section>

 ]]></description>
  <category>oop</category>
  <category>python</category>
  <category>rprogramming</category>
  <guid>https://lukmanaj.github.io/ailearningloop/posts/oop/</guid>
  <pubDate>Wed, 06 Mar 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Celebrating a Milestone in Statistics: A Journey through DataCamp’s ‘Statistics Fundamentals with R’ Track</title>
  <dc:creator>Lukman Aliyu Jibril</dc:creator>
  <link>https://lukmanaj.github.io/ailearningloop/posts/statistics-in-r/</link>
  <description><![CDATA[ 





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>It’s a moment of pride and immense satisfaction as I share the completion of the “Statistics Fundamentals with R” track offered by DataCamp, an educational journey made possible through the generous donations of DataCamp to Arewa Data Science Academy. This accomplishment is not just a testament to my dedication but also highlights the empowering vision of Arewa Data Science Academy and DataCamp in fostering data literacy and analytical skills among learners.</p>
</section>
<section id="the-pathway-to-mastery-delving-deep-into-statistical-analysis" class="level2">
<h2 class="anchored" data-anchor-id="the-pathway-to-mastery-delving-deep-into-statistical-analysis">The Pathway to Mastery: Delving Deep into Statistical Analysis</h2>
<p>The track was meticulously designed to guide learners through the foundational to advanced concepts of statistics applied in the versatile R programming environment. It commenced with the “Introduction to Statistics in R”, a course that laid the groundwork by introducing statistical concepts and how they are implemented in R. This initial phase was crucial as it set the tone for the rigorous analytical skills I was about to develop.</p>
<p>Transitioning from basics to more complex analyses, the “Introduction to Regression in R” and “Intermediate Regression in R” courses elevated my understanding of relationships between variables and how these relationships can be quantified and tested. Through these courses, I gained proficiency in building, diagnosing, and interpreting linear regression models, an indispensable skill in any data scientist’s toolkit.</p>
<p>The learning curve then ascended to “Sampling in R” and “Hypothesis Testing in R”, courses that are fundamental in statistical inference. These courses taught me the importance of sample design, the intricacies of drawing conclusions from data, and the procedures for testing hypotheses with confidence. The practical applications and real-world examples reinforced my understanding and applicability of these statistical methods.</p>
</section>
<section id="solidifying-knowledge-and-practical-application" class="level2">
<h2 class="anchored" data-anchor-id="solidifying-knowledge-and-practical-application">Solidifying Knowledge and Practical Application</h2>
<p>The culmination of the coursework was not merely theoretical; it demanded practical application of all the learned concepts. The project “Hypothesis Testing with Men’s and Women’s Soccer Matches” was a challenging yet thrilling part of the track. It tasked me with performing a hypothesis test to determine if there were differences in goals scored between women’s and men’s soccer matches. This project was not just an academic exercise but a real-world application that honed my analytical and critical thinking skills.</p>
</section>
<section id="benchmarking-skills-and-achieving-excellence" class="level2">
<h2 class="anchored" data-anchor-id="benchmarking-skills-and-achieving-excellence">Benchmarking Skills and Achieving Excellence</h2>
<p>The journey concluded with a skill assessment that tested the breadth and depth of my statistical knowledge and R programming skills. Achieving a rank in the 98th percentile was a moment of great pride and a testament to the quality of learning and understanding I had gained throughout the track.</p>
</section>
<section id="a-journey-of-empowerment-and-future-aspirations" class="level2">
<h2 class="anchored" data-anchor-id="a-journey-of-empowerment-and-future-aspirations">A Journey of Empowerment and Future Aspirations</h2>
<p>Completing the “Statistics Fundamentals with R” track is more than just an academic achievement; it’s an empowering journey that has equipped me with the skills to make data-driven decisions and insights. It has instilled in me a profound respect for data and its potential to influence real-world outcomes.</p>
<p>I am immensely grateful to Arewa Data Science Academy and DataCamp for providing this incredible learning opportunity. This experience has not only enhanced my statistical analysis and R programming skills but has also inspired me to pursue further studies in data science and contribute to the field.</p>
<p>As I reflect on this journey, I am filled with gratitude and motivation to apply these skills in real-world scenarios, continue learning, and contribute to the growing field of data science. Here’s to a future where data and analytics lead the way to innovation and informed decision-making.</p>


</section>

 ]]></description>
  <category>arewads</category>
  <category>datacamp</category>
  <category>rstats</category>
  <guid>https://lukmanaj.github.io/ailearningloop/posts/statistics-in-r/</guid>
  <pubDate>Sun, 03 Mar 2024 00:00:00 GMT</pubDate>
  <media:content url="https://lukmanaj.github.io/ailearningloop/posts/statistics-in-r/stats.png" medium="image" type="image/png" height="83" width="144"/>
</item>
<item>
  <title>Becoming a Mentor at Arewa Data Science Academy: A Journey of Growth and Contribution</title>
  <dc:creator>Lukman Aliyu Jibril</dc:creator>
  <link>https://lukmanaj.github.io/ailearningloop/posts/arewads-mentorship/</link>
  <description><![CDATA[ 





<p>I am thrilled to announce my recent role as a mentor for the second cohort of machine learning fellows at the Arewa Data Science Academy, guiding them through the Python programming phase of their fellowship. Reflecting on this experience brings a mix of emotions; it’s hard to believe that just a year ago, I was in their shoes, diligently learning Python myself. This journey has been a testament to how far I’ve come.</p>
<p>One of the most rewarding aspects of this role is the reciprocal nature of learning and teaching. As I share my knowledge and experience with the fellows, I find my own skills sharpening. The process of mentoring not only benefits the learners but also enhances my understanding and proficiency in data science.</p>
<p>Currently, I’m engaged in mentoring these bright individuals in the data science phase of their fellowship. The academy has graciously recognized my contributions and efforts by awarding me a certificate, which I proudly share with immense gratitude.</p>
<p>This past year has been a remarkable period of personal and professional growth for me. The journey with Arewa Data Science Academy has been incredibly fulfilling, offering me the chance to both impart knowledge and continuously learn. As I navigate this path of ongoing exploration and adaptation, I deeply appreciate the opportunity given to me by the Academy. It’s a privilege to contribute to the growth of others while simultaneously enriching my own experience.</p>
<p>I am grateful for this journey and look forward to what the future holds. May Allah guide us all on our path to success.</p>



 ]]></description>
  <category>arewads</category>
  <category>mentorship</category>
  <guid>https://lukmanaj.github.io/ailearningloop/posts/arewads-mentorship/</guid>
  <pubDate>Sun, 28 Jan 2024 00:00:00 GMT</pubDate>
  <media:content url="https://lukmanaj.github.io/ailearningloop/posts/arewads-mentorship/mentor.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>Advanced Optimization Techniques in Deep Learning: Mastering PyTorch</title>
  <dc:creator>Lukman Aliyu Jibril</dc:creator>
  <link>https://lukmanaj.github.io/ailearningloop/posts/optimization/</link>
  <description><![CDATA[ 





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>In the journey of training deep learning models, choosing the right optimization algorithm is crucial to achieve faster convergence and better final results. While Gradient Descent has been the go-to method, there are more advanced techniques that can significantly improve model performance. In this article, we will explore these advanced optimization methods, including Stochastic Gradient Descent (SGD), Momentum, RMSProp, and Adam, and how they can be applied effectively in PyTorch.</p>
</section>
<section id="understanding-optimization-in-deep-learning" class="level2">
<h2 class="anchored" data-anchor-id="understanding-optimization-in-deep-learning">Understanding Optimization in Deep Learning</h2>
<p>Optimization in deep learning is about finding the best parameters (weights and biases) of a neural network. It involves minimizing a cost function, analogous to finding the lowest point in a hilly landscape. This process is iterative and requires updating the model’s parameters in a certain direction at each training step.</p>
<ol type="1">
<li><p><strong>Stochastic Gradient Descent (SGD)</strong>: SGD is a variation of the gradient descent algorithm. It updates the model’s parameters using only a single training example at a time. This makes the updates faster but causes the parameter path to oscillate, taking a bit longer to converge.</p></li>
<li><p><strong>Mini-Batch Gradient Descent</strong>: This method strikes a balance by using a subset of the training set to perform each update. It’s faster than batch gradient descent and more stable than SGD. The mini-batches are randomly selected, ensuring that the model doesn’t see the same examples in each iteration.</p></li>
<li><p><strong>Momentum</strong>: Momentum optimization is like pushing a ball down a hill; it accumulates velocity as it rolls downhill, becoming faster and more powerful. This method helps accelerate SGD by navigating along relevant directions and dampening oscillations. It achieves this by taking into account the past gradients to update the weights. Essentially, it adds a fraction (denoted as the momentum term) of the update vector of the past step to the current update vector.</p></li>
<li><p><strong>RMSProp</strong>: Root Mean Square Propagation (RMSProp) is an adaptive learning rate method. It divides the learning rate for a weight by a running average of the magnitudes of recent gradients for that weight. This means that the learning rate gets adjusted automatically and is different for each parameter. RMSProp performs well in situations where the optimization landscape is very uneven, as it can adapt to the changing landscape accelerating the training process.</p></li>
<li><p><strong>Adam</strong>: Adaptive Moment Estimation (Adam) combines the ideas from RMSProp and Momentum. It keeps track of an exponentially decaying average of past gradients (like Momentum) and an exponentially decaying average of past squared gradients (like RMSProp). Adam calculates the adaptive learning rates for each parameter. This method is often recommended as the default optimizer due to its effectiveness in handling sparse gradients on noisy problems.</p></li>
</ol>
</section>
<section id="implementing-optimization-methods-in-pytorch" class="level2">
<h2 class="anchored" data-anchor-id="implementing-optimization-methods-in-pytorch">Implementing Optimization Methods in PyTorch</h2>
<p>In PyTorch, these optimization methods are readily available and can be easily implemented. Here’s a brief guide on how to use them:</p>
<p><strong>Gradient Descent</strong>: Use torch.optim.SGD with a learning rate parameter.</p>
<p><strong>Momentum</strong>: Add the momentum parameter to the torch.optim.SGD optimizer.</p>
<p><strong>RMSProp</strong>: Use torch.optim.RMSprop, providing parameters such as learning rate and decay rate.</p>
<p><strong>Adam</strong>: Simply use torch.optim.Adam, which requires minimal tuning.</p>
</section>
<section id="learning-rate-decay-and-scheduling" class="level2">
<h2 class="anchored" data-anchor-id="learning-rate-decay-and-scheduling">Learning Rate Decay and Scheduling</h2>
<p>Over time, reducing the learning rate can help the model converge by taking smaller steps. This is particularly important in the later stages of training. PyTorch provides scheduling utilities (e.g., torch.optim.lr_scheduler) to adjust the learning rate during training. By combining these schedulers with the optimization methods, you can achieve more robust and faster convergence.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Selecting the right optimizer and learning rate schedule can drastically improve the performance of your deep learning models. While Adam is a safe and effective default choice, exploring other optimizers like Momentum and RMSProp can provide better insights into the model’s learning dynamics. Always remember, the choice of optimizer might depend on the specific characteristics of your neural network and the nature of your problem. Experimenting with different optimizers and learning rate schedules is key to finding the most efficient path to training successful deep learning models. Stay tuned for more insights and tutorials on deep learning with PyTorch.</p>
<p>Our journey into mastering deep learning is well underway as we continue to participate in the Deep Learning with PyTorch fellowship with the Arewa Data Science Academy!</p>


</section>

 ]]></description>
  <category>deep learning</category>
  <category>pytorch</category>
  <category>optimization</category>
  <category>arewads</category>
  <category>fellowship</category>
  <guid>https://lukmanaj.github.io/ailearningloop/posts/optimization/</guid>
  <pubDate>Wed, 17 Jan 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>A Beginner’s Comprehensive Guide to PyTorch</title>
  <dc:creator>Lukman Aliyu Jibril</dc:creator>
  <link>https://lukmanaj.github.io/ailearningloop/posts/pytorch-intro/</link>
  <description><![CDATA[ 





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Welcome to the world of PyTorch, a dynamic and powerful machine learning framework that’s revolutionizing the way we approach deep learning. At the core of PyTorch are tensors, versatile structures that extend beyond simple matrices, allowing for more complex and efficient data representations. For instance, a color image with dimensions 64x64 pixels, represented in three color channels (red, green, and blue), is effectively a tensor in PyTorch.</p>
</section>
<section id="getting-started-with-pytorch" class="level2">
<h2 class="anchored" data-anchor-id="getting-started-with-pytorch">Getting Started with PyTorch</h2>
<p>To dive into PyTorch, start by importing it in your Python code with import torch, along with your other dependencies. This guide will serve as an introduction to PyTorch’s capabilities. Don’t worry about memorizing everything.</p>
<div id="6ddd375b" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># importing PyTorch</span></span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch</span>
<span id="cb1-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># how to import the nn module</span></span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch.nn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> nn</span></code></pre></div>
</div>
</section>
<section id="why-choose-pytorch" class="level2">
<h2 class="anchored" data-anchor-id="why-choose-pytorch">Why Choose PyTorch?</h2>
<p>A detailed analysis by the Gradient explains it well: PyTorch offers a more Pythonic experience, is easier to debug, and remains the leading choice in machine learning research. Despite TensorFlow’s efforts to integrate similar features, PyTorch’s intuitive design and growing dominance in both research and industry make it an optimal choice for education and forward-thinking development.</p>
</section>
<section id="exploring-tensor-properties" class="level2">
<h2 class="anchored" data-anchor-id="exploring-tensor-properties">Exploring Tensor Properties</h2>
<p>Creating tensors in PyTorch is straightforward. For instance, you can initiate a tensor using torch.Tensor. A simple example is:</p>
<div id="4a66caf3" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">example_tensor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.Tensor([</span>
<span id="cb2-2">                                [[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>], [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>]],</span>
<span id="cb2-3">                                [[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>], [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>]],</span>
<span id="cb2-4">                                [[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">9</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>]]</span>
<span id="cb2-5">                             ])</span>
<span id="cb2-6">example_tensor</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>tensor([[[1., 2.],
         [3., 4.]],

        [[5., 6.],
         [7., 8.]],

        [[9., 0.],
         [1., 2.]]])</code></pre>
</div>
</div>
<p>Understanding a tensor’s properties, such as its device (CPU or GPU) and shape, is crucial. You can explore these properties using methods like example_tensor.device and example_tensor.shape. These properties give insights into where the tensor resides (CPU or GPU) and its dimensional structure. For example, torch.Size([3, 2, 2]) indicates a tensor of rank 3 with specific dimensions.</p>
<div id="bf0928bd" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"tensor_shape: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>example_tensor<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>shape<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb4-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"tensor device: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>example_tensor<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>device<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor_shape: torch.Size([3, 2, 2])
tensor device: cpu</code></pre>
</div>
</div>
</section>
<section id="indexing-and-initializing-tensors" class="level2">
<h2 class="anchored" data-anchor-id="indexing-and-initializing-tensors">Indexing and Initializing Tensors</h2>
<p>Manipulating tensors in PyTorch is akin to handling NumPy arrays. You can access elements or slices of tensors using standard Python indexing. Moreover, initializing tensors is versatile in PyTorch. Functions like torch.ones_like and torch.zeros_like help create tensors filled with ones or zeros, mimicking the shape and device of a reference tensor.</p>
</section>
<section id="pytorchs-neural-network-module-torch.nn" class="level2">
<h2 class="anchored" data-anchor-id="pytorchs-neural-network-module-torch.nn">PyTorch’s Neural Network Module (torch.nn)</h2>
<p>PyTorch’s torch.nn module is a treasure trove for neural network enthusiasts. It offers a plethora of classes to build and transform tensors efficiently. For example, nn.Linear for linear transformations, nn.ReLU for applying the ReLU activation function, and nn.BatchNorm1d for batch normalization in one-dimensional data.</p>
</section>
<section id="optimization-techniques" class="level2">
<h2 class="anchored" data-anchor-id="optimization-techniques">Optimization Techniques</h2>
<p>One of PyTorch’s strengths is its optimization capabilities, crucial in machine learning. The torch.optim module provides various optimizers like Adam, essential for updating model parameters during training. A typical training loop in PyTorch involves setting gradients to zero, computing loss, backpropagating to calculate gradients, and then updating the parameters.</p>
</section>
<section id="extending-with-custom-nn-modules" class="level2">
<h2 class="anchored" data-anchor-id="extending-with-custom-nn-modules">Extending with Custom nn Modules</h2>
<p>PyTorch allows for the creation of custom classes extending the nn module. This feature lets you define unique model architectures suited to your specific problems. You can define the structure in the <strong>init</strong> method and specify the computation in the forward method.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>PyTorch is not just a tool but a playground for machine learning enthusiasts and researchers. Its intuitive design, Pythonic nature, and robust features make it a top choice for learning and developing cutting-edge machine learning models. As you embark on this journey, PyTorch will undoubtedly be a valuable ally in turning your machine learning aspirations into reality. Subsequent articles will put all these concepts into practice as we delve deeper into the world of PyTorch.</p>


</section>

 ]]></description>
  <category>deep learning</category>
  <category>pytorch</category>
  <category>arewads</category>
  <category>fellowship</category>
  <guid>https://lukmanaj.github.io/ailearningloop/posts/pytorch-intro/</guid>
  <pubDate>Tue, 16 Jan 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Embarking on a Deep Learning Journey with Arewa Data Science Academy’s Deep Learning with PyTorch Fellowship</title>
  <dc:creator>Lukman Aliyu Jibril</dc:creator>
  <link>https://lukmanaj.github.io/ailearningloop/posts/pytorch-fellowship/</link>
  <description><![CDATA[ 





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>This December marked the exciting kickoff of the “Deep Learning with PyTorch” fellowship, hosted by Arewa Data Science Academy. I’m thrilled to share my journey and insights as we delve into the fascinating world of deep learning.</p>
</section>
<section id="the-fellowship-begins" class="level2">
<h2 class="anchored" data-anchor-id="the-fellowship-begins">The Fellowship Begins</h2>
<p>The fellowship’s curriculum is centered around the comprehensive PyTorch deep learning book, a resource that’s freely available and highly recommended for enthusiasts in the field (find it here: <a href="https://github.com/mrdbourke/pytorch-deep-learning">PyTorch Deep Learning by Daniel Bourke</a>).</p>
</section>
<section id="inspirational-instruction" class="level2">
<h2 class="anchored" data-anchor-id="inspirational-instruction">Inspirational Instruction</h2>
<p>A key highlight of this program is our instructor, Mustapha Abdullahi. Fresh from his commendable achievement of completing his master’s with honors from Queen Mary University, UK, Mustapha brings a blend of youthful vigor and profound knowledge to our virtual classroom. His ability to break down complex concepts into digestible bits has been nothing short of remarkable.</p>
</section>
<section id="progress-and-personalization" class="level2">
<h2 class="anchored" data-anchor-id="progress-and-personalization">Progress and Personalization</h2>
<p>Over the past three weeks, we’ve journeyed through the initial three lectures of the course material, each session bringing new insights and challenges. The structure of the program encourages continuous learning and application, with weekly assignments that are meticulously reviewed by Mustapha. His personalized feedback has been instrumental in enhancing our understanding and skills.</p>
</section>
<section id="a-community-of-learners" class="level2">
<h2 class="anchored" data-anchor-id="a-community-of-learners">A Community of Learners</h2>
<p>What makes this fellowship stand out is the sense of community and collective learning. Each of us brings a unique perspective to the table, enriching discussions and collaborations. It’s been a privilege to be part of such a vibrant and passionate group of learners.</p>
</section>
<section id="looking-ahead" class="level2">
<h2 class="anchored" data-anchor-id="looking-ahead">Looking Ahead</h2>
<p>As we progress through the program, I plan to document and share my experiences and learnings in this blog. Whether it’s tackling a challenging concept, celebrating a breakthrough, or sharing useful resources, I hope to provide a window into the dynamic and ever-evolving world of deep learning with PyTorch.</p>
</section>
<section id="stay-connected" class="level2">
<h2 class="anchored" data-anchor-id="stay-connected">Stay Connected</h2>
<p>For those interested in following this journey, stay tuned to this blog. Your comments, questions, and insights are always welcome as we explore the frontiers of deep learning together.</p>


</section>

 ]]></description>
  <category>deep learning</category>
  <category>pytorch</category>
  <category>arewads</category>
  <category>fellowship</category>
  <guid>https://lukmanaj.github.io/ailearningloop/posts/pytorch-fellowship/</guid>
  <pubDate>Mon, 15 Jan 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Speed and Elegance: How Python’s List Comprehensions Outshine Traditional Loops</title>
  <dc:creator>Lukman Aliyu Jibril</dc:creator>
  <link>https://lukmanaj.github.io/ailearningloop/posts/list-comprehensions/</link>
  <description><![CDATA[ 





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Python is celebrated for its elegant syntax and the ability to express complex ideas in a few lines of code, with list comprehensions being a shining example. These powerful expressions streamline the process of creating new lists by transforming and filtering data seamlessly. While the trusty <code>for</code> loop has its merits, list comprehensions bring efficiency and clarity to the forefront of Python programming. Join me as we delve into the reasons that elevate list comprehensions above traditional loops, showcasing why they’re not just a tool but an essential Python idiom for any coder’s toolkit.</p>
</section>
<section id="readability-and-conciseness" class="level2">
<h2 class="anchored" data-anchor-id="readability-and-conciseness">Readability and Conciseness</h2>
<p>One of the main advantages of list comprehensions is their readability and conciseness. A list comprehension allows you to write a loop in a single line of code. For example, the code example below uses a list comprehension to square each number. To begin, let’s define a list of numbers.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1">main_list <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10000000</span>))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">opt_squared <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [num<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> num <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> main_list <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> num]</span>
<span id="cb2-2">opt_squared[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>:]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>[99999900000025,
 99999920000016,
 99999940000009,
 99999960000004,
 99999980000001]</code></pre>
</div>
</div>
<p>This is not only more readable but also reduces the chance of coding errors because it’s all in one compact line.</p>
<p>On the other hand, the traditional <code>for</code> loop is more verbose:</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">squared <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb4-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(main_list)):</span>
<span id="cb4-3">    main_list_i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> main_list[i] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb4-4">    squared.append(main_list_i)</span>
<span id="cb4-5">squared[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>:]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>[99999900000025,
 99999920000016,
 99999940000009,
 99999960000004,
 99999980000001]</code></pre>
</div>
</div>
<p>The answer gotten is the same in both methods. However, more lines are involved in the traditional <code>for</code> loop, which increases the complexity and the potential for bugs.</p>
</section>
<section id="performance" class="level2">
<h2 class="anchored" data-anchor-id="performance">Performance</h2>
<p>When evaluating algorithms, performance is often equated with speed—how swiftly can the algorithm accomplish its task? To quantitatively assess this aspect, we turn to the <code>timeit</code> module, a robust Python tool that meticulously measures the execution time of small code snippets. This approach allows us to compare the speed of list comprehensions with traditional loops under a precise and controlled benchmark.</p>
<p><code>List comprehension performance</code></p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%%</span>timeit</span>
<span id="cb6-2">opt_squared <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [num<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> num <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> main_list <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> num]</span>
<span id="cb6-3">opt_squared[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>:]</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>2.41 s ± 431 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</code></pre>
</div>
</div>
<p><code>Traditional loop performance</code></p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%%</span>timeit</span>
<span id="cb8-2">squared <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb8-3"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(main_list)):</span>
<span id="cb8-4">    main_list_i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> main_list[i] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb8-5">    squared.append(main_list_i)</span>
<span id="cb8-6">squared[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>:]</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>2.66 s ± 72.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</code></pre>
</div>
</div>
<p>List comprehensions are generally faster than traditional <code>for</code> loops because they are optimized by Python’s internal C-based engine. This optimization leads to better performance, particularly noticeable when dealing with large datasets, as seen above.</p>
<p>The timeit results indicate that the list comprehension approach is faster than the traditional <code>for</code> loop. While the difference in this case might seems small, it becomes significant as the complexity and size of the data grow.</p>
</section>
<section id="memory-efficiency" class="level2">
<h2 class="anchored" data-anchor-id="memory-efficiency">Memory Efficiency</h2>
<p>List comprehensions can be more memory-efficient than traditional loops. They generate the required list in a single expression, which allows Python’s memory allocator to optimize its strategy. This advantage can lead to better performance in memory usage, which is critical in large-scale applications.</p>
</section>
<section id="expressiveness-and-flexibility" class="level2">
<h2 class="anchored" data-anchor-id="expressiveness-and-flexibility">Expressiveness and Flexibility</h2>
<p>List comprehensions can incorporate complex expressions and multiple conditions in a single line. The inclusion of an if statement in the list comprehension above is an excellent example of this expressiveness:</p>
<div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb10-1">opt_squared <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [num<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> num <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> main_list <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> num]</span></code></pre></div>
<p>Here, the <code>if num</code> serves as a filter to exclude falsy values (like 0) before the squaring operation. To achieve the same with a traditional loop, additional lines and conditions would be necessary.</p>
</section>
<section id="pythonic-idiom" class="level2">
<h2 class="anchored" data-anchor-id="pythonic-idiom">Pythonic Idiom</h2>
<p>Using list comprehensions is considered more “Pythonic,” a term that refers to the idiomatic use of Python. Python’s philosophy emphasizes simplicity and the importance of writing clear, readable, and concise code as specified in the <code>Zen of Python</code>. List comprehensions align perfectly with this philosophy, encouraging clean and maintainable code.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>While traditional <code>for</code> loops have their place and are sometimes necessary, list comprehensions offer a more elegant and efficient way to create lists based on existing iterables. They provide better performance, improved readability, and a more Pythonic approach to coding. The code example given is a practical demonstration of how a seemingly small change in syntax and approach can lead to better code performance and maintainability, which is why list comprehensions are generally preferred in Python development.</p>


</section>

 ]]></description>
  <category>programming</category>
  <category>python</category>
  <guid>https://lukmanaj.github.io/ailearningloop/posts/list-comprehensions/</guid>
  <pubDate>Wed, 13 Dec 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Juggling Code and Career: My Arewa Data Science 30 Days Of Python Learning Journey Amidst a Full Schedule</title>
  <dc:creator>Lukman Aliyu Jibril</dc:creator>
  <link>https://lukmanaj.github.io/ailearningloop/posts/learning-motivation/</link>
  <description><![CDATA[ 





<p>As part of the first cohort of the Arewa Data Science Academy Data Science and Machine Learning fellowship, embarking on the thirty days of Python challenge, I found myself navigating through the demanding waters of 12-hour work shifts. Despite these professional commitments, I remained steadfast in my participation in the weekend sessions, often balancing work and learning simultaneously. My thirst for knowledge wasn’t confined to my job; it extended to personal interests like improving my French and Arabic, a testament to my multifaceted curiosity. This same drive propelled me towards learning Python, a field I ventured into without any formal background in computer science.</p>
<p>It’s enlightening to realize that, despite our hectic schedules, we often find pockets of time for activities like browsing social media. This observation led me to a pivotal conclusion: if we can dedicate time to scrolling through feeds, we can surely allocate moments for self-improvement and learning. My approach was methodical yet flexible - committing to consistent, daily progress, and embracing a journey of gradual improvement rather than seeking instant perfection. I wasn’t fixated on getting everything right on my first attempt. Rather, I aimed to complete each day’s exercises, occasionally revisiting previous tasks, to solidify my understanding. This consistent effort was a clear signal of my commitment and eagerness to learn, something that I hoped would be apparent to my mentors. I have since completed the python challenge and eventually the data science fellowship, but the lessons and experiences remain.</p>
<p>In sharing this experience, my aim is to offer a piece of advice to fellow learners, especially those balancing their professional lives with personal development goals. It’s crucial to assess the role of learning in your life and to find ways to integrate it into your daily routine. The journey of learning programming, particularly a versatile language like Python, is not only about acquiring technical skills. It significantly enhances cognitive abilities, problem-solving skills, and overall mental agility. The impact of this commitment is profound, stretching beyond immediate learning outcomes to influence your professional trajectory and personal growth.</p>
<p>While we are fortunate to have mentors to guide us, the crux of learning lies in our own hands. It’s about taking responsibility for our growth, proactively seeking help when needed, and not shying away from the challenges that come with stepping out of our comfort zones. This journey has been a rewarding one, filled with lessons that transcend the realm of programming, offering insights into persistence, time management, and the power of small, consistent steps towards a larger goal.</p>
<p>In conclusion, as you embark on your own learning adventures, remember that the path may not always be easy, but the rewards, both tangible and intangible, are truly worth the effort. Embrace the challenge, cherish the progress, and always keep the flame of curiosity burning bright.</p>



 ]]></description>
  <category>learning</category>
  <category>motivation</category>
  <category>programming</category>
  <category>python</category>
  <guid>https://lukmanaj.github.io/ailearningloop/posts/learning-motivation/</guid>
  <pubDate>Fri, 08 Dec 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Exploring Deep Learning Frameworks: FizzBuzz with PyTorch, TensorFlow, and Keras</title>
  <dc:creator>Lukman Aliyu Jibril</dc:creator>
  <link>https://lukmanaj.github.io/ailearningloop/posts/tensorflow-pytorch-fizzbuzz/</link>
  <description><![CDATA[ 





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction:</h2>
<p>The world of deep learning is dominated by a few key frameworks, each with its unique strengths and idiosyncrasies. PyTorch and TensorFlow are two of the most popular tools in this space, widely used by researchers and industry professionals alike. In this article, we’ll explore the differences between these frameworks by implementing the classic FizzBuzz problem in both PyTorch and TensorFlow. Additionally, we’ll touch upon Keras, a high-level API for TensorFlow, towards the end.</p>
</section>
<section id="fizzbuzz-in-pytorch" class="level2">
<h2 class="anchored" data-anchor-id="fizzbuzz-in-pytorch">FizzBuzz in PyTorch:</h2>
<p>PyTorch, developed by Facebook’s AI Research lab, is known for its simplicity, ease of use, and dynamic computational graph.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch</span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> fizzbuzz_pytorch(max_num):</span>
<span id="cb1-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> num <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, max_num <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>):</span>
<span id="cb1-5">        num_torch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.tensor(num)</span>
<span id="cb1-6">        fizz <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.eq(num_torch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb1-7">        buzz <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.eq(num_torch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb1-8">        fizzbuzz <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.logical_and(fizz, buzz)</span>
<span id="cb1-9"></span>
<span id="cb1-10">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> fizzbuzz.item():</span>
<span id="cb1-11">            <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"FizzBuzz"</span>)</span>
<span id="cb1-12">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">elif</span> fizz.item():</span>
<span id="cb1-13">            <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Fizz"</span>)</span>
<span id="cb1-14">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">elif</span> buzz.item():</span>
<span id="cb1-15">            <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Buzz"</span>)</span>
<span id="cb1-16">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb1-17">            <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(num)</span>
<span id="cb1-18"></span>
<span id="cb1-19">fizzbuzz_pytorch(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1
2
Fizz
4
Buzz
Fizz
7
8
Fizz
Buzz
11
Fizz
13
14
FizzBuzz</code></pre>
</div>
</div>
<section id="key-points" class="level3">
<h3 class="anchored" data-anchor-id="key-points">Key Points:</h3>
<ul>
<li>Dynamic Graphs: PyTorch uses dynamic computational graphs (also known as define-by-run graphs). This means that the graph is built on the fly as operations are executed. This is evident in the way PyTorch handles the FizzBuzz logic, providing a more intuitive Pythonic feel.</li>
<li>Ease of Debugging: Thanks to its dynamic nature, debugging in PyTorch can be more straightforward using standard Python debugging tools.</li>
</ul>
</section>
</section>
<section id="fizzbuzz-in-tensorflow" class="level2">
<h2 class="anchored" data-anchor-id="fizzbuzz-in-tensorflow">FizzBuzz in TensorFlow:</h2>
<p>TensorFlow, developed by the Google Brain team, is renowned for its powerful, scalable, and production-ready features.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> tensorflow <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> tf</span>
<span id="cb3-2"></span>
<span id="cb3-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> fizzbuzz_tensorflow(max_num):</span>
<span id="cb3-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> num <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, max_num <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>):</span>
<span id="cb3-5">        num_tf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.constant(num)</span>
<span id="cb3-6">        fizz <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.equal(tf.math.mod(num_tf, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>), <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb3-7">        buzz <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.equal(tf.math.mod(num_tf, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>), <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb3-8">        fizzbuzz <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.logical_and(fizz, buzz)</span>
<span id="cb3-9"></span>
<span id="cb3-10">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(tf.switch_case(tf.cast(fizzbuzz, tf.int32) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> tf.cast(fizz, tf.int32) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> tf.cast(buzz, tf.int32),</span>
<span id="cb3-11">                             branch_fns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{</span>
<span id="cb3-12">                                 <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>: <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span>: num_tf.numpy(),</span>
<span id="cb3-13">                                 <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>: <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span>: tf.constant(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Fizz"</span>).numpy().decode(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"utf-8"</span>),</span>
<span id="cb3-14">                                 <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>: <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span>: tf.constant(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Buzz"</span>).numpy().decode(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"utf-8"</span>),</span>
<span id="cb3-15">                                 <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>: <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span>: tf.constant(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"FizzBuzz"</span>).numpy().decode(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"utf-8"</span>)</span>
<span id="cb3-16">                             }))</span>
<span id="cb3-17"></span>
<span id="cb3-18">fizzbuzz_tensorflow(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>2023-11-07 21:59:05.053558: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-11-07 21:59:35.805842: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1
2
Fizz
4
Buzz
Fizz
7
8
Fizz
Buzz
11
Fizz
13
14
FizzBuzz</code></pre>
</div>
</div>
<section id="key-points-1" class="level3">
<h3 class="anchored" data-anchor-id="key-points-1">Key Points:</h3>
<ul>
<li>Static Graphs: TensorFlow traditionally used static computational graphs, where the graph is defined before it is executed. TensorFlow 2.x, however, introduced eager execution, which allows a more dynamic approach, similar to PyTorch.</li>
<li>Scalability and Deployment: TensorFlow shines in scalability and deployment, especially in distributed settings and production environments.</li>
</ul>
</section>
</section>
<section id="understanding-the-differences" class="level2">
<h2 class="anchored" data-anchor-id="understanding-the-differences">Understanding the Differences:</h2>
<p>While both implementations achieve the same goal, they highlight some fundamental differences between the two frameworks:</p>
<ul>
<li>Graph Building: In TensorFlow, you often define placeholders and sessions (though less so with eager execution), whereas PyTorch adopts a more straightforward approach using regular Python variables and loops.</li>
<li>Tensors: Both frameworks use tensors as their fundamental data structure, but the way they handle these tensors varies, reflecting their different approaches to graph computation.</li>
</ul>
</section>
<section id="a-note-on-keras" class="level2">
<h2 class="anchored" data-anchor-id="a-note-on-keras">A Note on Keras:</h2>
<p>Keras, now fully integrated into TensorFlow as tf.keras, offers a high-level, user-friendly API. It abstracts many details of TensorFlow, making it accessible for beginners. While Keras might not be the first choice for implementing a simple program like FizzBuzz, it’s an invaluable tool for more complex deep learning models, offering pre-built layers, models, and a wealth of utilities.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion:</h2>
<p>In conclusion, both PyTorch and TensorFlow have their distinct advantages, with PyTorch often being praised for its user-friendly approach and TensorFlow for its scalability and robust deployment capabilities. Keras, as part of TensorFlow, further simplifies the deep learning process, allowing developers to build complex models with ease. Understanding these differences and strengths is crucial for any aspiring or practicing data scientist or AI engineer, helping them choose the right tool for their specific needs and projects.</p>


</section>

 ]]></description>
  <category>ai</category>
  <category>pytorch</category>
  <category>tensorflow</category>
  <category>programming</category>
  <category>python</category>
  <guid>https://lukmanaj.github.io/ailearningloop/posts/tensorflow-pytorch-fizzbuzz/</guid>
  <pubDate>Mon, 06 Nov 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Computation Graphs, Eager Execution and Flow Control in TensorFlow</title>
  <dc:creator>Lukman Aliyu Jibril</dc:creator>
  <link>https://lukmanaj.github.io/ailearningloop/posts/tensorflow-fizzbuzz/</link>
  <description><![CDATA[ 





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction:</h2>
<p>TensorFlow is a popular deep learning framework that provides a robust platform for the creation and execution of computational graphs. Understanding how TensorFlow handles computation through graphs, eager execution, and flow control is pivotal for effectively deploying machine learning/deep learning models.</p>
<p><strong><em>1. Computation Graphs in TensorFlow:</em></strong></p>
<p>A computation graph is a series of TensorFlow operations arranged into a graph of nodes. Each node represents an operation, while the edges represent the data consumed or produced by an operation. This structure allows TensorFlow to optimize the computation, especially in deep learning models.</p>
<p><strong>Benefits of Computation Graphs:</strong></p>
<ul>
<li>Efficiency: Operations can be parallelized across different processors (CPUs, GPUs).</li>
<li>Portability: The graph can be executed on different devices and platforms.</li>
</ul>
<p><strong><em>2. Eager Execution in TensorFlow:</em></strong></p>
<p>Eager execution is an imperative programming environment that evaluates operations immediately. It contrasts with graph execution in that it doesn’t require a computational graph to be defined before running operations.</p>
<p><strong>Advantages of Eager Execution:</strong></p>
<ul>
<li>Interactive Debugging: Operations are executed as they are defined, facilitating easy debugging.</li>
<li>Intuitive Interface: It aligns with the way programmers are used to thinking about their programs.</li>
</ul>
<ol start="3" type="1">
<li>Flow Control in TensorFlow:</li>
</ol>
<p>TensorFlow provides various tools for flow control, enabling the creation of dynamic models. This includes conditionals and loops, which are essential in many machine learning algorithms.</p>
<section id="tensorflow-functions-for-flow-control" class="level3">
<h3 class="anchored" data-anchor-id="tensorflow-functions-for-flow-control">TensorFlow Functions for Flow Control:</h3>
<ul>
<li>tf.cond: Provides a way to perform conditional execution.</li>
<li>tf.while_loop: Allows for the creation of dynamic loops in the graph.</li>
<li>tf.switch_case: Used for creating conditional branching.</li>
</ul>
<section id="demonstrating-flow-control-using-fizzbuzz-in-tensorflow" class="level4">
<h4 class="anchored" data-anchor-id="demonstrating-flow-control-using-fizzbuzz-in-tensorflow">Demonstrating Flow Control using FizzBuzz in Tensorflow</h4>
<p>In a few lines of code, I try to demonstate some tensorflow functionalities using the popular FizzBuzz.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> tensorflow <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> tf</span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> fizzbuzz(max_num):</span>
<span id="cb1-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> num <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, max_num <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>):</span>
<span id="cb1-5">        num_tf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.constant(num)</span>
<span id="cb1-6">        fizz <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.equal(tf.math.mod(num_tf, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>), <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb1-7">        buzz <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.equal(tf.math.mod(num_tf, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>), <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb1-8">        fizzbuzz <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tf.logical_and(fizz, buzz)</span>
<span id="cb1-9"></span>
<span id="cb1-10">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(tf.switch_case(tf.cast(fizzbuzz, tf.int32) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> tf.cast(fizz, tf.int32) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> tf.cast(buzz, tf.int32),</span>
<span id="cb1-11">                             branch_fns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{</span>
<span id="cb1-12">                                 <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>: <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span>: num_tf.numpy(),</span>
<span id="cb1-13">                                 <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>: <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span>: tf.constant(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Fizz"</span>).numpy().decode(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"utf-8"</span>),</span>
<span id="cb1-14">                                 <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>: <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span>: tf.constant(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Buzz"</span>).numpy().decode(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"utf-8"</span>),</span>
<span id="cb1-15">                                 <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>: <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span>: tf.constant(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"FizzBuzz"</span>).numpy().decode(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"utf-8"</span>)</span>
<span id="cb1-16">                             }))</span>
<span id="cb1-17"></span>
<span id="cb1-18">fizzbuzz(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>2023-11-05 21:07:15.366672: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-11-05 21:07:22.171045: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1
2
Fizz
4
Buzz
Fizz
7
8
Fizz
Buzz
11
Fizz
13
14
FizzBuzz</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion:</h2>
<p>The versatility of TensorFlow lies in its ability to seamlessly switch between a static computation graph and eager execution, providing both efficiency and flexibility. Understanding these concepts is essential for any machine learning practitioner working with TensorFlow. Whether you’re implementing simple programs like FizzBuzz or developing complex neural networks, mastering these aspects of TensorFlow will greatly enhance your ability to develop and optimize machine learning models.</p>
<p>On a final note, readers should remember that TensorFlow is an evolving platform and therefore try to refer to the latest documentation for up-to-date practices and API usage.</p>


</section>

 ]]></description>
  <category>ai</category>
  <category>tensorflow</category>
  <category>programming</category>
  <category>python</category>
  <guid>https://lukmanaj.github.io/ailearningloop/posts/tensorflow-fizzbuzz/</guid>
  <pubDate>Sun, 05 Nov 2023 00:00:00 GMT</pubDate>
  <media:content url="https://lukmanaj.github.io/ailearningloop/posts/tensorflow-fizzbuzz/ibm.png" medium="image" type="image/png"/>
</item>
</channel>
</rss>
